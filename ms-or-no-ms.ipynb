{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f933f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1660, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset (Bangla ( Bengali ) sentiment analysis classification benchmark dataset corpus) : https://data.mendeley.com/datasets/p6zc7krs37/4\n",
    "\"\"\"\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import *\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc3c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_units = 50\n",
    "w_decay = 0.05\n",
    "dropout_rate = 0.2\n",
    "epochs_to_run = 500\n",
    "sequence_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8966bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500 positive sentences\n",
      "3307 negative sentences\n",
      "3307 positive sentences\n",
      "3307 negative sentences\n"
     ]
    }
   ],
   "source": [
    "# Loading Bangla ( Bengali ) sentiment analysis classification benchmark dataset\n",
    "positive_sentences = []\n",
    "f = open('../datasets/all_positive_8500.txt','r', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    positive_sentences.append(line.strip())\n",
    "\n",
    "negative_sentences = []\n",
    "f = open('../datasets/all_negative_3307.txt','r', encoding = 'utf-8')\n",
    "for line in f:\n",
    "    negative_sentences.append(line.strip())\n",
    "    \n",
    "print(len(positive_sentences), 'positive sentences')\n",
    "print(len(negative_sentences), 'negative sentences')\n",
    "\n",
    "import random\n",
    "random.shuffle(positive_sentences)\n",
    "\n",
    "for i in range(len(positive_sentences)-len(negative_sentences)):\n",
    "    positive_sentences.pop(0)\n",
    "\n",
    "print(len(positive_sentences), 'positive sentences')\n",
    "print(len(negative_sentences), 'negative sentences')\n",
    "\n",
    "\n",
    "y_pos = [1 for i in range(len(positive_sentences))]\n",
    "y_neg = [0 for i in range(len(negative_sentences))]\n",
    "\n",
    "X = positive_sentences + negative_sentences\n",
    "y = y_pos + y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c24841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141148\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4113e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(X)\n",
    "y = np.array(y)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, sequence_length)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "X_train_val = X_train\n",
    "y_train_val = y_train\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_val, test_size=0.2, random_state=1, stratify=y_train_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "747457da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "def gensim_to_keras_embedding(model, train_embeddings=False):\n",
    "    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_embeddings : bool\n",
    "        If False, the returned weights are frozen and stopped from being updated.\n",
    "        If True, the weights can / will be further updated in Keras.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `keras.layers.Embedding`\n",
    "        Embedding layer, to be used as input to deeper network layers.\n",
    "\n",
    "    \"\"\"\n",
    "    keyed_vectors = model.wv  # structure holding the result of training\n",
    "    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n",
    "    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n",
    "\n",
    "    layer = Embedding(\n",
    "        input_dim=weights.shape[0],\n",
    "        output_dim=weights.shape[1],\n",
    "        weights=[weights],\n",
    "        trainable=train_embeddings,\n",
    "    )\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41679010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 80)\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 100)     14114800    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200, 50), (N 30200       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 200, 1)       51          lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 200)          0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 200)          0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 200)          0           global_average_pooling1d[0][0]   \n",
      "                                                                 flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "before_split (Activation)       (None, 200)          0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 20), (None,  0           before_split[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 80)           0           tf_op_layer_split[0][0]          \n",
      "                                                                 tf_op_layer_split[0][1]          \n",
      "                                                                 tf_op_layer_split[0][8]          \n",
      "                                                                 tf_op_layer_split[0][9]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 8, 10, 1)     0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 8, 10, 2)     130         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 8, 10, 2)     8           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 160)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "op_main (Dense)                 (None, 1)            51          lstm[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "op_conv (Dense)                 (None, 1)            161         flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "avg (Average)                   (None, 1)            0           op_main[0][0]                    \n",
      "                                                                 op_conv[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,145,401\n",
      "Trainable params: 30,597\n",
      "Non-trainable params: 14,114,804\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 10.3386 - op_main_loss: 0.6905 - op_conv_loss: 0.6968 - avg_loss: 0.6873 - op_main_accuracy: 0.5491 - op_conv_accuracy: 0.4993 - avg_accuracy: 0.5447\n",
      "Epoch 00001: val_avg_accuracy improved from -inf to 0.59018, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 3s 21ms/step - loss: 10.3386 - op_main_loss: 0.6905 - op_conv_loss: 0.6968 - avg_loss: 0.6873 - op_main_accuracy: 0.5491 - op_conv_accuracy: 0.4993 - avg_accuracy: 0.5447 - val_loss: 9.3241 - val_op_main_loss: 0.6634 - val_op_conv_loss: 0.6932 - val_avg_loss: 0.6747 - val_op_main_accuracy: 0.5892 - val_op_conv_accuracy: 0.5042 - val_avg_accuracy: 0.5902\n",
      "Epoch 2/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 8.5112 - op_main_loss: 0.6615 - op_conv_loss: 0.6940 - avg_loss: 0.6740 - op_main_accuracy: 0.5959 - op_conv_accuracy: 0.4988 - avg_accuracy: 0.5949\n",
      "Epoch 00002: val_avg_accuracy improved from 0.59018 to 0.63173, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 8.5097 - op_main_loss: 0.6615 - op_conv_loss: 0.6940 - avg_loss: 0.6739 - op_main_accuracy: 0.5959 - op_conv_accuracy: 0.4988 - avg_accuracy: 0.5950 - val_loss: 7.7008 - val_op_main_loss: 0.6393 - val_op_conv_loss: 0.6931 - val_avg_loss: 0.6621 - val_op_main_accuracy: 0.6327 - val_op_conv_accuracy: 0.5156 - val_avg_accuracy: 0.6317\n",
      "Epoch 3/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 7.0548 - op_main_loss: 0.6394 - op_conv_loss: 0.6931 - avg_loss: 0.6617 - op_main_accuracy: 0.6446 - op_conv_accuracy: 0.5005 - avg_accuracy: 0.6427\n",
      "Epoch 00003: val_avg_accuracy improved from 0.63173 to 0.67517, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 7.0548 - op_main_loss: 0.6394 - op_conv_loss: 0.6931 - avg_loss: 0.6617 - op_main_accuracy: 0.6446 - op_conv_accuracy: 0.5005 - avg_accuracy: 0.6427 - val_loss: 6.4106 - val_op_main_loss: 0.6192 - val_op_conv_loss: 0.6931 - val_avg_loss: 0.6506 - val_op_main_accuracy: 0.6761 - val_op_conv_accuracy: 0.5146 - val_avg_accuracy: 0.6752\n",
      "Epoch 4/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 5.8992 - op_main_loss: 0.6178 - op_conv_loss: 0.6931 - avg_loss: 0.6498 - op_main_accuracy: 0.6732 - op_conv_accuracy: 0.5047 - avg_accuracy: 0.6730\n",
      "Epoch 00004: val_avg_accuracy improved from 0.67517 to 0.69311, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 5.8992 - op_main_loss: 0.6178 - op_conv_loss: 0.6931 - avg_loss: 0.6498 - op_main_accuracy: 0.6732 - op_conv_accuracy: 0.5047 - avg_accuracy: 0.6730 - val_loss: 5.3859 - val_op_main_loss: 0.5981 - val_op_conv_loss: 0.6930 - val_avg_loss: 0.6387 - val_op_main_accuracy: 0.6922 - val_op_conv_accuracy: 0.5137 - val_avg_accuracy: 0.6931\n",
      "Epoch 5/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 4.9920 - op_main_loss: 0.6011 - op_conv_loss: 0.6930 - avg_loss: 0.6399 - op_main_accuracy: 0.6821 - op_conv_accuracy: 0.5111 - avg_accuracy: 0.6847\n",
      "Epoch 00005: val_avg_accuracy improved from 0.69311 to 0.71766, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 4.9912 - op_main_loss: 0.6010 - op_conv_loss: 0.6930 - avg_loss: 0.6399 - op_main_accuracy: 0.6822 - op_conv_accuracy: 0.5111 - avg_accuracy: 0.6848 - val_loss: 4.5803 - val_op_main_loss: 0.5796 - val_op_conv_loss: 0.6930 - val_avg_loss: 0.6275 - val_op_main_accuracy: 0.7205 - val_op_conv_accuracy: 0.5118 - val_avg_accuracy: 0.7177\n",
      "Epoch 6/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 4.2707 - op_main_loss: 0.5799 - op_conv_loss: 0.6931 - avg_loss: 0.6277 - op_main_accuracy: 0.7117 - op_conv_accuracy: 0.5106 - avg_accuracy: 0.7101\n",
      "Epoch 00006: val_avg_accuracy improved from 0.71766 to 0.72144, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 4.2707 - op_main_loss: 0.5799 - op_conv_loss: 0.6931 - avg_loss: 0.6277 - op_main_accuracy: 0.7117 - op_conv_accuracy: 0.5106 - avg_accuracy: 0.7101 - val_loss: 3.9445 - val_op_main_loss: 0.5587 - val_op_conv_loss: 0.6930 - val_avg_loss: 0.6151 - val_op_main_accuracy: 0.7214 - val_op_conv_accuracy: 0.5080 - val_avg_accuracy: 0.7214\n",
      "Epoch 7/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 3.7130 - op_main_loss: 0.5611 - op_conv_loss: 0.6932 - avg_loss: 0.6164 - op_main_accuracy: 0.7243 - op_conv_accuracy: 0.5130 - avg_accuracy: 0.7219\n",
      "Epoch 00007: val_avg_accuracy improved from 0.72144 to 0.73182, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 3.7098 - op_main_loss: 0.5616 - op_conv_loss: 0.6931 - avg_loss: 0.6166 - op_main_accuracy: 0.7240 - op_conv_accuracy: 0.5137 - avg_accuracy: 0.7216 - val_loss: 3.4491 - val_op_main_loss: 0.5398 - val_op_conv_loss: 0.6921 - val_avg_loss: 0.6030 - val_op_main_accuracy: 0.7347 - val_op_conv_accuracy: 0.5127 - val_avg_accuracy: 0.7318\n",
      "Epoch 8/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 3.2670 - op_main_loss: 0.5450 - op_conv_loss: 0.6861 - avg_loss: 0.6031 - op_main_accuracy: 0.7311 - op_conv_accuracy: 0.5568 - avg_accuracy: 0.7249\n",
      "Epoch 00008: val_avg_accuracy improved from 0.73182 to 0.74976, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 3.2670 - op_main_loss: 0.5451 - op_conv_loss: 0.6861 - avg_loss: 0.6032 - op_main_accuracy: 0.7309 - op_conv_accuracy: 0.5565 - avg_accuracy: 0.7247 - val_loss: 3.0375 - val_op_main_loss: 0.5248 - val_op_conv_loss: 0.6674 - val_avg_loss: 0.5823 - val_op_main_accuracy: 0.7394 - val_op_conv_accuracy: 0.5477 - val_avg_accuracy: 0.7498\n",
      "Epoch 9/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 2.8716 - op_main_loss: 0.5295 - op_conv_loss: 0.6370 - avg_loss: 0.5734 - op_main_accuracy: 0.7481 - op_conv_accuracy: 0.6613 - avg_accuracy: 0.7400\n",
      "Epoch 00009: val_avg_accuracy improved from 0.74976 to 0.76393, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.8681 - op_main_loss: 0.5287 - op_conv_loss: 0.6362 - avg_loss: 0.5726 - op_main_accuracy: 0.7488 - op_conv_accuracy: 0.6612 - avg_accuracy: 0.7403 - val_loss: 2.6382 - val_op_main_loss: 0.5037 - val_op_conv_loss: 0.5908 - val_avg_loss: 0.5339 - val_op_main_accuracy: 0.7545 - val_op_conv_accuracy: 0.6686 - val_avg_accuracy: 0.7639\n",
      "Epoch 10/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 2.5091 - op_main_loss: 0.5062 - op_conv_loss: 0.5607 - avg_loss: 0.5242 - op_main_accuracy: 0.7631 - op_conv_accuracy: 0.7245 - avg_accuracy: 0.7669\n",
      "Epoch 00010: val_avg_accuracy improved from 0.76393 to 0.78848, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.5088 - op_main_loss: 0.5063 - op_conv_loss: 0.5609 - avg_loss: 0.5243 - op_main_accuracy: 0.7630 - op_conv_accuracy: 0.7242 - avg_accuracy: 0.7663 - val_loss: 2.3151 - val_op_main_loss: 0.4842 - val_op_conv_loss: 0.5120 - val_avg_loss: 0.4868 - val_op_main_accuracy: 0.7658 - val_op_conv_accuracy: 0.7847 - val_avg_accuracy: 0.7885\n",
      "Epoch 11/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 2.2751 - op_main_loss: 0.4944 - op_conv_loss: 0.5179 - avg_loss: 0.4959 - op_main_accuracy: 0.7687 - op_conv_accuracy: 0.7556 - avg_accuracy: 0.7711\n",
      "Epoch 00011: val_avg_accuracy improved from 0.78848 to 0.80548, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 2.2714 - op_main_loss: 0.4937 - op_conv_loss: 0.5171 - avg_loss: 0.4952 - op_main_accuracy: 0.7689 - op_conv_accuracy: 0.7566 - avg_accuracy: 0.7722 - val_loss: 2.1119 - val_op_main_loss: 0.4707 - val_op_conv_loss: 0.4749 - val_avg_loss: 0.4637 - val_op_main_accuracy: 0.7856 - val_op_conv_accuracy: 0.7998 - val_avg_accuracy: 0.8055\n",
      "Epoch 12/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 2.0762 - op_main_loss: 0.4736 - op_conv_loss: 0.4794 - avg_loss: 0.4664 - op_main_accuracy: 0.7887 - op_conv_accuracy: 0.7784 - avg_accuracy: 0.7971\n",
      "Epoch 00012: val_avg_accuracy did not improve from 0.80548\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 2.0709 - op_main_loss: 0.4720 - op_conv_loss: 0.4780 - avg_loss: 0.4649 - op_main_accuracy: 0.7906 - op_conv_accuracy: 0.7793 - avg_accuracy: 0.7980 - val_loss: 1.9702 - val_op_main_loss: 0.4539 - val_op_conv_loss: 0.4593 - val_avg_loss: 0.4459 - val_op_main_accuracy: 0.7951 - val_op_conv_accuracy: 0.7856 - val_avg_accuracy: 0.7979\n",
      "Epoch 13/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.9485 - op_main_loss: 0.4640 - op_conv_loss: 0.4583 - avg_loss: 0.4508 - op_main_accuracy: 0.7934 - op_conv_accuracy: 0.7948 - avg_accuracy: 0.8060\n",
      "Epoch 00013: val_avg_accuracy did not improve from 0.80548\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.9487 - op_main_loss: 0.4639 - op_conv_loss: 0.4590 - avg_loss: 0.4511 - op_main_accuracy: 0.7940 - op_conv_accuracy: 0.7954 - avg_accuracy: 0.8065 - val_loss: 1.9788 - val_op_main_loss: 0.4682 - val_op_conv_loss: 0.4970 - val_avg_loss: 0.4703 - val_op_main_accuracy: 0.7668 - val_op_conv_accuracy: 0.7583 - val_avg_accuracy: 0.7734\n",
      "Epoch 14/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.8354 - op_main_loss: 0.4483 - op_conv_loss: 0.4380 - avg_loss: 0.4327 - op_main_accuracy: 0.8001 - op_conv_accuracy: 0.8092 - avg_accuracy: 0.8161\n",
      "Epoch 00014: val_avg_accuracy did not improve from 0.80548\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.8334 - op_main_loss: 0.4477 - op_conv_loss: 0.4373 - avg_loss: 0.4322 - op_main_accuracy: 0.8010 - op_conv_accuracy: 0.8098 - avg_accuracy: 0.8166 - val_loss: 1.8587 - val_op_main_loss: 0.4482 - val_op_conv_loss: 0.4701 - val_avg_loss: 0.4467 - val_op_main_accuracy: 0.7847 - val_op_conv_accuracy: 0.7781 - val_avg_accuracy: 0.7923\n",
      "Epoch 15/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.7531 - op_main_loss: 0.4374 - op_conv_loss: 0.4205 - avg_loss: 0.4193 - op_main_accuracy: 0.8082 - op_conv_accuracy: 0.8188 - avg_accuracy: 0.8224\n",
      "Epoch 00015: val_avg_accuracy improved from 0.80548 to 0.81681, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.7546 - op_main_loss: 0.4379 - op_conv_loss: 0.4212 - avg_loss: 0.4199 - op_main_accuracy: 0.8067 - op_conv_accuracy: 0.8178 - avg_accuracy: 0.8214 - val_loss: 1.6840 - val_op_main_loss: 0.4207 - val_op_conv_loss: 0.4025 - val_avg_loss: 0.4029 - val_op_main_accuracy: 0.8111 - val_op_conv_accuracy: 0.8196 - val_avg_accuracy: 0.8168\n",
      "Epoch 16/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.6955 - op_main_loss: 0.4298 - op_conv_loss: 0.4110 - avg_loss: 0.4110 - op_main_accuracy: 0.8145 - op_conv_accuracy: 0.8188 - avg_accuracy: 0.8266\n",
      "Epoch 00016: val_avg_accuracy improved from 0.81681 to 0.82625, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.6955 - op_main_loss: 0.4298 - op_conv_loss: 0.4110 - avg_loss: 0.4110 - op_main_accuracy: 0.8145 - op_conv_accuracy: 0.8188 - avg_accuracy: 0.8266 - val_loss: 1.6418 - val_op_main_loss: 0.4158 - val_op_conv_loss: 0.3968 - val_avg_loss: 0.3977 - val_op_main_accuracy: 0.8159 - val_op_conv_accuracy: 0.8263 - val_avg_accuracy: 0.8263\n",
      "Epoch 17/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.6334 - op_main_loss: 0.4194 - op_conv_loss: 0.3942 - avg_loss: 0.3974 - op_main_accuracy: 0.8209 - op_conv_accuracy: 0.8303 - avg_accuracy: 0.8332\n",
      "Epoch 00017: val_avg_accuracy improved from 0.82625 to 0.83097, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.6334 - op_main_loss: 0.4194 - op_conv_loss: 0.3942 - avg_loss: 0.3974 - op_main_accuracy: 0.8209 - op_conv_accuracy: 0.8303 - avg_accuracy: 0.8332 - val_loss: 1.5859 - val_op_main_loss: 0.4041 - val_op_conv_loss: 0.3843 - val_avg_loss: 0.3848 - val_op_main_accuracy: 0.8140 - val_op_conv_accuracy: 0.8253 - val_avg_accuracy: 0.8310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.5828 - op_main_loss: 0.4107 - op_conv_loss: 0.3800 - avg_loss: 0.3861 - op_main_accuracy: 0.8239 - op_conv_accuracy: 0.8358 - avg_accuracy: 0.8370\n",
      "Epoch 00018: val_avg_accuracy improved from 0.83097 to 0.83853, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.5843 - op_main_loss: 0.4117 - op_conv_loss: 0.3801 - avg_loss: 0.3866 - op_main_accuracy: 0.8233 - op_conv_accuracy: 0.8358 - avg_accuracy: 0.8365 - val_loss: 1.5542 - val_op_main_loss: 0.4004 - val_op_conv_loss: 0.3753 - val_avg_loss: 0.3800 - val_op_main_accuracy: 0.8244 - val_op_conv_accuracy: 0.8395 - val_avg_accuracy: 0.8385\n",
      "Epoch 19/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.5648 - op_main_loss: 0.4120 - op_conv_loss: 0.3766 - avg_loss: 0.3848 - op_main_accuracy: 0.8221 - op_conv_accuracy: 0.8382 - avg_accuracy: 0.8380\n",
      "Epoch 00019: val_avg_accuracy improved from 0.83853 to 0.84419, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.5627 - op_main_loss: 0.4114 - op_conv_loss: 0.3758 - avg_loss: 0.3842 - op_main_accuracy: 0.8230 - op_conv_accuracy: 0.8384 - avg_accuracy: 0.8384 - val_loss: 1.5194 - val_op_main_loss: 0.3960 - val_op_conv_loss: 0.3650 - val_avg_loss: 0.3729 - val_op_main_accuracy: 0.8225 - val_op_conv_accuracy: 0.8395 - val_avg_accuracy: 0.8442\n",
      "Epoch 20/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.5243 - op_main_loss: 0.4033 - op_conv_loss: 0.3641 - avg_loss: 0.3748 - op_main_accuracy: 0.8285 - op_conv_accuracy: 0.8411 - avg_accuracy: 0.8406\n",
      "Epoch 00020: val_avg_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.5286 - op_main_loss: 0.4049 - op_conv_loss: 0.3654 - avg_loss: 0.3762 - op_main_accuracy: 0.8268 - op_conv_accuracy: 0.8400 - avg_accuracy: 0.8393 - val_loss: 1.6019 - val_op_main_loss: 0.4031 - val_op_conv_loss: 0.4210 - val_avg_loss: 0.4002 - val_op_main_accuracy: 0.8187 - val_op_conv_accuracy: 0.8083 - val_avg_accuracy: 0.8253\n",
      "Epoch 21/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.4783 - op_main_loss: 0.3932 - op_conv_loss: 0.3484 - avg_loss: 0.3621 - op_main_accuracy: 0.8331 - op_conv_accuracy: 0.8487 - avg_accuracy: 0.8442\n",
      "Epoch 00021: val_avg_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.4782 - op_main_loss: 0.3930 - op_conv_loss: 0.3485 - avg_loss: 0.3621 - op_main_accuracy: 0.8334 - op_conv_accuracy: 0.8485 - avg_accuracy: 0.8443 - val_loss: 1.4753 - val_op_main_loss: 0.3823 - val_op_conv_loss: 0.3594 - val_avg_loss: 0.3606 - val_op_main_accuracy: 0.8310 - val_op_conv_accuracy: 0.8414 - val_avg_accuracy: 0.8414\n",
      "Epoch 22/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.4616 - op_main_loss: 0.3903 - op_conv_loss: 0.3434 - avg_loss: 0.3578 - op_main_accuracy: 0.8318 - op_conv_accuracy: 0.8564 - avg_accuracy: 0.8521\n",
      "Epoch 00022: val_avg_accuracy did not improve from 0.84419\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.4610 - op_main_loss: 0.3904 - op_conv_loss: 0.3429 - avg_loss: 0.3577 - op_main_accuracy: 0.8315 - op_conv_accuracy: 0.8568 - avg_accuracy: 0.8521 - val_loss: 1.4819 - val_op_main_loss: 0.3855 - val_op_conv_loss: 0.3629 - val_avg_loss: 0.3652 - val_op_main_accuracy: 0.8253 - val_op_conv_accuracy: 0.8338 - val_avg_accuracy: 0.8329\n",
      "Epoch 23/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.4340 - op_main_loss: 0.3843 - op_conv_loss: 0.3338 - avg_loss: 0.3497 - op_main_accuracy: 0.8440 - op_conv_accuracy: 0.8625 - avg_accuracy: 0.8603\n",
      "Epoch 00023: val_avg_accuracy improved from 0.84419 to 0.84797, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4340 - op_main_loss: 0.3843 - op_conv_loss: 0.3338 - avg_loss: 0.3497 - op_main_accuracy: 0.8440 - op_conv_accuracy: 0.8625 - avg_accuracy: 0.8603 - val_loss: 1.4733 - val_op_main_loss: 0.3885 - val_op_conv_loss: 0.3586 - val_avg_loss: 0.3618 - val_op_main_accuracy: 0.8319 - val_op_conv_accuracy: 0.8499 - val_avg_accuracy: 0.8480\n",
      "Epoch 24/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.4080 - op_main_loss: 0.3784 - op_conv_loss: 0.3249 - avg_loss: 0.3420 - op_main_accuracy: 0.8377 - op_conv_accuracy: 0.8589 - avg_accuracy: 0.8585\n",
      "Epoch 00024: val_avg_accuracy improved from 0.84797 to 0.85080, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.4080 - op_main_loss: 0.3784 - op_conv_loss: 0.3249 - avg_loss: 0.3420 - op_main_accuracy: 0.8377 - op_conv_accuracy: 0.8589 - avg_accuracy: 0.8585 - val_loss: 1.4247 - val_op_main_loss: 0.3702 - val_op_conv_loss: 0.3454 - val_avg_loss: 0.3475 - val_op_main_accuracy: 0.8442 - val_op_conv_accuracy: 0.8508 - val_avg_accuracy: 0.8508\n",
      "Epoch 25/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.3813 - op_main_loss: 0.3716 - op_conv_loss: 0.3151 - avg_loss: 0.3335 - op_main_accuracy: 0.8454 - op_conv_accuracy: 0.8707 - avg_accuracy: 0.8660\n",
      "Epoch 00025: val_avg_accuracy did not improve from 0.85080\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3806 - op_main_loss: 0.3714 - op_conv_loss: 0.3148 - avg_loss: 0.3333 - op_main_accuracy: 0.8457 - op_conv_accuracy: 0.8710 - avg_accuracy: 0.8663 - val_loss: 1.5001 - val_op_main_loss: 0.3747 - val_op_conv_loss: 0.3955 - val_avg_loss: 0.3695 - val_op_main_accuracy: 0.8404 - val_op_conv_accuracy: 0.8244 - val_avg_accuracy: 0.8404\n",
      "Epoch 26/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.3662 - op_main_loss: 0.3683 - op_conv_loss: 0.3090 - avg_loss: 0.3292 - op_main_accuracy: 0.8481 - op_conv_accuracy: 0.8687 - avg_accuracy: 0.8636\n",
      "Epoch 00026: val_avg_accuracy improved from 0.85080 to 0.85364, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3633 - op_main_loss: 0.3672 - op_conv_loss: 0.3082 - avg_loss: 0.3283 - op_main_accuracy: 0.8490 - op_conv_accuracy: 0.8691 - avg_accuracy: 0.8646 - val_loss: 1.4123 - val_op_main_loss: 0.3616 - val_op_conv_loss: 0.3487 - val_avg_loss: 0.3430 - val_op_main_accuracy: 0.8499 - val_op_conv_accuracy: 0.8489 - val_avg_accuracy: 0.8536\n",
      "Epoch 27/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.3600 - op_main_loss: 0.3680 - op_conv_loss: 0.3072 - avg_loss: 0.3273 - op_main_accuracy: 0.8447 - op_conv_accuracy: 0.8721 - avg_accuracy: 0.8671\n",
      "Epoch 00027: val_avg_accuracy improved from 0.85364 to 0.85741, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3582 - op_main_loss: 0.3678 - op_conv_loss: 0.3062 - avg_loss: 0.3267 - op_main_accuracy: 0.8455 - op_conv_accuracy: 0.8729 - avg_accuracy: 0.8677 - val_loss: 1.4125 - val_op_main_loss: 0.3598 - val_op_conv_loss: 0.3575 - val_avg_loss: 0.3382 - val_op_main_accuracy: 0.8461 - val_op_conv_accuracy: 0.8395 - val_avg_accuracy: 0.8574\n",
      "Epoch 28/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.3327 - op_main_loss: 0.3591 - op_conv_loss: 0.2986 - avg_loss: 0.3181 - op_main_accuracy: 0.8500 - op_conv_accuracy: 0.8740 - avg_accuracy: 0.8694\n",
      "Epoch 00028: val_avg_accuracy improved from 0.85741 to 0.86497, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.3324 - op_main_loss: 0.3592 - op_conv_loss: 0.2984 - avg_loss: 0.3180 - op_main_accuracy: 0.8507 - op_conv_accuracy: 0.8748 - avg_accuracy: 0.8700 - val_loss: 1.3575 - val_op_main_loss: 0.3528 - val_op_conv_loss: 0.3229 - val_avg_loss: 0.3266 - val_op_main_accuracy: 0.8517 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8650\n",
      "Epoch 29/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.3116 - op_main_loss: 0.3545 - op_conv_loss: 0.2904 - avg_loss: 0.3115 - op_main_accuracy: 0.8570 - op_conv_accuracy: 0.8841 - avg_accuracy: 0.8793\n",
      "Epoch 00029: val_avg_accuracy did not improve from 0.86497\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3108 - op_main_loss: 0.3538 - op_conv_loss: 0.2906 - avg_loss: 0.3113 - op_main_accuracy: 0.8566 - op_conv_accuracy: 0.8837 - avg_accuracy: 0.8793 - val_loss: 1.3722 - val_op_main_loss: 0.3527 - val_op_conv_loss: 0.3348 - val_avg_loss: 0.3304 - val_op_main_accuracy: 0.8508 - val_op_conv_accuracy: 0.8584 - val_avg_accuracy: 0.8536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.3040 - op_main_loss: 0.3555 - op_conv_loss: 0.2856 - avg_loss: 0.3098 - op_main_accuracy: 0.8483 - op_conv_accuracy: 0.8825 - avg_accuracy: 0.8724\n",
      "Epoch 00030: val_avg_accuracy did not improve from 0.86497\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3036 - op_main_loss: 0.3556 - op_conv_loss: 0.2852 - avg_loss: 0.3097 - op_main_accuracy: 0.8476 - op_conv_accuracy: 0.8823 - avg_accuracy: 0.8719 - val_loss: 1.4253 - val_op_main_loss: 0.3588 - val_op_conv_loss: 0.3678 - val_avg_loss: 0.3459 - val_op_main_accuracy: 0.8470 - val_op_conv_accuracy: 0.8376 - val_avg_accuracy: 0.8489\n",
      "Epoch 31/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.2913 - op_main_loss: 0.3516 - op_conv_loss: 0.2815 - avg_loss: 0.3053 - op_main_accuracy: 0.8618 - op_conv_accuracy: 0.8832 - avg_accuracy: 0.8822\n",
      "Epoch 00031: val_avg_accuracy did not improve from 0.86497\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2961 - op_main_loss: 0.3527 - op_conv_loss: 0.2837 - avg_loss: 0.3068 - op_main_accuracy: 0.8603 - op_conv_accuracy: 0.8821 - avg_accuracy: 0.8814 - val_loss: 1.3967 - val_op_main_loss: 0.3538 - val_op_conv_loss: 0.3532 - val_avg_loss: 0.3373 - val_op_main_accuracy: 0.8470 - val_op_conv_accuracy: 0.8489 - val_avg_accuracy: 0.8527\n",
      "Epoch 32/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.2649 - op_main_loss: 0.3399 - op_conv_loss: 0.2757 - avg_loss: 0.2974 - op_main_accuracy: 0.8634 - op_conv_accuracy: 0.8888 - avg_accuracy: 0.8842\n",
      "Epoch 00032: val_avg_accuracy did not improve from 0.86497\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2617 - op_main_loss: 0.3389 - op_conv_loss: 0.2746 - avg_loss: 0.2964 - op_main_accuracy: 0.8639 - op_conv_accuracy: 0.8892 - avg_accuracy: 0.8842 - val_loss: 1.3594 - val_op_main_loss: 0.3515 - val_op_conv_loss: 0.3296 - val_avg_loss: 0.3263 - val_op_main_accuracy: 0.8527 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8602\n",
      "Epoch 33/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2599 - op_main_loss: 0.3388 - op_conv_loss: 0.2739 - avg_loss: 0.2954 - op_main_accuracy: 0.8667 - op_conv_accuracy: 0.8882 - avg_accuracy: 0.8885\n",
      "Epoch 00033: val_avg_accuracy did not improve from 0.86497\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2599 - op_main_loss: 0.3388 - op_conv_loss: 0.2739 - avg_loss: 0.2954 - op_main_accuracy: 0.8667 - op_conv_accuracy: 0.8882 - avg_accuracy: 0.8885 - val_loss: 1.4723 - val_op_main_loss: 0.3439 - val_op_conv_loss: 0.4262 - val_avg_loss: 0.3506 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8319 - val_avg_accuracy: 0.8480\n",
      "Epoch 34/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2538 - op_main_loss: 0.3347 - op_conv_loss: 0.2752 - avg_loss: 0.2926 - op_main_accuracy: 0.8665 - op_conv_accuracy: 0.8894 - avg_accuracy: 0.8856\n",
      "Epoch 00034: val_avg_accuracy improved from 0.86497 to 0.87252, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.2538 - op_main_loss: 0.3347 - op_conv_loss: 0.2752 - avg_loss: 0.2926 - op_main_accuracy: 0.8665 - op_conv_accuracy: 0.8894 - avg_accuracy: 0.8856 - val_loss: 1.3485 - val_op_main_loss: 0.3403 - val_op_conv_loss: 0.3377 - val_avg_loss: 0.3202 - val_op_main_accuracy: 0.8555 - val_op_conv_accuracy: 0.8555 - val_avg_accuracy: 0.8725\n",
      "Epoch 35/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.2340 - op_main_loss: 0.3324 - op_conv_loss: 0.2640 - avg_loss: 0.2867 - op_main_accuracy: 0.8665 - op_conv_accuracy: 0.8906 - avg_accuracy: 0.8871\n",
      "Epoch 00035: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2340 - op_main_loss: 0.3324 - op_conv_loss: 0.2640 - avg_loss: 0.2867 - op_main_accuracy: 0.8665 - op_conv_accuracy: 0.8906 - avg_accuracy: 0.8871 - val_loss: 1.4353 - val_op_main_loss: 0.3393 - val_op_conv_loss: 0.4037 - val_avg_loss: 0.3427 - val_op_main_accuracy: 0.8536 - val_op_conv_accuracy: 0.8083 - val_avg_accuracy: 0.8366\n",
      "Epoch 36/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.2046 - op_main_loss: 0.3232 - op_conv_loss: 0.2535 - avg_loss: 0.2783 - op_main_accuracy: 0.8709 - op_conv_accuracy: 0.8951 - avg_accuracy: 0.8900\n",
      "Epoch 00036: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2043 - op_main_loss: 0.3234 - op_conv_loss: 0.2531 - avg_loss: 0.2782 - op_main_accuracy: 0.8710 - op_conv_accuracy: 0.8951 - avg_accuracy: 0.8897 - val_loss: 1.3342 - val_op_main_loss: 0.3447 - val_op_conv_loss: 0.3229 - val_avg_loss: 0.3168 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8716\n",
      "Epoch 37/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.2081 - op_main_loss: 0.3265 - op_conv_loss: 0.2528 - avg_loss: 0.2790 - op_main_accuracy: 0.8692 - op_conv_accuracy: 0.9002 - avg_accuracy: 0.8935\n",
      "Epoch 00037: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2054 - op_main_loss: 0.3260 - op_conv_loss: 0.2515 - avg_loss: 0.2781 - op_main_accuracy: 0.8696 - op_conv_accuracy: 0.9010 - avg_accuracy: 0.8941 - val_loss: 1.7505 - val_op_main_loss: 0.3860 - val_op_conv_loss: 0.5835 - val_avg_loss: 0.4319 - val_op_main_accuracy: 0.8432 - val_op_conv_accuracy: 0.7856 - val_avg_accuracy: 0.8168\n",
      "Epoch 38/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.2029 - op_main_loss: 0.3247 - op_conv_loss: 0.2524 - avg_loss: 0.2768 - op_main_accuracy: 0.8695 - op_conv_accuracy: 0.8983 - avg_accuracy: 0.8916\n",
      "Epoch 00038: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.2045 - op_main_loss: 0.3255 - op_conv_loss: 0.2527 - avg_loss: 0.2774 - op_main_accuracy: 0.8693 - op_conv_accuracy: 0.8984 - avg_accuracy: 0.8918 - val_loss: 1.4598 - val_op_main_loss: 0.3490 - val_op_conv_loss: 0.4097 - val_avg_loss: 0.3517 - val_op_main_accuracy: 0.8536 - val_op_conv_accuracy: 0.8404 - val_avg_accuracy: 0.8480\n",
      "Epoch 39/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1923 - op_main_loss: 0.3172 - op_conv_loss: 0.2522 - avg_loss: 0.2743 - op_main_accuracy: 0.8723 - op_conv_accuracy: 0.9000 - avg_accuracy: 0.8985\n",
      "Epoch 00039: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1909 - op_main_loss: 0.3166 - op_conv_loss: 0.2518 - avg_loss: 0.2739 - op_main_accuracy: 0.8726 - op_conv_accuracy: 0.8998 - avg_accuracy: 0.8984 - val_loss: 1.4487 - val_op_main_loss: 0.3554 - val_op_conv_loss: 0.3935 - val_avg_loss: 0.3518 - val_op_main_accuracy: 0.8508 - val_op_conv_accuracy: 0.8442 - val_avg_accuracy: 0.8499\n",
      "Epoch 40/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.1966 - op_main_loss: 0.3237 - op_conv_loss: 0.2497 - avg_loss: 0.2757 - op_main_accuracy: 0.8698 - op_conv_accuracy: 0.8962 - avg_accuracy: 0.8929\n",
      "Epoch 00040: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1941 - op_main_loss: 0.3228 - op_conv_loss: 0.2488 - avg_loss: 0.2749 - op_main_accuracy: 0.8705 - op_conv_accuracy: 0.8967 - avg_accuracy: 0.8937 - val_loss: 1.2973 - val_op_main_loss: 0.3306 - val_op_conv_loss: 0.3136 - val_avg_loss: 0.3065 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8631 - val_avg_accuracy: 0.8706\n",
      "Epoch 41/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1751 - op_main_loss: 0.3160 - op_conv_loss: 0.2439 - avg_loss: 0.2690 - op_main_accuracy: 0.8789 - op_conv_accuracy: 0.9060 - avg_accuracy: 0.9004\n",
      "Epoch 00041: val_avg_accuracy did not improve from 0.87252\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1764 - op_main_loss: 0.3167 - op_conv_loss: 0.2440 - avg_loss: 0.2695 - op_main_accuracy: 0.8774 - op_conv_accuracy: 0.9055 - avg_accuracy: 0.8998 - val_loss: 1.3218 - val_op_main_loss: 0.3321 - val_op_conv_loss: 0.3309 - val_avg_loss: 0.3129 - val_op_main_accuracy: 0.8584 - val_op_conv_accuracy: 0.8602 - val_avg_accuracy: 0.8621\n",
      "Epoch 42/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 1.1717 - op_main_loss: 0.3173 - op_conv_loss: 0.2406 - avg_loss: 0.2678 - op_main_accuracy: 0.8755 - op_conv_accuracy: 0.8990 - avg_accuracy: 0.8983\n",
      "Epoch 00042: val_avg_accuracy improved from 0.87252 to 0.87441, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1688 - op_main_loss: 0.3163 - op_conv_loss: 0.2397 - avg_loss: 0.2668 - op_main_accuracy: 0.8757 - op_conv_accuracy: 0.8996 - avg_accuracy: 0.8989 - val_loss: 1.2951 - val_op_main_loss: 0.3326 - val_op_conv_loss: 0.3119 - val_avg_loss: 0.3056 - val_op_main_accuracy: 0.8584 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8744\n",
      "Epoch 43/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1695 - op_main_loss: 0.3143 - op_conv_loss: 0.2424 - avg_loss: 0.2679 - op_main_accuracy: 0.8738 - op_conv_accuracy: 0.9038 - avg_accuracy: 0.8989\n",
      "Epoch 00043: val_avg_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1695 - op_main_loss: 0.3143 - op_conv_loss: 0.2424 - avg_loss: 0.2679 - op_main_accuracy: 0.8738 - op_conv_accuracy: 0.9038 - avg_accuracy: 0.8989 - val_loss: 1.2871 - val_op_main_loss: 0.3278 - val_op_conv_loss: 0.3124 - val_avg_loss: 0.3029 - val_op_main_accuracy: 0.8565 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8725\n",
      "Epoch 44/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.1417 - op_main_loss: 0.3074 - op_conv_loss: 0.2311 - avg_loss: 0.2587 - op_main_accuracy: 0.8793 - op_conv_accuracy: 0.9071 - avg_accuracy: 0.9060\n",
      "Epoch 00044: val_avg_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1417 - op_main_loss: 0.3074 - op_conv_loss: 0.2311 - avg_loss: 0.2587 - op_main_accuracy: 0.8793 - op_conv_accuracy: 0.9071 - avg_accuracy: 0.9060 - val_loss: 1.3074 - val_op_main_loss: 0.3309 - val_op_conv_loss: 0.3227 - val_avg_loss: 0.3088 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8706\n",
      "Epoch 45/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1355 - op_main_loss: 0.3035 - op_conv_loss: 0.2308 - avg_loss: 0.2563 - op_main_accuracy: 0.8835 - op_conv_accuracy: 0.9109 - avg_accuracy: 0.9116\n",
      "Epoch 00045: val_avg_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1401 - op_main_loss: 0.3052 - op_conv_loss: 0.2322 - avg_loss: 0.2578 - op_main_accuracy: 0.8823 - op_conv_accuracy: 0.9097 - avg_accuracy: 0.9107 - val_loss: 1.2957 - val_op_main_loss: 0.3277 - val_op_conv_loss: 0.3171 - val_avg_loss: 0.3064 - val_op_main_accuracy: 0.8584 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8669\n",
      "Epoch 46/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.1111 - op_main_loss: 0.2976 - op_conv_loss: 0.2201 - avg_loss: 0.2483 - op_main_accuracy: 0.8871 - op_conv_accuracy: 0.9172 - avg_accuracy: 0.9125\n",
      "Epoch 00046: val_avg_accuracy did not improve from 0.87441\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1080 - op_main_loss: 0.2966 - op_conv_loss: 0.2189 - avg_loss: 0.2474 - op_main_accuracy: 0.8882 - op_conv_accuracy: 0.9175 - avg_accuracy: 0.9130 - val_loss: 1.3051 - val_op_main_loss: 0.3245 - val_op_conv_loss: 0.3287 - val_avg_loss: 0.3069 - val_op_main_accuracy: 0.8584 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8687\n",
      "Epoch 47/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.1107 - op_main_loss: 0.2972 - op_conv_loss: 0.2208 - avg_loss: 0.2484 - op_main_accuracy: 0.8829 - op_conv_accuracy: 0.9132 - avg_accuracy: 0.9108\n",
      "Epoch 00047: val_avg_accuracy improved from 0.87441 to 0.87535, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.1095 - op_main_loss: 0.2970 - op_conv_loss: 0.2202 - avg_loss: 0.2480 - op_main_accuracy: 0.8830 - op_conv_accuracy: 0.9130 - avg_accuracy: 0.9107 - val_loss: 1.2694 - val_op_main_loss: 0.3197 - val_op_conv_loss: 0.3086 - val_avg_loss: 0.2968 - val_op_main_accuracy: 0.8602 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8754\n",
      "Epoch 48/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.1154 - op_main_loss: 0.2979 - op_conv_loss: 0.2245 - avg_loss: 0.2493 - op_main_accuracy: 0.8847 - op_conv_accuracy: 0.9119 - avg_accuracy: 0.9086\n",
      "Epoch 00048: val_avg_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1155 - op_main_loss: 0.2979 - op_conv_loss: 0.2245 - avg_loss: 0.2493 - op_main_accuracy: 0.8847 - op_conv_accuracy: 0.9119 - avg_accuracy: 0.9086 - val_loss: 1.2877 - val_op_main_loss: 0.3221 - val_op_conv_loss: 0.3203 - val_avg_loss: 0.3025 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8687 - val_avg_accuracy: 0.8744\n",
      "Epoch 49/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0992 - op_main_loss: 0.2939 - op_conv_loss: 0.2178 - avg_loss: 0.2448 - op_main_accuracy: 0.8880 - op_conv_accuracy: 0.9125 - avg_accuracy: 0.9082\n",
      "Epoch 00049: val_avg_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0974 - op_main_loss: 0.2933 - op_conv_loss: 0.2172 - avg_loss: 0.2443 - op_main_accuracy: 0.8878 - op_conv_accuracy: 0.9121 - avg_accuracy: 0.9078 - val_loss: 1.3145 - val_op_main_loss: 0.3370 - val_op_conv_loss: 0.3231 - val_avg_loss: 0.3125 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8744 - val_avg_accuracy: 0.8659\n",
      "Epoch 50/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0883 - op_main_loss: 0.2914 - op_conv_loss: 0.2133 - avg_loss: 0.2414 - op_main_accuracy: 0.8868 - op_conv_accuracy: 0.9132 - avg_accuracy: 0.9089\n",
      "Epoch 00050: val_avg_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0860 - op_main_loss: 0.2908 - op_conv_loss: 0.2123 - avg_loss: 0.2407 - op_main_accuracy: 0.8868 - op_conv_accuracy: 0.9133 - avg_accuracy: 0.9090 - val_loss: 1.3219 - val_op_main_loss: 0.3368 - val_op_conv_loss: 0.3296 - val_avg_loss: 0.3144 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8669\n",
      "Epoch 51/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0878 - op_main_loss: 0.2881 - op_conv_loss: 0.2165 - avg_loss: 0.2419 - op_main_accuracy: 0.8888 - op_conv_accuracy: 0.9113 - avg_accuracy: 0.9099\n",
      "Epoch 00051: val_avg_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0922 - op_main_loss: 0.2888 - op_conv_loss: 0.2187 - avg_loss: 0.2433 - op_main_accuracy: 0.8885 - op_conv_accuracy: 0.9107 - avg_accuracy: 0.9093 - val_loss: 1.3413 - val_op_main_loss: 0.3311 - val_op_conv_loss: 0.3492 - val_avg_loss: 0.3192 - val_op_main_accuracy: 0.8602 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8678\n",
      "Epoch 52/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.0854 - op_main_loss: 0.2899 - op_conv_loss: 0.2133 - avg_loss: 0.2404 - op_main_accuracy: 0.8823 - op_conv_accuracy: 0.9148 - avg_accuracy: 0.9096\n",
      "Epoch 00052: val_avg_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0848 - op_main_loss: 0.2896 - op_conv_loss: 0.2131 - avg_loss: 0.2402 - op_main_accuracy: 0.8826 - op_conv_accuracy: 0.9147 - avg_accuracy: 0.9097 - val_loss: 1.2799 - val_op_main_loss: 0.3162 - val_op_conv_loss: 0.3238 - val_avg_loss: 0.2978 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8754\n",
      "Epoch 53/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0798 - op_main_loss: 0.2883 - op_conv_loss: 0.2106 - avg_loss: 0.2391 - op_main_accuracy: 0.8854 - op_conv_accuracy: 0.9169 - avg_accuracy: 0.9075\n",
      "Epoch 00053: val_avg_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0869 - op_main_loss: 0.2902 - op_conv_loss: 0.2135 - avg_loss: 0.2413 - op_main_accuracy: 0.8837 - op_conv_accuracy: 0.9154 - avg_accuracy: 0.9064 - val_loss: 1.3277 - val_op_main_loss: 0.3260 - val_op_conv_loss: 0.3459 - val_avg_loss: 0.3154 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 1.0841 - op_main_loss: 0.2903 - op_conv_loss: 0.2138 - avg_loss: 0.2395 - op_main_accuracy: 0.8873 - op_conv_accuracy: 0.9166 - avg_accuracy: 0.9126\n",
      "Epoch 00054: val_avg_accuracy did not improve from 0.87535\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0841 - op_main_loss: 0.2903 - op_conv_loss: 0.2138 - avg_loss: 0.2395 - op_main_accuracy: 0.8873 - op_conv_accuracy: 0.9166 - avg_accuracy: 0.9126 - val_loss: 1.3334 - val_op_main_loss: 0.3343 - val_op_conv_loss: 0.3402 - val_avg_loss: 0.3186 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8659\n",
      "Epoch 55/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0704 - op_main_loss: 0.2838 - op_conv_loss: 0.2093 - avg_loss: 0.2363 - op_main_accuracy: 0.8893 - op_conv_accuracy: 0.9150 - avg_accuracy: 0.9133\n",
      "Epoch 00055: val_avg_accuracy improved from 0.87535 to 0.88196, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0712 - op_main_loss: 0.2846 - op_conv_loss: 0.2091 - avg_loss: 0.2365 - op_main_accuracy: 0.8882 - op_conv_accuracy: 0.9149 - avg_accuracy: 0.9128 - val_loss: 1.2964 - val_op_main_loss: 0.3277 - val_op_conv_loss: 0.3235 - val_avg_loss: 0.3045 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8820\n",
      "Epoch 56/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.0650 - op_main_loss: 0.2876 - op_conv_loss: 0.2031 - avg_loss: 0.2345 - op_main_accuracy: 0.8842 - op_conv_accuracy: 0.9167 - avg_accuracy: 0.9112\n",
      "Epoch 00056: val_avg_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0651 - op_main_loss: 0.2874 - op_conv_loss: 0.2034 - avg_loss: 0.2345 - op_main_accuracy: 0.8845 - op_conv_accuracy: 0.9164 - avg_accuracy: 0.9112 - val_loss: 1.3091 - val_op_main_loss: 0.3157 - val_op_conv_loss: 0.3479 - val_avg_loss: 0.3065 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8687 - val_avg_accuracy: 0.8735\n",
      "Epoch 57/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.0543 - op_main_loss: 0.2797 - op_conv_loss: 0.2039 - avg_loss: 0.2312 - op_main_accuracy: 0.8923 - op_conv_accuracy: 0.9169 - avg_accuracy: 0.9164\n",
      "Epoch 00057: val_avg_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0536 - op_main_loss: 0.2795 - op_conv_loss: 0.2036 - avg_loss: 0.2310 - op_main_accuracy: 0.8925 - op_conv_accuracy: 0.9171 - avg_accuracy: 0.9166 - val_loss: 1.6819 - val_op_main_loss: 0.3736 - val_op_conv_loss: 0.5541 - val_avg_loss: 0.4148 - val_op_main_accuracy: 0.8499 - val_op_conv_accuracy: 0.8093 - val_avg_accuracy: 0.8225\n",
      "Epoch 58/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0534 - op_main_loss: 0.2790 - op_conv_loss: 0.2043 - avg_loss: 0.2316 - op_main_accuracy: 0.8925 - op_conv_accuracy: 0.9161 - avg_accuracy: 0.9111\n",
      "Epoch 00058: val_avg_accuracy improved from 0.88196 to 0.88291, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0531 - op_main_loss: 0.2785 - op_conv_loss: 0.2045 - avg_loss: 0.2315 - op_main_accuracy: 0.8922 - op_conv_accuracy: 0.9161 - avg_accuracy: 0.9112 - val_loss: 1.2509 - val_op_main_loss: 0.3135 - val_op_conv_loss: 0.3070 - val_avg_loss: 0.2922 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8829\n",
      "Epoch 59/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.0505 - op_main_loss: 0.2785 - op_conv_loss: 0.2032 - avg_loss: 0.2309 - op_main_accuracy: 0.8911 - op_conv_accuracy: 0.9167 - avg_accuracy: 0.9141\n",
      "Epoch 00059: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0502 - op_main_loss: 0.2783 - op_conv_loss: 0.2031 - avg_loss: 0.2308 - op_main_accuracy: 0.8913 - op_conv_accuracy: 0.9166 - avg_accuracy: 0.9140 - val_loss: 1.3185 - val_op_main_loss: 0.3322 - val_op_conv_loss: 0.3337 - val_avg_loss: 0.3156 - val_op_main_accuracy: 0.8602 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8791\n",
      "Epoch 60/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0432 - op_main_loss: 0.2766 - op_conv_loss: 0.2010 - avg_loss: 0.2290 - op_main_accuracy: 0.8941 - op_conv_accuracy: 0.9213 - avg_accuracy: 0.9198\n",
      "Epoch 00060: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0376 - op_main_loss: 0.2750 - op_conv_loss: 0.1988 - avg_loss: 0.2272 - op_main_accuracy: 0.8951 - op_conv_accuracy: 0.9223 - avg_accuracy: 0.9204 - val_loss: 1.3240 - val_op_main_loss: 0.3291 - val_op_conv_loss: 0.3422 - val_avg_loss: 0.3165 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8650\n",
      "Epoch 61/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.0623 - op_main_loss: 0.2824 - op_conv_loss: 0.2099 - avg_loss: 0.2343 - op_main_accuracy: 0.8902 - op_conv_accuracy: 0.9157 - avg_accuracy: 0.9138\n",
      "Epoch 00061: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0621 - op_main_loss: 0.2824 - op_conv_loss: 0.2097 - avg_loss: 0.2342 - op_main_accuracy: 0.8901 - op_conv_accuracy: 0.9156 - avg_accuracy: 0.9138 - val_loss: 1.2873 - val_op_main_loss: 0.3159 - val_op_conv_loss: 0.3334 - val_avg_loss: 0.3011 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8791\n",
      "Epoch 62/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.0328 - op_main_loss: 0.2719 - op_conv_loss: 0.1982 - avg_loss: 0.2254 - op_main_accuracy: 0.8991 - op_conv_accuracy: 0.9231 - avg_accuracy: 0.9221\n",
      "Epoch 00062: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0352 - op_main_loss: 0.2728 - op_conv_loss: 0.1989 - avg_loss: 0.2262 - op_main_accuracy: 0.8989 - op_conv_accuracy: 0.9227 - avg_accuracy: 0.9218 - val_loss: 1.3164 - val_op_main_loss: 0.3129 - val_op_conv_loss: 0.3592 - val_avg_loss: 0.3078 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8527 - val_avg_accuracy: 0.8669\n",
      "Epoch 63/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0194 - op_main_loss: 0.2678 - op_conv_loss: 0.1935 - avg_loss: 0.2210 - op_main_accuracy: 0.8915 - op_conv_accuracy: 0.9225 - avg_accuracy: 0.9188\n",
      "Epoch 00063: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0162 - op_main_loss: 0.2670 - op_conv_loss: 0.1921 - avg_loss: 0.2200 - op_main_accuracy: 0.8925 - op_conv_accuracy: 0.9230 - avg_accuracy: 0.9197 - val_loss: 1.3165 - val_op_main_loss: 0.3220 - val_op_conv_loss: 0.3457 - val_avg_loss: 0.3114 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8744\n",
      "Epoch 64/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 1.0274 - op_main_loss: 0.2720 - op_conv_loss: 0.1956 - avg_loss: 0.2236 - op_main_accuracy: 0.8970 - op_conv_accuracy: 0.9219 - avg_accuracy: 0.9202\n",
      "Epoch 00064: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0280 - op_main_loss: 0.2721 - op_conv_loss: 0.1959 - avg_loss: 0.2238 - op_main_accuracy: 0.8970 - op_conv_accuracy: 0.9218 - avg_accuracy: 0.9201 - val_loss: 1.2895 - val_op_main_loss: 0.3247 - val_op_conv_loss: 0.3241 - val_avg_loss: 0.3054 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8735\n",
      "Epoch 65/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 1.0074 - op_main_loss: 0.2643 - op_conv_loss: 0.1902 - avg_loss: 0.2176 - op_main_accuracy: 0.8988 - op_conv_accuracy: 0.9303 - avg_accuracy: 0.9255\n",
      "Epoch 00065: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0030 - op_main_loss: 0.2630 - op_conv_loss: 0.1885 - avg_loss: 0.2162 - op_main_accuracy: 0.8996 - op_conv_accuracy: 0.9310 - avg_accuracy: 0.9263 - val_loss: 1.3139 - val_op_main_loss: 0.3131 - val_op_conv_loss: 0.3575 - val_avg_loss: 0.3086 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8659 - val_avg_accuracy: 0.8763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 1.0014 - op_main_loss: 0.2662 - op_conv_loss: 0.1848 - avg_loss: 0.2157 - op_main_accuracy: 0.8970 - op_conv_accuracy: 0.9288 - avg_accuracy: 0.9232\n",
      "Epoch 00066: val_avg_accuracy did not improve from 0.88291\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0007 - op_main_loss: 0.2658 - op_conv_loss: 0.1848 - avg_loss: 0.2154 - op_main_accuracy: 0.8967 - op_conv_accuracy: 0.9289 - avg_accuracy: 0.9232 - val_loss: 1.3079 - val_op_main_loss: 0.3256 - val_op_conv_loss: 0.3367 - val_avg_loss: 0.3110 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8754\n",
      "Epoch 67/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9991 - op_main_loss: 0.2632 - op_conv_loss: 0.1865 - avg_loss: 0.2148 - op_main_accuracy: 0.9020 - op_conv_accuracy: 0.9250 - avg_accuracy: 0.9238\n",
      "Epoch 00067: val_avg_accuracy improved from 0.88291 to 0.88574, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 1.0001 - op_main_loss: 0.2635 - op_conv_loss: 0.1868 - avg_loss: 0.2152 - op_main_accuracy: 0.9017 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9234 - val_loss: 1.2469 - val_op_main_loss: 0.3064 - val_op_conv_loss: 0.3166 - val_avg_loss: 0.2897 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8857\n",
      "Epoch 68/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9999 - op_main_loss: 0.2620 - op_conv_loss: 0.1876 - avg_loss: 0.2152 - op_main_accuracy: 0.9002 - op_conv_accuracy: 0.9245 - avg_accuracy: 0.9216\n",
      "Epoch 00068: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9991 - op_main_loss: 0.2615 - op_conv_loss: 0.1875 - avg_loss: 0.2149 - op_main_accuracy: 0.9008 - op_conv_accuracy: 0.9244 - avg_accuracy: 0.9223 - val_loss: 1.3534 - val_op_main_loss: 0.3320 - val_op_conv_loss: 0.3615 - val_avg_loss: 0.3243 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8669 - val_avg_accuracy: 0.8659\n",
      "Epoch 69/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9884 - op_main_loss: 0.2592 - op_conv_loss: 0.1827 - avg_loss: 0.2113 - op_main_accuracy: 0.9012 - op_conv_accuracy: 0.9276 - avg_accuracy: 0.9250\n",
      "Epoch 00069: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9885 - op_main_loss: 0.2588 - op_conv_loss: 0.1832 - avg_loss: 0.2113 - op_main_accuracy: 0.9012 - op_conv_accuracy: 0.9272 - avg_accuracy: 0.9251 - val_loss: 1.3265 - val_op_main_loss: 0.3346 - val_op_conv_loss: 0.3402 - val_avg_loss: 0.3164 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8763\n",
      "Epoch 70/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9866 - op_main_loss: 0.2567 - op_conv_loss: 0.1843 - avg_loss: 0.2112 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9250 - avg_accuracy: 0.9235\n",
      "Epoch 00070: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9856 - op_main_loss: 0.2563 - op_conv_loss: 0.1840 - avg_loss: 0.2109 - op_main_accuracy: 0.9050 - op_conv_accuracy: 0.9251 - avg_accuracy: 0.9237 - val_loss: 1.3476 - val_op_main_loss: 0.3302 - val_op_conv_loss: 0.3604 - val_avg_loss: 0.3224 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8687\n",
      "Epoch 71/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9917 - op_main_loss: 0.2597 - op_conv_loss: 0.1853 - avg_loss: 0.2130 - op_main_accuracy: 0.9021 - op_conv_accuracy: 0.9251 - avg_accuracy: 0.9249\n",
      "Epoch 00071: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9946 - op_main_loss: 0.2613 - op_conv_loss: 0.1856 - avg_loss: 0.2139 - op_main_accuracy: 0.9015 - op_conv_accuracy: 0.9251 - avg_accuracy: 0.9241 - val_loss: 1.3361 - val_op_main_loss: 0.3418 - val_op_conv_loss: 0.3392 - val_avg_loss: 0.3224 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8716\n",
      "Epoch 72/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9866 - op_main_loss: 0.2616 - op_conv_loss: 0.1797 - avg_loss: 0.2117 - op_main_accuracy: 0.8977 - op_conv_accuracy: 0.9249 - avg_accuracy: 0.9244\n",
      "Epoch 00072: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9866 - op_main_loss: 0.2616 - op_conv_loss: 0.1797 - avg_loss: 0.2117 - op_main_accuracy: 0.8977 - op_conv_accuracy: 0.9249 - avg_accuracy: 0.9244 - val_loss: 1.3736 - val_op_main_loss: 0.3353 - val_op_conv_loss: 0.3737 - val_avg_loss: 0.3306 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8678\n",
      "Epoch 73/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9759 - op_main_loss: 0.2573 - op_conv_loss: 0.1773 - avg_loss: 0.2076 - op_main_accuracy: 0.9034 - op_conv_accuracy: 0.9337 - avg_accuracy: 0.9299\n",
      "Epoch 00073: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9752 - op_main_loss: 0.2571 - op_conv_loss: 0.1770 - avg_loss: 0.2073 - op_main_accuracy: 0.9036 - op_conv_accuracy: 0.9338 - avg_accuracy: 0.9301 - val_loss: 1.4291 - val_op_main_loss: 0.3576 - val_op_conv_loss: 0.3882 - val_avg_loss: 0.3498 - val_op_main_accuracy: 0.8565 - val_op_conv_accuracy: 0.8640 - val_avg_accuracy: 0.8650\n",
      "Epoch 74/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9682 - op_main_loss: 0.2543 - op_conv_loss: 0.1756 - avg_loss: 0.2051 - op_main_accuracy: 0.9055 - op_conv_accuracy: 0.9334 - avg_accuracy: 0.9265\n",
      "Epoch 00074: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9682 - op_main_loss: 0.2543 - op_conv_loss: 0.1756 - avg_loss: 0.2051 - op_main_accuracy: 0.9055 - op_conv_accuracy: 0.9334 - avg_accuracy: 0.9265 - val_loss: 1.3380 - val_op_main_loss: 0.3224 - val_op_conv_loss: 0.3641 - val_avg_loss: 0.3181 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8791\n",
      "Epoch 75/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9745 - op_main_loss: 0.2551 - op_conv_loss: 0.1799 - avg_loss: 0.2063 - op_main_accuracy: 0.9065 - op_conv_accuracy: 0.9292 - avg_accuracy: 0.9283\n",
      "Epoch 00075: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9744 - op_main_loss: 0.2553 - op_conv_loss: 0.1798 - avg_loss: 0.2063 - op_main_accuracy: 0.9062 - op_conv_accuracy: 0.9293 - avg_accuracy: 0.9282 - val_loss: 1.3293 - val_op_main_loss: 0.3224 - val_op_conv_loss: 0.3585 - val_avg_loss: 0.3164 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8772\n",
      "Epoch 76/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9774 - op_main_loss: 0.2583 - op_conv_loss: 0.1787 - avg_loss: 0.2086 - op_main_accuracy: 0.9001 - op_conv_accuracy: 0.9271 - avg_accuracy: 0.9235\n",
      "Epoch 00076: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9787 - op_main_loss: 0.2588 - op_conv_loss: 0.1791 - avg_loss: 0.2090 - op_main_accuracy: 0.8996 - op_conv_accuracy: 0.9265 - avg_accuracy: 0.9230 - val_loss: 1.2954 - val_op_main_loss: 0.3128 - val_op_conv_loss: 0.3459 - val_avg_loss: 0.3060 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8829\n",
      "Epoch 77/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9698 - op_main_loss: 0.2520 - op_conv_loss: 0.1806 - avg_loss: 0.2065 - op_main_accuracy: 0.9058 - op_conv_accuracy: 0.9262 - avg_accuracy: 0.9255\n",
      "Epoch 00077: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9690 - op_main_loss: 0.2514 - op_conv_loss: 0.1806 - avg_loss: 0.2062 - op_main_accuracy: 0.9064 - op_conv_accuracy: 0.9263 - avg_accuracy: 0.9256 - val_loss: 1.2426 - val_op_main_loss: 0.3044 - val_op_conv_loss: 0.3179 - val_avg_loss: 0.2894 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8848\n",
      "Epoch 78/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.9685 - op_main_loss: 0.2573 - op_conv_loss: 0.1747 - avg_loss: 0.2059 - op_main_accuracy: 0.8984 - op_conv_accuracy: 0.9285 - avg_accuracy: 0.9250\n",
      "Epoch 00078: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9679 - op_main_loss: 0.2571 - op_conv_loss: 0.1745 - avg_loss: 0.2057 - op_main_accuracy: 0.8986 - op_conv_accuracy: 0.9286 - avg_accuracy: 0.9251 - val_loss: 1.3061 - val_op_main_loss: 0.3141 - val_op_conv_loss: 0.3534 - val_avg_loss: 0.3083 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8791\n",
      "Epoch 79/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9579 - op_main_loss: 0.2497 - op_conv_loss: 0.1746 - avg_loss: 0.2034 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9332 - avg_accuracy: 0.9294\n",
      "Epoch 00079: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9593 - op_main_loss: 0.2497 - op_conv_loss: 0.1756 - avg_loss: 0.2037 - op_main_accuracy: 0.9041 - op_conv_accuracy: 0.9329 - avg_accuracy: 0.9289 - val_loss: 1.2905 - val_op_main_loss: 0.3214 - val_op_conv_loss: 0.3328 - val_avg_loss: 0.3067 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8772\n",
      "Epoch 80/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9509 - op_main_loss: 0.2497 - op_conv_loss: 0.1714 - avg_loss: 0.2002 - op_main_accuracy: 0.9057 - op_conv_accuracy: 0.9350 - avg_accuracy: 0.9315\n",
      "Epoch 00080: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9509 - op_main_loss: 0.2497 - op_conv_loss: 0.1714 - avg_loss: 0.2002 - op_main_accuracy: 0.9057 - op_conv_accuracy: 0.9350 - avg_accuracy: 0.9315 - val_loss: 1.3370 - val_op_main_loss: 0.3219 - val_op_conv_loss: 0.3685 - val_avg_loss: 0.3184 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8659 - val_avg_accuracy: 0.8706\n",
      "Epoch 81/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9299 - op_main_loss: 0.2444 - op_conv_loss: 0.1619 - avg_loss: 0.1947 - op_main_accuracy: 0.9108 - op_conv_accuracy: 0.9389 - avg_accuracy: 0.9330\n",
      "Epoch 00081: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9292 - op_main_loss: 0.2441 - op_conv_loss: 0.1616 - avg_loss: 0.1945 - op_main_accuracy: 0.9112 - op_conv_accuracy: 0.9386 - avg_accuracy: 0.9331 - val_loss: 1.3578 - val_op_main_loss: 0.3300 - val_op_conv_loss: 0.3725 - val_avg_loss: 0.3256 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8659\n",
      "Epoch 82/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9468 - op_main_loss: 0.2454 - op_conv_loss: 0.1722 - avg_loss: 0.2000 - op_main_accuracy: 0.9079 - op_conv_accuracy: 0.9351 - avg_accuracy: 0.9329\n",
      "Epoch 00082: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9506 - op_main_loss: 0.2466 - op_conv_loss: 0.1736 - avg_loss: 0.2013 - op_main_accuracy: 0.9067 - op_conv_accuracy: 0.9341 - avg_accuracy: 0.9315 - val_loss: 1.2665 - val_op_main_loss: 0.3072 - val_op_conv_loss: 0.3350 - val_avg_loss: 0.2955 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8857\n",
      "Epoch 83/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9636 - op_main_loss: 0.2524 - op_conv_loss: 0.1776 - avg_loss: 0.2050 - op_main_accuracy: 0.9019 - op_conv_accuracy: 0.9305 - avg_accuracy: 0.9272\n",
      "Epoch 00083: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9616 - op_main_loss: 0.2519 - op_conv_loss: 0.1768 - avg_loss: 0.2044 - op_main_accuracy: 0.9022 - op_conv_accuracy: 0.9308 - avg_accuracy: 0.9272 - val_loss: 1.2935 - val_op_main_loss: 0.3112 - val_op_conv_loss: 0.3518 - val_avg_loss: 0.3025 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8669 - val_avg_accuracy: 0.8754\n",
      "Epoch 84/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9463 - op_main_loss: 0.2461 - op_conv_loss: 0.1727 - avg_loss: 0.1998 - op_main_accuracy: 0.9088 - op_conv_accuracy: 0.9350 - avg_accuracy: 0.9327\n",
      "Epoch 00084: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9463 - op_main_loss: 0.2461 - op_conv_loss: 0.1727 - avg_loss: 0.1998 - op_main_accuracy: 0.9088 - op_conv_accuracy: 0.9350 - avg_accuracy: 0.9327 - val_loss: 1.2349 - val_op_main_loss: 0.3018 - val_op_conv_loss: 0.3180 - val_avg_loss: 0.2877 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8857\n",
      "Epoch 85/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9360 - op_main_loss: 0.2423 - op_conv_loss: 0.1699 - avg_loss: 0.1962 - op_main_accuracy: 0.9123 - op_conv_accuracy: 0.9369 - avg_accuracy: 0.9341\n",
      "Epoch 00085: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9360 - op_main_loss: 0.2423 - op_conv_loss: 0.1699 - avg_loss: 0.1962 - op_main_accuracy: 0.9123 - op_conv_accuracy: 0.9369 - avg_accuracy: 0.9341 - val_loss: 1.2745 - val_op_main_loss: 0.3121 - val_op_conv_loss: 0.3348 - val_avg_loss: 0.3011 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8810\n",
      "Epoch 86/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9256 - op_main_loss: 0.2409 - op_conv_loss: 0.1643 - avg_loss: 0.1937 - op_main_accuracy: 0.9117 - op_conv_accuracy: 0.9404 - avg_accuracy: 0.9365\n",
      "Epoch 00086: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9261 - op_main_loss: 0.2410 - op_conv_loss: 0.1645 - avg_loss: 0.1939 - op_main_accuracy: 0.9114 - op_conv_accuracy: 0.9400 - avg_accuracy: 0.9360 - val_loss: 1.4683 - val_op_main_loss: 0.3352 - val_op_conv_loss: 0.4530 - val_avg_loss: 0.3536 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8480 - val_avg_accuracy: 0.8536\n",
      "Epoch 87/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.9162 - op_main_loss: 0.2366 - op_conv_loss: 0.1623 - avg_loss: 0.1901 - op_main_accuracy: 0.9138 - op_conv_accuracy: 0.9380 - avg_accuracy: 0.9366\n",
      "Epoch 00087: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9156 - op_main_loss: 0.2365 - op_conv_loss: 0.1621 - avg_loss: 0.1899 - op_main_accuracy: 0.9140 - op_conv_accuracy: 0.9381 - avg_accuracy: 0.9367 - val_loss: 1.3215 - val_op_main_loss: 0.3310 - val_op_conv_loss: 0.3470 - val_avg_loss: 0.3165 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8735\n",
      "Epoch 88/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9637 - op_main_loss: 0.2520 - op_conv_loss: 0.1798 - avg_loss: 0.2057 - op_main_accuracy: 0.9058 - op_conv_accuracy: 0.9280 - avg_accuracy: 0.9241\n",
      "Epoch 00088: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9618 - op_main_loss: 0.2516 - op_conv_loss: 0.1789 - avg_loss: 0.2051 - op_main_accuracy: 0.9062 - op_conv_accuracy: 0.9284 - avg_accuracy: 0.9246 - val_loss: 1.3154 - val_op_main_loss: 0.3267 - val_op_conv_loss: 0.3511 - val_avg_loss: 0.3113 - val_op_main_accuracy: 0.8508 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8706\n",
      "Epoch 89/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9426 - op_main_loss: 0.2500 - op_conv_loss: 0.1683 - avg_loss: 0.1992 - op_main_accuracy: 0.9015 - op_conv_accuracy: 0.9362 - avg_accuracy: 0.9308\n",
      "Epoch 00089: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9426 - op_main_loss: 0.2500 - op_conv_loss: 0.1683 - avg_loss: 0.1992 - op_main_accuracy: 0.9015 - op_conv_accuracy: 0.9362 - avg_accuracy: 0.9308 - val_loss: 1.2234 - val_op_main_loss: 0.2968 - val_op_conv_loss: 0.3184 - val_avg_loss: 0.2833 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8839\n",
      "Epoch 90/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.9294 - op_main_loss: 0.2436 - op_conv_loss: 0.1654 - avg_loss: 0.1948 - op_main_accuracy: 0.9096 - op_conv_accuracy: 0.9349 - avg_accuracy: 0.9351\n",
      "Epoch 00090: val_avg_accuracy did not improve from 0.88574\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9315 - op_main_loss: 0.2441 - op_conv_loss: 0.1662 - avg_loss: 0.1955 - op_main_accuracy: 0.9090 - op_conv_accuracy: 0.9345 - avg_accuracy: 0.9345 - val_loss: 1.2520 - val_op_main_loss: 0.2974 - val_op_conv_loss: 0.3389 - val_avg_loss: 0.2899 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8857\n",
      "Epoch 91/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.9147 - op_main_loss: 0.2378 - op_conv_loss: 0.1614 - avg_loss: 0.1909 - op_main_accuracy: 0.9122 - op_conv_accuracy: 0.9361 - avg_accuracy: 0.9334\n",
      "Epoch 00091: val_avg_accuracy improved from 0.88574 to 0.88669, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.9146 - op_main_loss: 0.2379 - op_conv_loss: 0.1613 - avg_loss: 0.1909 - op_main_accuracy: 0.9121 - op_conv_accuracy: 0.9360 - avg_accuracy: 0.9334 - val_loss: 1.2624 - val_op_main_loss: 0.3060 - val_op_conv_loss: 0.3354 - val_avg_loss: 0.2962 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8867\n",
      "Epoch 92/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9056 - op_main_loss: 0.2352 - op_conv_loss: 0.1570 - avg_loss: 0.1877 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9397 - avg_accuracy: 0.9388\n",
      "Epoch 00092: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9056 - op_main_loss: 0.2352 - op_conv_loss: 0.1570 - avg_loss: 0.1877 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9397 - avg_accuracy: 0.9388 - val_loss: 1.2527 - val_op_main_loss: 0.3030 - val_op_conv_loss: 0.3323 - val_avg_loss: 0.2916 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8782\n",
      "Epoch 93/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9054 - op_main_loss: 0.2345 - op_conv_loss: 0.1582 - avg_loss: 0.1875 - op_main_accuracy: 0.9147 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9345\n",
      "Epoch 00093: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9054 - op_main_loss: 0.2345 - op_conv_loss: 0.1582 - avg_loss: 0.1875 - op_main_accuracy: 0.9147 - op_conv_accuracy: 0.9374 - avg_accuracy: 0.9345 - val_loss: 1.4293 - val_op_main_loss: 0.3463 - val_op_conv_loss: 0.4091 - val_avg_loss: 0.3491 - val_op_main_accuracy: 0.8631 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8640\n",
      "Epoch 94/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9074 - op_main_loss: 0.2343 - op_conv_loss: 0.1605 - avg_loss: 0.1881 - op_main_accuracy: 0.9151 - op_conv_accuracy: 0.9389 - avg_accuracy: 0.9332\n",
      "Epoch 00094: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9103 - op_main_loss: 0.2356 - op_conv_loss: 0.1611 - avg_loss: 0.1890 - op_main_accuracy: 0.9138 - op_conv_accuracy: 0.9388 - avg_accuracy: 0.9329 - val_loss: 1.4192 - val_op_main_loss: 0.3525 - val_op_conv_loss: 0.3949 - val_avg_loss: 0.3473 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8612\n",
      "Epoch 95/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.9055 - op_main_loss: 0.2348 - op_conv_loss: 0.1579 - avg_loss: 0.1874 - op_main_accuracy: 0.9157 - op_conv_accuracy: 0.9387 - avg_accuracy: 0.9353\n",
      "Epoch 00095: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9060 - op_main_loss: 0.2342 - op_conv_loss: 0.1589 - avg_loss: 0.1875 - op_main_accuracy: 0.9168 - op_conv_accuracy: 0.9383 - avg_accuracy: 0.9353 - val_loss: 1.3287 - val_op_main_loss: 0.3237 - val_op_conv_loss: 0.3628 - val_avg_loss: 0.3178 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8763\n",
      "Epoch 96/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8784 - op_main_loss: 0.2246 - op_conv_loss: 0.1503 - avg_loss: 0.1792 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9407 - avg_accuracy: 0.9397\n",
      "Epoch 00096: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8784 - op_main_loss: 0.2246 - op_conv_loss: 0.1503 - avg_loss: 0.1792 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9407 - avg_accuracy: 0.9397 - val_loss: 1.2989 - val_op_main_loss: 0.3024 - val_op_conv_loss: 0.3687 - val_avg_loss: 0.3038 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8782\n",
      "Epoch 97/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8859 - op_main_loss: 0.2261 - op_conv_loss: 0.1540 - avg_loss: 0.1818 - op_main_accuracy: 0.9171 - op_conv_accuracy: 0.9399 - avg_accuracy: 0.9370\n",
      "Epoch 00097: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8856 - op_main_loss: 0.2261 - op_conv_loss: 0.1539 - avg_loss: 0.1817 - op_main_accuracy: 0.9171 - op_conv_accuracy: 0.9400 - avg_accuracy: 0.9371 - val_loss: 1.4945 - val_op_main_loss: 0.3900 - val_op_conv_loss: 0.4067 - val_avg_loss: 0.3745 - val_op_main_accuracy: 0.8517 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8584\n",
      "Epoch 98/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8978 - op_main_loss: 0.2332 - op_conv_loss: 0.1565 - avg_loss: 0.1853 - op_main_accuracy: 0.9150 - op_conv_accuracy: 0.9375 - avg_accuracy: 0.9370\n",
      "Epoch 00098: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8974 - op_main_loss: 0.2329 - op_conv_loss: 0.1565 - avg_loss: 0.1853 - op_main_accuracy: 0.9154 - op_conv_accuracy: 0.9376 - avg_accuracy: 0.9371 - val_loss: 1.4244 - val_op_main_loss: 0.3345 - val_op_conv_loss: 0.4237 - val_avg_loss: 0.3435 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8669 - val_avg_accuracy: 0.8659\n",
      "Epoch 99/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.9290 - op_main_loss: 0.2465 - op_conv_loss: 0.1649 - avg_loss: 0.1951 - op_main_accuracy: 0.9041 - op_conv_accuracy: 0.9351 - avg_accuracy: 0.9337\n",
      "Epoch 00099: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.9270 - op_main_loss: 0.2460 - op_conv_loss: 0.1641 - avg_loss: 0.1945 - op_main_accuracy: 0.9048 - op_conv_accuracy: 0.9355 - avg_accuracy: 0.9341 - val_loss: 1.2899 - val_op_main_loss: 0.3221 - val_op_conv_loss: 0.3392 - val_avg_loss: 0.3072 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8801\n",
      "Epoch 100/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8903 - op_main_loss: 0.2326 - op_conv_loss: 0.1518 - avg_loss: 0.1835 - op_main_accuracy: 0.9194 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9400\n",
      "Epoch 00100: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8903 - op_main_loss: 0.2326 - op_conv_loss: 0.1518 - avg_loss: 0.1835 - op_main_accuracy: 0.9194 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9400 - val_loss: 1.3084 - val_op_main_loss: 0.3240 - val_op_conv_loss: 0.3513 - val_avg_loss: 0.3101 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8810\n",
      "Epoch 101/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8868 - op_main_loss: 0.2319 - op_conv_loss: 0.1498 - avg_loss: 0.1816 - op_main_accuracy: 0.9147 - op_conv_accuracy: 0.9450 - avg_accuracy: 0.9419\n",
      "Epoch 00101: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8849 - op_main_loss: 0.2314 - op_conv_loss: 0.1491 - avg_loss: 0.1810 - op_main_accuracy: 0.9154 - op_conv_accuracy: 0.9452 - avg_accuracy: 0.9419 - val_loss: 1.2546 - val_op_main_loss: 0.3004 - val_op_conv_loss: 0.3410 - val_avg_loss: 0.2897 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8867\n",
      "Epoch 102/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.8889 - op_main_loss: 0.2278 - op_conv_loss: 0.1555 - avg_loss: 0.1828 - op_main_accuracy: 0.9183 - op_conv_accuracy: 0.9397 - avg_accuracy: 0.9375\n",
      "Epoch 00102: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8864 - op_main_loss: 0.2269 - op_conv_loss: 0.1547 - avg_loss: 0.1820 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9400 - avg_accuracy: 0.9381 - val_loss: 1.3612 - val_op_main_loss: 0.3302 - val_op_conv_loss: 0.3814 - val_avg_loss: 0.3258 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8801\n",
      "Epoch 103/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8616 - op_main_loss: 0.2217 - op_conv_loss: 0.1431 - avg_loss: 0.1736 - op_main_accuracy: 0.9218 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9412\n",
      "Epoch 00103: val_avg_accuracy did not improve from 0.88669\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8616 - op_main_loss: 0.2217 - op_conv_loss: 0.1431 - avg_loss: 0.1736 - op_main_accuracy: 0.9218 - op_conv_accuracy: 0.9433 - avg_accuracy: 0.9412 - val_loss: 1.3003 - val_op_main_loss: 0.3183 - val_op_conv_loss: 0.3549 - val_avg_loss: 0.3038 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8857\n",
      "Epoch 104/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8801 - op_main_loss: 0.2247 - op_conv_loss: 0.1527 - avg_loss: 0.1798 - op_main_accuracy: 0.9190 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9383\n",
      "Epoch 00104: val_avg_accuracy improved from 0.88669 to 0.88952, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.8801 - op_main_loss: 0.2247 - op_conv_loss: 0.1527 - avg_loss: 0.1798 - op_main_accuracy: 0.9190 - op_conv_accuracy: 0.9409 - avg_accuracy: 0.9383 - val_loss: 1.2943 - val_op_main_loss: 0.3201 - val_op_conv_loss: 0.3442 - val_avg_loss: 0.3066 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8895\n",
      "Epoch 105/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8846 - op_main_loss: 0.2283 - op_conv_loss: 0.1532 - avg_loss: 0.1807 - op_main_accuracy: 0.9216 - op_conv_accuracy: 0.9422 - avg_accuracy: 0.9389\n",
      "Epoch 00105: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8845 - op_main_loss: 0.2283 - op_conv_loss: 0.1531 - avg_loss: 0.1807 - op_main_accuracy: 0.9216 - op_conv_accuracy: 0.9421 - avg_accuracy: 0.9388 - val_loss: 1.4058 - val_op_main_loss: 0.3379 - val_op_conv_loss: 0.4053 - val_avg_loss: 0.3405 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8669\n",
      "Epoch 106/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8700 - op_main_loss: 0.2212 - op_conv_loss: 0.1499 - avg_loss: 0.1768 - op_main_accuracy: 0.9208 - op_conv_accuracy: 0.9400 - avg_accuracy: 0.9419\n",
      "Epoch 00106: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8700 - op_main_loss: 0.2212 - op_conv_loss: 0.1499 - avg_loss: 0.1768 - op_main_accuracy: 0.9208 - op_conv_accuracy: 0.9400 - avg_accuracy: 0.9419 - val_loss: 1.3369 - val_op_main_loss: 0.3336 - val_op_conv_loss: 0.3610 - val_avg_loss: 0.3202 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8810\n",
      "Epoch 107/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8679 - op_main_loss: 0.2198 - op_conv_loss: 0.1499 - avg_loss: 0.1762 - op_main_accuracy: 0.9191 - op_conv_accuracy: 0.9460 - avg_accuracy: 0.9416\n",
      "Epoch 00107: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8643 - op_main_loss: 0.2190 - op_conv_loss: 0.1483 - avg_loss: 0.1751 - op_main_accuracy: 0.9199 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9423 - val_loss: 1.3830 - val_op_main_loss: 0.3427 - val_op_conv_loss: 0.3827 - val_avg_loss: 0.3358 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8725\n",
      "Epoch 108/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8698 - op_main_loss: 0.2250 - op_conv_loss: 0.1468 - avg_loss: 0.1769 - op_main_accuracy: 0.9198 - op_conv_accuracy: 0.9426 - avg_accuracy: 0.9394\n",
      "Epoch 00108: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8702 - op_main_loss: 0.2250 - op_conv_loss: 0.1471 - avg_loss: 0.1769 - op_main_accuracy: 0.9204 - op_conv_accuracy: 0.9428 - avg_accuracy: 0.9395 - val_loss: 1.2788 - val_op_main_loss: 0.3089 - val_op_conv_loss: 0.3481 - val_avg_loss: 0.3011 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8848\n",
      "Epoch 109/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8512 - op_main_loss: 0.2158 - op_conv_loss: 0.1431 - avg_loss: 0.1704 - op_main_accuracy: 0.9186 - op_conv_accuracy: 0.9428 - avg_accuracy: 0.9423\n",
      "Epoch 00109: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8537 - op_main_loss: 0.2158 - op_conv_loss: 0.1449 - avg_loss: 0.1712 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9419 - avg_accuracy: 0.9421 - val_loss: 1.7008 - val_op_main_loss: 0.4105 - val_op_conv_loss: 0.5383 - val_avg_loss: 0.4299 - val_op_main_accuracy: 0.8414 - val_op_conv_accuracy: 0.8376 - val_avg_accuracy: 0.8414\n",
      "Epoch 110/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8674 - op_main_loss: 0.2265 - op_conv_loss: 0.1441 - avg_loss: 0.1765 - op_main_accuracy: 0.9164 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9390\n",
      "Epoch 00110: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8658 - op_main_loss: 0.2259 - op_conv_loss: 0.1435 - avg_loss: 0.1760 - op_main_accuracy: 0.9168 - op_conv_accuracy: 0.9435 - avg_accuracy: 0.9397 - val_loss: 1.2695 - val_op_main_loss: 0.3053 - val_op_conv_loss: 0.3456 - val_avg_loss: 0.2981 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8857\n",
      "Epoch 111/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8518 - op_main_loss: 0.2161 - op_conv_loss: 0.1434 - avg_loss: 0.1711 - op_main_accuracy: 0.9236 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9423\n",
      "Epoch 00111: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8515 - op_main_loss: 0.2161 - op_conv_loss: 0.1432 - avg_loss: 0.1711 - op_main_accuracy: 0.9237 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9426 - val_loss: 1.2743 - val_op_main_loss: 0.3011 - val_op_conv_loss: 0.3558 - val_avg_loss: 0.2964 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8839\n",
      "Epoch 112/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8624 - op_main_loss: 0.2212 - op_conv_loss: 0.1458 - avg_loss: 0.1747 - op_main_accuracy: 0.9184 - op_conv_accuracy: 0.9448 - avg_accuracy: 0.9411\n",
      "Epoch 00112: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8614 - op_main_loss: 0.2210 - op_conv_loss: 0.1453 - avg_loss: 0.1744 - op_main_accuracy: 0.9194 - op_conv_accuracy: 0.9449 - avg_accuracy: 0.9416 - val_loss: 1.3458 - val_op_main_loss: 0.3298 - val_op_conv_loss: 0.3727 - val_avg_loss: 0.3231 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8848\n",
      "Epoch 113/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8628 - op_main_loss: 0.2196 - op_conv_loss: 0.1479 - avg_loss: 0.1747 - op_main_accuracy: 0.9174 - op_conv_accuracy: 0.9426 - avg_accuracy: 0.9394\n",
      "Epoch 00113: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8603 - op_main_loss: 0.2189 - op_conv_loss: 0.1469 - avg_loss: 0.1739 - op_main_accuracy: 0.9180 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9402 - val_loss: 1.4975 - val_op_main_loss: 0.3651 - val_op_conv_loss: 0.4428 - val_avg_loss: 0.3698 - val_op_main_accuracy: 0.8640 - val_op_conv_accuracy: 0.8555 - val_avg_accuracy: 0.8621\n",
      "Epoch 114/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.8594 - op_main_loss: 0.2201 - op_conv_loss: 0.1456 - avg_loss: 0.1740 - op_main_accuracy: 0.9188 - op_conv_accuracy: 0.9432 - avg_accuracy: 0.9384\n",
      "Epoch 00114: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8599 - op_main_loss: 0.2204 - op_conv_loss: 0.1457 - avg_loss: 0.1742 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9431 - avg_accuracy: 0.9383 - val_loss: 1.3630 - val_op_main_loss: 0.3343 - val_op_conv_loss: 0.3809 - val_avg_loss: 0.3286 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8763\n",
      "Epoch 115/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8689 - op_main_loss: 0.2202 - op_conv_loss: 0.1528 - avg_loss: 0.1764 - op_main_accuracy: 0.9160 - op_conv_accuracy: 0.9403 - avg_accuracy: 0.9354\n",
      "Epoch 00115: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8683 - op_main_loss: 0.2199 - op_conv_loss: 0.1526 - avg_loss: 0.1762 - op_main_accuracy: 0.9161 - op_conv_accuracy: 0.9405 - avg_accuracy: 0.9355 - val_loss: 1.3517 - val_op_main_loss: 0.3347 - val_op_conv_loss: 0.3710 - val_avg_loss: 0.3271 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8782\n",
      "Epoch 116/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8354 - op_main_loss: 0.2106 - op_conv_loss: 0.1391 - avg_loss: 0.1670 - op_main_accuracy: 0.9273 - op_conv_accuracy: 0.9451 - avg_accuracy: 0.9444\n",
      "Epoch 00116: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8373 - op_main_loss: 0.2110 - op_conv_loss: 0.1399 - avg_loss: 0.1676 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9447 - avg_accuracy: 0.9440 - val_loss: 1.2657 - val_op_main_loss: 0.3062 - val_op_conv_loss: 0.3447 - val_avg_loss: 0.2958 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8857\n",
      "Epoch 117/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8220 - op_main_loss: 0.2090 - op_conv_loss: 0.1316 - avg_loss: 0.1624 - op_main_accuracy: 0.9243 - op_conv_accuracy: 0.9522 - avg_accuracy: 0.9462\n",
      "Epoch 00117: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8233 - op_main_loss: 0.2093 - op_conv_loss: 0.1322 - avg_loss: 0.1629 - op_main_accuracy: 0.9246 - op_conv_accuracy: 0.9516 - avg_accuracy: 0.9461 - val_loss: 1.3250 - val_op_main_loss: 0.3047 - val_op_conv_loss: 0.3923 - val_avg_loss: 0.3092 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8565 - val_avg_accuracy: 0.8650\n",
      "Epoch 118/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8395 - op_main_loss: 0.2136 - op_conv_loss: 0.1390 - avg_loss: 0.1682 - op_main_accuracy: 0.9241 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9430\n",
      "Epoch 00118: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8405 - op_main_loss: 0.2139 - op_conv_loss: 0.1394 - avg_loss: 0.1685 - op_main_accuracy: 0.9241 - op_conv_accuracy: 0.9464 - avg_accuracy: 0.9426 - val_loss: 1.5310 - val_op_main_loss: 0.3606 - val_op_conv_loss: 0.4749 - val_avg_loss: 0.3767 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8546 - val_avg_accuracy: 0.8546\n",
      "Epoch 119/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8755 - op_main_loss: 0.2314 - op_conv_loss: 0.1469 - avg_loss: 0.1797 - op_main_accuracy: 0.9124 - op_conv_accuracy: 0.9434 - avg_accuracy: 0.9413\n",
      "Epoch 00119: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8746 - op_main_loss: 0.2310 - op_conv_loss: 0.1466 - avg_loss: 0.1795 - op_main_accuracy: 0.9126 - op_conv_accuracy: 0.9435 - avg_accuracy: 0.9414 - val_loss: 1.3213 - val_op_main_loss: 0.3212 - val_op_conv_loss: 0.3669 - val_avg_loss: 0.3167 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8782\n",
      "Epoch 120/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8521 - op_main_loss: 0.2174 - op_conv_loss: 0.1459 - avg_loss: 0.1725 - op_main_accuracy: 0.9228 - op_conv_accuracy: 0.9457 - avg_accuracy: 0.9445\n",
      "Epoch 00120: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8518 - op_main_loss: 0.2171 - op_conv_loss: 0.1459 - avg_loss: 0.1724 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9457 - avg_accuracy: 0.9445 - val_loss: 1.2690 - val_op_main_loss: 0.3020 - val_op_conv_loss: 0.3518 - val_avg_loss: 0.2987 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8848\n",
      "Epoch 121/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8402 - op_main_loss: 0.2166 - op_conv_loss: 0.1384 - avg_loss: 0.1685 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9459\n",
      "Epoch 00121: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8402 - op_main_loss: 0.2166 - op_conv_loss: 0.1384 - avg_loss: 0.1685 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9478 - avg_accuracy: 0.9459 - val_loss: 1.3660 - val_op_main_loss: 0.3199 - val_op_conv_loss: 0.4034 - val_avg_loss: 0.3260 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8716\n",
      "Epoch 122/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8204 - op_main_loss: 0.2069 - op_conv_loss: 0.1341 - avg_loss: 0.1618 - op_main_accuracy: 0.9281 - op_conv_accuracy: 0.9479 - avg_accuracy: 0.9460\n",
      "Epoch 00122: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8225 - op_main_loss: 0.2079 - op_conv_loss: 0.1344 - avg_loss: 0.1625 - op_main_accuracy: 0.9277 - op_conv_accuracy: 0.9480 - avg_accuracy: 0.9457 - val_loss: 1.3531 - val_op_main_loss: 0.3325 - val_op_conv_loss: 0.3772 - val_avg_loss: 0.3252 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8763\n",
      "Epoch 123/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8583 - op_main_loss: 0.2150 - op_conv_loss: 0.1510 - avg_loss: 0.1746 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9402 - avg_accuracy: 0.9409\n",
      "Epoch 00123: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8583 - op_main_loss: 0.2150 - op_conv_loss: 0.1510 - avg_loss: 0.1746 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9402 - avg_accuracy: 0.9409 - val_loss: 1.4854 - val_op_main_loss: 0.3683 - val_op_conv_loss: 0.4320 - val_avg_loss: 0.3681 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8640\n",
      "Epoch 124/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8298 - op_main_loss: 0.2088 - op_conv_loss: 0.1385 - avg_loss: 0.1655 - op_main_accuracy: 0.9214 - op_conv_accuracy: 0.9474 - avg_accuracy: 0.9435\n",
      "Epoch 00124: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8279 - op_main_loss: 0.2078 - op_conv_loss: 0.1382 - avg_loss: 0.1649 - op_main_accuracy: 0.9220 - op_conv_accuracy: 0.9473 - avg_accuracy: 0.9438 - val_loss: 1.3188 - val_op_main_loss: 0.3267 - val_op_conv_loss: 0.3596 - val_avg_loss: 0.3155 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8820\n",
      "Epoch 125/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8205 - op_main_loss: 0.2048 - op_conv_loss: 0.1365 - avg_loss: 0.1631 - op_main_accuracy: 0.9300 - op_conv_accuracy: 0.9472 - avg_accuracy: 0.9455\n",
      "Epoch 00125: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8229 - op_main_loss: 0.2055 - op_conv_loss: 0.1374 - avg_loss: 0.1638 - op_main_accuracy: 0.9293 - op_conv_accuracy: 0.9466 - avg_accuracy: 0.9452 - val_loss: 1.2709 - val_op_main_loss: 0.3030 - val_op_conv_loss: 0.3534 - val_avg_loss: 0.2982 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8876\n",
      "Epoch 126/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.8147 - op_main_loss: 0.2097 - op_conv_loss: 0.1281 - avg_loss: 0.1608 - op_main_accuracy: 0.9245 - op_conv_accuracy: 0.9526 - avg_accuracy: 0.9469\n",
      "Epoch 00126: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8133 - op_main_loss: 0.2089 - op_conv_loss: 0.1279 - avg_loss: 0.1603 - op_main_accuracy: 0.9253 - op_conv_accuracy: 0.9527 - avg_accuracy: 0.9468 - val_loss: 1.4192 - val_op_main_loss: 0.3419 - val_op_conv_loss: 0.4173 - val_avg_loss: 0.3437 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8735\n",
      "Epoch 127/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8382 - op_main_loss: 0.2125 - op_conv_loss: 0.1413 - avg_loss: 0.1682 - op_main_accuracy: 0.9187 - op_conv_accuracy: 0.9458 - avg_accuracy: 0.9416\n",
      "Epoch 00127: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8370 - op_main_loss: 0.2123 - op_conv_loss: 0.1408 - avg_loss: 0.1679 - op_main_accuracy: 0.9192 - op_conv_accuracy: 0.9459 - avg_accuracy: 0.9419 - val_loss: 1.2841 - val_op_main_loss: 0.3138 - val_op_conv_loss: 0.3488 - val_avg_loss: 0.3052 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8829\n",
      "Epoch 128/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8256 - op_main_loss: 0.2069 - op_conv_loss: 0.1380 - avg_loss: 0.1641 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9442 - avg_accuracy: 0.9466\n",
      "Epoch 00128: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8256 - op_main_loss: 0.2069 - op_conv_loss: 0.1380 - avg_loss: 0.1641 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9442 - avg_accuracy: 0.9466 - val_loss: 1.2751 - val_op_main_loss: 0.3121 - val_op_conv_loss: 0.3465 - val_avg_loss: 0.2998 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8876\n",
      "Epoch 129/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8437 - op_main_loss: 0.2107 - op_conv_loss: 0.1473 - avg_loss: 0.1696 - op_main_accuracy: 0.9273 - op_conv_accuracy: 0.9453 - avg_accuracy: 0.9429\n",
      "Epoch 00129: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8435 - op_main_loss: 0.2106 - op_conv_loss: 0.1472 - avg_loss: 0.1695 - op_main_accuracy: 0.9275 - op_conv_accuracy: 0.9454 - avg_accuracy: 0.9431 - val_loss: 1.3050 - val_op_main_loss: 0.3205 - val_op_conv_loss: 0.3578 - val_avg_loss: 0.3110 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8886\n",
      "Epoch 130/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8067 - op_main_loss: 0.2039 - op_conv_loss: 0.1284 - avg_loss: 0.1587 - op_main_accuracy: 0.9288 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9508\n",
      "Epoch 00130: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8051 - op_main_loss: 0.2033 - op_conv_loss: 0.1279 - avg_loss: 0.1582 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9506 - val_loss: 1.3483 - val_op_main_loss: 0.3331 - val_op_conv_loss: 0.3738 - val_avg_loss: 0.3262 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8810\n",
      "Epoch 131/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8112 - op_main_loss: 0.2053 - op_conv_loss: 0.1301 - avg_loss: 0.1602 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9499\n",
      "Epoch 00131: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8112 - op_main_loss: 0.2053 - op_conv_loss: 0.1301 - avg_loss: 0.1602 - op_main_accuracy: 0.9263 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9499 - val_loss: 1.2934 - val_op_main_loss: 0.3158 - val_op_conv_loss: 0.3536 - val_avg_loss: 0.3079 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8829\n",
      "Epoch 132/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8163 - op_main_loss: 0.2089 - op_conv_loss: 0.1306 - avg_loss: 0.1605 - op_main_accuracy: 0.9216 - op_conv_accuracy: 0.9500 - avg_accuracy: 0.9478\n",
      "Epoch 00132: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8172 - op_main_loss: 0.2089 - op_conv_loss: 0.1312 - avg_loss: 0.1609 - op_main_accuracy: 0.9220 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9475 - val_loss: 1.2686 - val_op_main_loss: 0.3099 - val_op_conv_loss: 0.3440 - val_avg_loss: 0.2985 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8867\n",
      "Epoch 133/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8175 - op_main_loss: 0.2048 - op_conv_loss: 0.1353 - avg_loss: 0.1613 - op_main_accuracy: 0.9270 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9478\n",
      "Epoch 00133: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8171 - op_main_loss: 0.2051 - op_conv_loss: 0.1347 - avg_loss: 0.1612 - op_main_accuracy: 0.9265 - op_conv_accuracy: 0.9480 - avg_accuracy: 0.9480 - val_loss: 1.4452 - val_op_main_loss: 0.3564 - val_op_conv_loss: 0.4227 - val_avg_loss: 0.3516 - val_op_main_accuracy: 0.8357 - val_op_conv_accuracy: 0.8470 - val_avg_accuracy: 0.8489\n",
      "Epoch 134/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8177 - op_main_loss: 0.2070 - op_conv_loss: 0.1337 - avg_loss: 0.1624 - op_main_accuracy: 0.9255 - op_conv_accuracy: 0.9430 - avg_accuracy: 0.9421\n",
      "Epoch 00134: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8181 - op_main_loss: 0.2069 - op_conv_loss: 0.1339 - avg_loss: 0.1626 - op_main_accuracy: 0.9256 - op_conv_accuracy: 0.9426 - avg_accuracy: 0.9419 - val_loss: 1.2727 - val_op_main_loss: 0.2992 - val_op_conv_loss: 0.3619 - val_avg_loss: 0.2970 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8763\n",
      "Epoch 135/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8343 - op_main_loss: 0.2116 - op_conv_loss: 0.1408 - avg_loss: 0.1672 - op_main_accuracy: 0.9212 - op_conv_accuracy: 0.9489 - avg_accuracy: 0.9422\n",
      "Epoch 00135: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8342 - op_main_loss: 0.2115 - op_conv_loss: 0.1409 - avg_loss: 0.1672 - op_main_accuracy: 0.9211 - op_conv_accuracy: 0.9487 - avg_accuracy: 0.9421 - val_loss: 1.2564 - val_op_main_loss: 0.3000 - val_op_conv_loss: 0.3474 - val_avg_loss: 0.2946 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8886\n",
      "Epoch 136/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.8017 - op_main_loss: 0.2015 - op_conv_loss: 0.1292 - avg_loss: 0.1565 - op_main_accuracy: 0.9336 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9496\n",
      "Epoch 00136: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7970 - op_main_loss: 0.1996 - op_conv_loss: 0.1279 - avg_loss: 0.1550 - op_main_accuracy: 0.9348 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9501 - val_loss: 1.3212 - val_op_main_loss: 0.3151 - val_op_conv_loss: 0.3773 - val_avg_loss: 0.3143 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8876\n",
      "Epoch 137/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8084 - op_main_loss: 0.2041 - op_conv_loss: 0.1310 - avg_loss: 0.1590 - op_main_accuracy: 0.9309 - op_conv_accuracy: 0.9489 - avg_accuracy: 0.9474\n",
      "Epoch 00137: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8079 - op_main_loss: 0.2038 - op_conv_loss: 0.1310 - avg_loss: 0.1588 - op_main_accuracy: 0.9310 - op_conv_accuracy: 0.9490 - avg_accuracy: 0.9475 - val_loss: 1.2780 - val_op_main_loss: 0.3046 - val_op_conv_loss: 0.3587 - val_avg_loss: 0.3000 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8876\n",
      "Epoch 138/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.8099 - op_main_loss: 0.2036 - op_conv_loss: 0.1325 - avg_loss: 0.1593 - op_main_accuracy: 0.9249 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9451\n",
      "Epoch 00138: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8098 - op_main_loss: 0.2035 - op_conv_loss: 0.1325 - avg_loss: 0.1593 - op_main_accuracy: 0.9251 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9452 - val_loss: 1.3130 - val_op_main_loss: 0.3118 - val_op_conv_loss: 0.3751 - val_avg_loss: 0.3122 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8839\n",
      "Epoch 139/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7895 - op_main_loss: 0.1938 - op_conv_loss: 0.1277 - avg_loss: 0.1534 - op_main_accuracy: 0.9315 - op_conv_accuracy: 0.9488 - avg_accuracy: 0.9474\n",
      "Epoch 00139: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7912 - op_main_loss: 0.1952 - op_conv_loss: 0.1276 - avg_loss: 0.1539 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9487 - avg_accuracy: 0.9473 - val_loss: 1.4561 - val_op_main_loss: 0.3698 - val_op_conv_loss: 0.4108 - val_avg_loss: 0.3609 - val_op_main_accuracy: 0.8593 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8687\n",
      "Epoch 140/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7875 - op_main_loss: 0.1980 - op_conv_loss: 0.1221 - avg_loss: 0.1528 - op_main_accuracy: 0.9300 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9516\n",
      "Epoch 00140: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7887 - op_main_loss: 0.1978 - op_conv_loss: 0.1231 - avg_loss: 0.1532 - op_main_accuracy: 0.9308 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9516 - val_loss: 1.3868 - val_op_main_loss: 0.3375 - val_op_conv_loss: 0.3993 - val_avg_loss: 0.3353 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8772\n",
      "Epoch 141/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8041 - op_main_loss: 0.2032 - op_conv_loss: 0.1290 - avg_loss: 0.1579 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9461\n",
      "Epoch 00141: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8041 - op_main_loss: 0.2032 - op_conv_loss: 0.1290 - avg_loss: 0.1579 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9461 - val_loss: 1.4536 - val_op_main_loss: 0.3623 - val_op_conv_loss: 0.4187 - val_avg_loss: 0.3592 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8631 - val_avg_accuracy: 0.8650\n",
      "Epoch 142/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8015 - op_main_loss: 0.2028 - op_conv_loss: 0.1283 - avg_loss: 0.1571 - op_main_accuracy: 0.9225 - op_conv_accuracy: 0.9516 - avg_accuracy: 0.9504\n",
      "Epoch 00142: val_avg_accuracy did not improve from 0.88952\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8013 - op_main_loss: 0.2025 - op_conv_loss: 0.1285 - avg_loss: 0.1571 - op_main_accuracy: 0.9227 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9501 - val_loss: 1.2810 - val_op_main_loss: 0.3083 - val_op_conv_loss: 0.3577 - val_avg_loss: 0.3016 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8876\n",
      "Epoch 143/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7713 - op_main_loss: 0.1914 - op_conv_loss: 0.1185 - avg_loss: 0.1476 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9525 - avg_accuracy: 0.9504\n",
      "Epoch 00143: val_avg_accuracy improved from 0.88952 to 0.89424, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7713 - op_main_loss: 0.1914 - op_conv_loss: 0.1185 - avg_loss: 0.1476 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9525 - avg_accuracy: 0.9504 - val_loss: 1.2860 - val_op_main_loss: 0.3078 - val_op_conv_loss: 0.3626 - val_avg_loss: 0.3017 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8942\n",
      "Epoch 144/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7975 - op_main_loss: 0.2013 - op_conv_loss: 0.1266 - avg_loss: 0.1555 - op_main_accuracy: 0.9303 - op_conv_accuracy: 0.9494 - avg_accuracy: 0.9487\n",
      "Epoch 00144: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7954 - op_main_loss: 0.2006 - op_conv_loss: 0.1258 - avg_loss: 0.1549 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9499 - avg_accuracy: 0.9492 - val_loss: 1.3002 - val_op_main_loss: 0.3187 - val_op_conv_loss: 0.3588 - val_avg_loss: 0.3090 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8886\n",
      "Epoch 145/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7747 - op_main_loss: 0.1912 - op_conv_loss: 0.1206 - avg_loss: 0.1489 - op_main_accuracy: 0.9370 - op_conv_accuracy: 0.9515 - avg_accuracy: 0.9484\n",
      "Epoch 00145: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7739 - op_main_loss: 0.1909 - op_conv_loss: 0.1204 - avg_loss: 0.1486 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9516 - avg_accuracy: 0.9485 - val_loss: 1.3084 - val_op_main_loss: 0.3053 - val_op_conv_loss: 0.3814 - val_avg_loss: 0.3075 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8886\n",
      "Epoch 146/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.8100 - op_main_loss: 0.2047 - op_conv_loss: 0.1321 - avg_loss: 0.1597 - op_main_accuracy: 0.9235 - op_conv_accuracy: 0.9472 - avg_accuracy: 0.9444\n",
      "Epoch 00146: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8127 - op_main_loss: 0.2054 - op_conv_loss: 0.1333 - avg_loss: 0.1606 - op_main_accuracy: 0.9232 - op_conv_accuracy: 0.9468 - avg_accuracy: 0.9440 - val_loss: 1.2701 - val_op_main_loss: 0.3082 - val_op_conv_loss: 0.3499 - val_avg_loss: 0.2988 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8933\n",
      "Epoch 147/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7868 - op_main_loss: 0.1968 - op_conv_loss: 0.1246 - avg_loss: 0.1527 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9520 - avg_accuracy: 0.9497\n",
      "Epoch 00147: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7868 - op_main_loss: 0.1968 - op_conv_loss: 0.1246 - avg_loss: 0.1527 - op_main_accuracy: 0.9296 - op_conv_accuracy: 0.9520 - avg_accuracy: 0.9497 - val_loss: 1.2703 - val_op_main_loss: 0.3036 - val_op_conv_loss: 0.3553 - val_avg_loss: 0.2988 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8905\n",
      "Epoch 148/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7749 - op_main_loss: 0.1917 - op_conv_loss: 0.1216 - avg_loss: 0.1491 - op_main_accuracy: 0.9341 - op_conv_accuracy: 0.9525 - avg_accuracy: 0.9530\n",
      "Epoch 00148: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7749 - op_main_loss: 0.1917 - op_conv_loss: 0.1216 - avg_loss: 0.1491 - op_main_accuracy: 0.9341 - op_conv_accuracy: 0.9525 - avg_accuracy: 0.9530 - val_loss: 1.3890 - val_op_main_loss: 0.3320 - val_op_conv_loss: 0.4092 - val_avg_loss: 0.3352 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8725\n",
      "Epoch 149/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7911 - op_main_loss: 0.1997 - op_conv_loss: 0.1242 - avg_loss: 0.1538 - op_main_accuracy: 0.9311 - op_conv_accuracy: 0.9512 - avg_accuracy: 0.9482\n",
      "Epoch 00149: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7909 - op_main_loss: 0.1996 - op_conv_loss: 0.1241 - avg_loss: 0.1538 - op_main_accuracy: 0.9312 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9483 - val_loss: 1.3746 - val_op_main_loss: 0.3280 - val_op_conv_loss: 0.4036 - val_avg_loss: 0.3301 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8810\n",
      "Epoch 150/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.7936 - op_main_loss: 0.1970 - op_conv_loss: 0.1289 - avg_loss: 0.1548 - op_main_accuracy: 0.9268 - op_conv_accuracy: 0.9491 - avg_accuracy: 0.9460\n",
      "Epoch 00150: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7975 - op_main_loss: 0.1986 - op_conv_loss: 0.1300 - avg_loss: 0.1560 - op_main_accuracy: 0.9265 - op_conv_accuracy: 0.9483 - avg_accuracy: 0.9452 - val_loss: 2.7524 - val_op_main_loss: 0.6692 - val_op_conv_loss: 1.0280 - val_avg_loss: 0.7418 - val_op_main_accuracy: 0.7734 - val_op_conv_accuracy: 0.7620 - val_avg_accuracy: 0.7677\n",
      "Epoch 151/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.8054 - op_main_loss: 0.2021 - op_conv_loss: 0.1326 - avg_loss: 0.1581 - op_main_accuracy: 0.9258 - op_conv_accuracy: 0.9520 - avg_accuracy: 0.9461\n",
      "Epoch 00151: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8054 - op_main_loss: 0.2021 - op_conv_loss: 0.1326 - avg_loss: 0.1581 - op_main_accuracy: 0.9258 - op_conv_accuracy: 0.9520 - avg_accuracy: 0.9461 - val_loss: 1.3017 - val_op_main_loss: 0.3176 - val_op_conv_loss: 0.3610 - val_avg_loss: 0.3103 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8895\n",
      "Epoch 152/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7791 - op_main_loss: 0.1921 - op_conv_loss: 0.1237 - avg_loss: 0.1505 - op_main_accuracy: 0.9307 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9491\n",
      "Epoch 00152: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7768 - op_main_loss: 0.1913 - op_conv_loss: 0.1229 - avg_loss: 0.1498 - op_main_accuracy: 0.9312 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9497 - val_loss: 1.5842 - val_op_main_loss: 0.3951 - val_op_conv_loss: 0.4767 - val_avg_loss: 0.3998 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8527 - val_avg_accuracy: 0.8536\n",
      "Epoch 153/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7859 - op_main_loss: 0.1960 - op_conv_loss: 0.1251 - avg_loss: 0.1525 - op_main_accuracy: 0.9295 - op_conv_accuracy: 0.9500 - avg_accuracy: 0.9527\n",
      "Epoch 00153: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7887 - op_main_loss: 0.1967 - op_conv_loss: 0.1264 - avg_loss: 0.1533 - op_main_accuracy: 0.9289 - op_conv_accuracy: 0.9497 - avg_accuracy: 0.9520 - val_loss: 1.3139 - val_op_main_loss: 0.3102 - val_op_conv_loss: 0.3815 - val_avg_loss: 0.3098 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8924\n",
      "Epoch 154/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7838 - op_main_loss: 0.1942 - op_conv_loss: 0.1258 - avg_loss: 0.1510 - op_main_accuracy: 0.9294 - op_conv_accuracy: 0.9528 - avg_accuracy: 0.9521\n",
      "Epoch 00154: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7832 - op_main_loss: 0.1940 - op_conv_loss: 0.1256 - avg_loss: 0.1508 - op_main_accuracy: 0.9298 - op_conv_accuracy: 0.9527 - avg_accuracy: 0.9523 - val_loss: 1.3509 - val_op_main_loss: 0.3304 - val_op_conv_loss: 0.3822 - val_avg_loss: 0.3253 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8886\n",
      "Epoch 155/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7636 - op_main_loss: 0.1888 - op_conv_loss: 0.1168 - avg_loss: 0.1455 - op_main_accuracy: 0.9373 - op_conv_accuracy: 0.9555 - avg_accuracy: 0.9550\n",
      "Epoch 00155: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7637 - op_main_loss: 0.1888 - op_conv_loss: 0.1168 - avg_loss: 0.1455 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9553 - avg_accuracy: 0.9549 - val_loss: 1.2871 - val_op_main_loss: 0.3048 - val_op_conv_loss: 0.3697 - val_avg_loss: 0.2999 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8895\n",
      "Epoch 156/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.8138 - op_main_loss: 0.2096 - op_conv_loss: 0.1304 - avg_loss: 0.1619 - op_main_accuracy: 0.9269 - op_conv_accuracy: 0.9514 - avg_accuracy: 0.9471\n",
      "Epoch 00156: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.8134 - op_main_loss: 0.2095 - op_conv_loss: 0.1303 - avg_loss: 0.1619 - op_main_accuracy: 0.9265 - op_conv_accuracy: 0.9509 - avg_accuracy: 0.9464 - val_loss: 1.4099 - val_op_main_loss: 0.3415 - val_op_conv_loss: 0.4144 - val_avg_loss: 0.3426 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8801\n",
      "Epoch 157/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7857 - op_main_loss: 0.1954 - op_conv_loss: 0.1257 - avg_loss: 0.1531 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9471\n",
      "Epoch 00157: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7857 - op_main_loss: 0.1954 - op_conv_loss: 0.1257 - avg_loss: 0.1531 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9506 - avg_accuracy: 0.9471 - val_loss: 1.2448 - val_op_main_loss: 0.2937 - val_op_conv_loss: 0.3512 - val_avg_loss: 0.2879 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8886\n",
      "Epoch 158/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7799 - op_main_loss: 0.1957 - op_conv_loss: 0.1222 - avg_loss: 0.1501 - op_main_accuracy: 0.9280 - op_conv_accuracy: 0.9548 - avg_accuracy: 0.9500\n",
      "Epoch 00158: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7817 - op_main_loss: 0.1963 - op_conv_loss: 0.1228 - avg_loss: 0.1507 - op_main_accuracy: 0.9277 - op_conv_accuracy: 0.9544 - avg_accuracy: 0.9497 - val_loss: 1.3595 - val_op_main_loss: 0.3219 - val_op_conv_loss: 0.4004 - val_avg_loss: 0.3259 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8829\n",
      "Epoch 159/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7630 - op_main_loss: 0.1891 - op_conv_loss: 0.1169 - avg_loss: 0.1457 - op_main_accuracy: 0.9353 - op_conv_accuracy: 0.9562 - avg_accuracy: 0.9532\n",
      "Epoch 00159: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7626 - op_main_loss: 0.1888 - op_conv_loss: 0.1169 - avg_loss: 0.1456 - op_main_accuracy: 0.9357 - op_conv_accuracy: 0.9563 - avg_accuracy: 0.9537 - val_loss: 1.2848 - val_op_main_loss: 0.3054 - val_op_conv_loss: 0.3651 - val_avg_loss: 0.3028 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8895\n",
      "Epoch 160/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7469 - op_main_loss: 0.1815 - op_conv_loss: 0.1137 - avg_loss: 0.1401 - op_main_accuracy: 0.9394 - op_conv_accuracy: 0.9576 - avg_accuracy: 0.9555\n",
      "Epoch 00160: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7464 - op_main_loss: 0.1813 - op_conv_loss: 0.1135 - avg_loss: 0.1399 - op_main_accuracy: 0.9395 - op_conv_accuracy: 0.9577 - avg_accuracy: 0.9556 - val_loss: 1.2831 - val_op_main_loss: 0.3062 - val_op_conv_loss: 0.3642 - val_avg_loss: 0.3008 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8933\n",
      "Epoch 161/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7905 - op_main_loss: 0.2004 - op_conv_loss: 0.1249 - avg_loss: 0.1538 - op_main_accuracy: 0.9243 - op_conv_accuracy: 0.9510 - avg_accuracy: 0.9471\n",
      "Epoch 00161: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7894 - op_main_loss: 0.2001 - op_conv_loss: 0.1244 - avg_loss: 0.1535 - op_main_accuracy: 0.9241 - op_conv_accuracy: 0.9511 - avg_accuracy: 0.9473 - val_loss: 1.2422 - val_op_main_loss: 0.2924 - val_op_conv_loss: 0.3530 - val_avg_loss: 0.2871 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8857\n",
      "Epoch 162/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.7642 - op_main_loss: 0.1890 - op_conv_loss: 0.1195 - avg_loss: 0.1463 - op_main_accuracy: 0.9344 - op_conv_accuracy: 0.9540 - avg_accuracy: 0.9521\n",
      "Epoch 00162: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7632 - op_main_loss: 0.1887 - op_conv_loss: 0.1190 - avg_loss: 0.1460 - op_main_accuracy: 0.9345 - op_conv_accuracy: 0.9544 - avg_accuracy: 0.9525 - val_loss: 1.3692 - val_op_main_loss: 0.3132 - val_op_conv_loss: 0.4211 - val_avg_loss: 0.3252 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8754\n",
      "Epoch 163/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7713 - op_main_loss: 0.1926 - op_conv_loss: 0.1203 - avg_loss: 0.1486 - op_main_accuracy: 0.9365 - op_conv_accuracy: 0.9578 - avg_accuracy: 0.9537\n",
      "Epoch 00163: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7689 - op_main_loss: 0.1918 - op_conv_loss: 0.1194 - avg_loss: 0.1478 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9542 - val_loss: 1.3466 - val_op_main_loss: 0.3209 - val_op_conv_loss: 0.3935 - val_avg_loss: 0.3222 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8763\n",
      "Epoch 164/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7578 - op_main_loss: 0.1865 - op_conv_loss: 0.1173 - avg_loss: 0.1438 - op_main_accuracy: 0.9350 - op_conv_accuracy: 0.9549 - avg_accuracy: 0.9530\n",
      "Epoch 00164: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7578 - op_main_loss: 0.1865 - op_conv_loss: 0.1173 - avg_loss: 0.1438 - op_main_accuracy: 0.9350 - op_conv_accuracy: 0.9549 - avg_accuracy: 0.9530 - val_loss: 1.4356 - val_op_main_loss: 0.3385 - val_op_conv_loss: 0.4385 - val_avg_loss: 0.3483 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8716\n",
      "Epoch 165/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7619 - op_main_loss: 0.1872 - op_conv_loss: 0.1187 - avg_loss: 0.1455 - op_main_accuracy: 0.9363 - op_conv_accuracy: 0.9538 - avg_accuracy: 0.9517\n",
      "Epoch 00165: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7614 - op_main_loss: 0.1871 - op_conv_loss: 0.1185 - avg_loss: 0.1454 - op_main_accuracy: 0.9364 - op_conv_accuracy: 0.9539 - avg_accuracy: 0.9518 - val_loss: 1.2889 - val_op_main_loss: 0.3116 - val_op_conv_loss: 0.3632 - val_avg_loss: 0.3038 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8914\n",
      "Epoch 166/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7540 - op_main_loss: 0.1819 - op_conv_loss: 0.1182 - avg_loss: 0.1431 - op_main_accuracy: 0.9421 - op_conv_accuracy: 0.9576 - avg_accuracy: 0.9537\n",
      "Epoch 00166: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7566 - op_main_loss: 0.1830 - op_conv_loss: 0.1189 - avg_loss: 0.1440 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9575 - avg_accuracy: 0.9532 - val_loss: 1.4785 - val_op_main_loss: 0.3523 - val_op_conv_loss: 0.4533 - val_avg_loss: 0.3625 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8640 - val_avg_accuracy: 0.8640\n",
      "Epoch 167/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7658 - op_main_loss: 0.1905 - op_conv_loss: 0.1186 - avg_loss: 0.1460 - op_main_accuracy: 0.9314 - op_conv_accuracy: 0.9552 - avg_accuracy: 0.9523\n",
      "Epoch 00167: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7660 - op_main_loss: 0.1907 - op_conv_loss: 0.1185 - avg_loss: 0.1461 - op_main_accuracy: 0.9305 - op_conv_accuracy: 0.9551 - avg_accuracy: 0.9523 - val_loss: 1.3119 - val_op_main_loss: 0.3050 - val_op_conv_loss: 0.3869 - val_avg_loss: 0.3095 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8839\n",
      "Epoch 168/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7565 - op_main_loss: 0.1837 - op_conv_loss: 0.1187 - avg_loss: 0.1432 - op_main_accuracy: 0.9413 - op_conv_accuracy: 0.9535 - avg_accuracy: 0.9537\n",
      "Epoch 00168: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7585 - op_main_loss: 0.1842 - op_conv_loss: 0.1195 - avg_loss: 0.1439 - op_main_accuracy: 0.9407 - op_conv_accuracy: 0.9525 - avg_accuracy: 0.9527 - val_loss: 1.3256 - val_op_main_loss: 0.3266 - val_op_conv_loss: 0.3697 - val_avg_loss: 0.3183 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8905\n",
      "Epoch 169/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7338 - op_main_loss: 0.1756 - op_conv_loss: 0.1107 - avg_loss: 0.1366 - op_main_accuracy: 0.9431 - op_conv_accuracy: 0.9581 - avg_accuracy: 0.9576\n",
      "Epoch 00169: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7326 - op_main_loss: 0.1752 - op_conv_loss: 0.1103 - avg_loss: 0.1363 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9577 - val_loss: 1.3520 - val_op_main_loss: 0.3215 - val_op_conv_loss: 0.3981 - val_avg_loss: 0.3219 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8848\n",
      "Epoch 170/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7358 - op_main_loss: 0.1791 - op_conv_loss: 0.1092 - avg_loss: 0.1375 - op_main_accuracy: 0.9397 - op_conv_accuracy: 0.9600 - avg_accuracy: 0.9600\n",
      "Epoch 00170: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7349 - op_main_loss: 0.1791 - op_conv_loss: 0.1086 - avg_loss: 0.1372 - op_main_accuracy: 0.9402 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9603 - val_loss: 1.2981 - val_op_main_loss: 0.3127 - val_op_conv_loss: 0.3682 - val_avg_loss: 0.3072 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8924\n",
      "Epoch 171/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7717 - op_main_loss: 0.1871 - op_conv_loss: 0.1267 - avg_loss: 0.1482 - op_main_accuracy: 0.9272 - op_conv_accuracy: 0.9475 - avg_accuracy: 0.9468\n",
      "Epoch 00171: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7727 - op_main_loss: 0.1875 - op_conv_loss: 0.1270 - avg_loss: 0.1485 - op_main_accuracy: 0.9267 - op_conv_accuracy: 0.9473 - avg_accuracy: 0.9464 - val_loss: 1.3492 - val_op_main_loss: 0.3195 - val_op_conv_loss: 0.3984 - val_avg_loss: 0.3222 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8820\n",
      "Epoch 172/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7358 - op_main_loss: 0.1832 - op_conv_loss: 0.1062 - avg_loss: 0.1374 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9584 - avg_accuracy: 0.9594\n",
      "Epoch 00172: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7358 - op_main_loss: 0.1832 - op_conv_loss: 0.1062 - avg_loss: 0.1374 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9584 - avg_accuracy: 0.9594 - val_loss: 1.3659 - val_op_main_loss: 0.3322 - val_op_conv_loss: 0.3950 - val_avg_loss: 0.3297 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8791\n",
      "Epoch 173/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7521 - op_main_loss: 0.1825 - op_conv_loss: 0.1191 - avg_loss: 0.1417 - op_main_accuracy: 0.9332 - op_conv_accuracy: 0.9562 - avg_accuracy: 0.9534\n",
      "Epoch 00173: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7524 - op_main_loss: 0.1825 - op_conv_loss: 0.1193 - avg_loss: 0.1418 - op_main_accuracy: 0.9334 - op_conv_accuracy: 0.9563 - avg_accuracy: 0.9534 - val_loss: 1.2825 - val_op_main_loss: 0.3027 - val_op_conv_loss: 0.3717 - val_avg_loss: 0.2989 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8914\n",
      "Epoch 174/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.7428 - op_main_loss: 0.1818 - op_conv_loss: 0.1127 - avg_loss: 0.1392 - op_main_accuracy: 0.9388 - op_conv_accuracy: 0.9556 - avg_accuracy: 0.9534\n",
      "Epoch 00174: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7428 - op_main_loss: 0.1818 - op_conv_loss: 0.1127 - avg_loss: 0.1392 - op_main_accuracy: 0.9388 - op_conv_accuracy: 0.9556 - avg_accuracy: 0.9534 - val_loss: 1.9239 - val_op_main_loss: 0.4670 - val_op_conv_loss: 0.6536 - val_avg_loss: 0.4945 - val_op_main_accuracy: 0.8461 - val_op_conv_accuracy: 0.8366 - val_avg_accuracy: 0.8366\n",
      "Epoch 175/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7631 - op_main_loss: 0.1876 - op_conv_loss: 0.1203 - avg_loss: 0.1468 - op_main_accuracy: 0.9353 - op_conv_accuracy: 0.9512 - avg_accuracy: 0.9495\n",
      "Epoch 00175: val_avg_accuracy did not improve from 0.89424\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7619 - op_main_loss: 0.1872 - op_conv_loss: 0.1199 - avg_loss: 0.1465 - op_main_accuracy: 0.9357 - op_conv_accuracy: 0.9513 - avg_accuracy: 0.9499 - val_loss: 1.3355 - val_op_main_loss: 0.3180 - val_op_conv_loss: 0.3903 - val_avg_loss: 0.3189 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8848\n",
      "Epoch 176/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7256 - op_main_loss: 0.1774 - op_conv_loss: 0.1051 - avg_loss: 0.1348 - op_main_accuracy: 0.9403 - op_conv_accuracy: 0.9600 - avg_accuracy: 0.9583\n",
      "Epoch 00176: val_avg_accuracy improved from 0.89424 to 0.89518, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.7271 - op_main_loss: 0.1778 - op_conv_loss: 0.1057 - avg_loss: 0.1353 - op_main_accuracy: 0.9400 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9582 - val_loss: 1.2716 - val_op_main_loss: 0.3084 - val_op_conv_loss: 0.3558 - val_avg_loss: 0.2987 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8952\n",
      "Epoch 177/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7224 - op_main_loss: 0.1767 - op_conv_loss: 0.1038 - avg_loss: 0.1330 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9610\n",
      "Epoch 00177: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7224 - op_main_loss: 0.1767 - op_conv_loss: 0.1038 - avg_loss: 0.1330 - op_main_accuracy: 0.9371 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9610 - val_loss: 1.3362 - val_op_main_loss: 0.3237 - val_op_conv_loss: 0.3857 - val_avg_loss: 0.3179 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8942\n",
      "Epoch 178/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7615 - op_main_loss: 0.1894 - op_conv_loss: 0.1180 - avg_loss: 0.1454 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9577 - avg_accuracy: 0.9530\n",
      "Epoch 00178: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7615 - op_main_loss: 0.1894 - op_conv_loss: 0.1180 - avg_loss: 0.1454 - op_main_accuracy: 0.9329 - op_conv_accuracy: 0.9577 - avg_accuracy: 0.9530 - val_loss: 1.4064 - val_op_main_loss: 0.3358 - val_op_conv_loss: 0.4219 - val_avg_loss: 0.3403 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8735\n",
      "Epoch 179/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7268 - op_main_loss: 0.1758 - op_conv_loss: 0.1081 - avg_loss: 0.1342 - op_main_accuracy: 0.9403 - op_conv_accuracy: 0.9588 - avg_accuracy: 0.9579\n",
      "Epoch 00179: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7266 - op_main_loss: 0.1758 - op_conv_loss: 0.1080 - avg_loss: 0.1341 - op_main_accuracy: 0.9405 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9579 - val_loss: 1.4250 - val_op_main_loss: 0.3581 - val_op_conv_loss: 0.4097 - val_avg_loss: 0.3491 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8763\n",
      "Epoch 180/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7372 - op_main_loss: 0.1787 - op_conv_loss: 0.1131 - avg_loss: 0.1373 - op_main_accuracy: 0.9361 - op_conv_accuracy: 0.9583 - avg_accuracy: 0.9568\n",
      "Epoch 00180: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7374 - op_main_loss: 0.1790 - op_conv_loss: 0.1129 - avg_loss: 0.1374 - op_main_accuracy: 0.9360 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9572 - val_loss: 1.3047 - val_op_main_loss: 0.3073 - val_op_conv_loss: 0.3834 - val_avg_loss: 0.3059 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8886\n",
      "Epoch 181/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7259 - op_main_loss: 0.1753 - op_conv_loss: 0.1083 - avg_loss: 0.1343 - op_main_accuracy: 0.9401 - op_conv_accuracy: 0.9575 - avg_accuracy: 0.9555\n",
      "Epoch 00181: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7294 - op_main_loss: 0.1763 - op_conv_loss: 0.1096 - avg_loss: 0.1355 - op_main_accuracy: 0.9388 - op_conv_accuracy: 0.9568 - avg_accuracy: 0.9549 - val_loss: 1.2999 - val_op_main_loss: 0.3014 - val_op_conv_loss: 0.3858 - val_avg_loss: 0.3051 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8848\n",
      "Epoch 182/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7487 - op_main_loss: 0.1827 - op_conv_loss: 0.1162 - avg_loss: 0.1426 - op_main_accuracy: 0.9354 - op_conv_accuracy: 0.9518 - avg_accuracy: 0.9532\n",
      "Epoch 00182: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7484 - op_main_loss: 0.1827 - op_conv_loss: 0.1161 - avg_loss: 0.1425 - op_main_accuracy: 0.9350 - op_conv_accuracy: 0.9523 - avg_accuracy: 0.9534 - val_loss: 1.4117 - val_op_main_loss: 0.3418 - val_op_conv_loss: 0.4192 - val_avg_loss: 0.3434 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8820\n",
      "Epoch 183/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7221 - op_main_loss: 0.1746 - op_conv_loss: 0.1073 - avg_loss: 0.1335 - op_main_accuracy: 0.9392 - op_conv_accuracy: 0.9608 - avg_accuracy: 0.9581\n",
      "Epoch 00183: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7234 - op_main_loss: 0.1753 - op_conv_loss: 0.1075 - avg_loss: 0.1339 - op_main_accuracy: 0.9393 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9572 - val_loss: 1.3604 - val_op_main_loss: 0.3248 - val_op_conv_loss: 0.4021 - val_avg_loss: 0.3268 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8829\n",
      "Epoch 184/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7055 - op_main_loss: 0.1726 - op_conv_loss: 0.0976 - avg_loss: 0.1282 - op_main_accuracy: 0.9403 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9633\n",
      "Epoch 00184: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7050 - op_main_loss: 0.1724 - op_conv_loss: 0.0974 - avg_loss: 0.1280 - op_main_accuracy: 0.9405 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9634 - val_loss: 1.2960 - val_op_main_loss: 0.3033 - val_op_conv_loss: 0.3824 - val_avg_loss: 0.3029 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8820\n",
      "Epoch 185/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7058 - op_main_loss: 0.1682 - op_conv_loss: 0.1023 - avg_loss: 0.1280 - op_main_accuracy: 0.9474 - op_conv_accuracy: 0.9652 - avg_accuracy: 0.9657\n",
      "Epoch 00185: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7058 - op_main_loss: 0.1682 - op_conv_loss: 0.1023 - avg_loss: 0.1280 - op_main_accuracy: 0.9475 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9657 - val_loss: 1.4168 - val_op_main_loss: 0.3430 - val_op_conv_loss: 0.4234 - val_avg_loss: 0.3433 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8857\n",
      "Epoch 186/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.7294 - op_main_loss: 0.1773 - op_conv_loss: 0.1099 - avg_loss: 0.1357 - op_main_accuracy: 0.9339 - op_conv_accuracy: 0.9576 - avg_accuracy: 0.9564\n",
      "Epoch 00186: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7298 - op_main_loss: 0.1776 - op_conv_loss: 0.1099 - avg_loss: 0.1358 - op_main_accuracy: 0.9341 - op_conv_accuracy: 0.9575 - avg_accuracy: 0.9565 - val_loss: 1.3204 - val_op_main_loss: 0.3137 - val_op_conv_loss: 0.3883 - val_avg_loss: 0.3123 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8895\n",
      "Epoch 187/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7285 - op_main_loss: 0.1733 - op_conv_loss: 0.1139 - avg_loss: 0.1353 - op_main_accuracy: 0.9380 - op_conv_accuracy: 0.9574 - avg_accuracy: 0.9540\n",
      "Epoch 00187: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7282 - op_main_loss: 0.1735 - op_conv_loss: 0.1135 - avg_loss: 0.1352 - op_main_accuracy: 0.9374 - op_conv_accuracy: 0.9575 - avg_accuracy: 0.9539 - val_loss: 1.2892 - val_op_main_loss: 0.3069 - val_op_conv_loss: 0.3736 - val_avg_loss: 0.3031 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8905\n",
      "Epoch 188/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7348 - op_main_loss: 0.1764 - op_conv_loss: 0.1148 - avg_loss: 0.1380 - op_main_accuracy: 0.9416 - op_conv_accuracy: 0.9535 - avg_accuracy: 0.9532\n",
      "Epoch 00188: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7394 - op_main_loss: 0.1774 - op_conv_loss: 0.1169 - avg_loss: 0.1394 - op_main_accuracy: 0.9412 - op_conv_accuracy: 0.9532 - avg_accuracy: 0.9532 - val_loss: 1.3679 - val_op_main_loss: 0.3388 - val_op_conv_loss: 0.3923 - val_avg_loss: 0.3313 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8801\n",
      "Epoch 189/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7290 - op_main_loss: 0.1755 - op_conv_loss: 0.1116 - avg_loss: 0.1364 - op_main_accuracy: 0.9369 - op_conv_accuracy: 0.9544 - avg_accuracy: 0.9560\n",
      "Epoch 00189: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7290 - op_main_loss: 0.1755 - op_conv_loss: 0.1116 - avg_loss: 0.1364 - op_main_accuracy: 0.9369 - op_conv_accuracy: 0.9544 - avg_accuracy: 0.9560 - val_loss: 1.2911 - val_op_main_loss: 0.3031 - val_op_conv_loss: 0.3794 - val_avg_loss: 0.3031 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8886\n",
      "Epoch 190/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7390 - op_main_loss: 0.1805 - op_conv_loss: 0.1139 - avg_loss: 0.1389 - op_main_accuracy: 0.9351 - op_conv_accuracy: 0.9547 - avg_accuracy: 0.9544\n",
      "Epoch 00190: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7395 - op_main_loss: 0.1806 - op_conv_loss: 0.1141 - avg_loss: 0.1391 - op_main_accuracy: 0.9353 - op_conv_accuracy: 0.9546 - avg_accuracy: 0.9544 - val_loss: 1.3223 - val_op_main_loss: 0.3146 - val_op_conv_loss: 0.3884 - val_avg_loss: 0.3138 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8905\n",
      "Epoch 191/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7407 - op_main_loss: 0.1819 - op_conv_loss: 0.1134 - avg_loss: 0.1400 - op_main_accuracy: 0.9361 - op_conv_accuracy: 0.9550 - avg_accuracy: 0.9563\n",
      "Epoch 00191: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7380 - op_main_loss: 0.1811 - op_conv_loss: 0.1123 - avg_loss: 0.1392 - op_main_accuracy: 0.9367 - op_conv_accuracy: 0.9556 - avg_accuracy: 0.9570 - val_loss: 1.3952 - val_op_main_loss: 0.3324 - val_op_conv_loss: 0.4199 - val_avg_loss: 0.3376 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8782\n",
      "Epoch 192/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7148 - op_main_loss: 0.1711 - op_conv_loss: 0.1066 - avg_loss: 0.1320 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9556\n",
      "Epoch 00192: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7148 - op_main_loss: 0.1711 - op_conv_loss: 0.1066 - avg_loss: 0.1320 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9556 - val_loss: 1.5916 - val_op_main_loss: 0.3656 - val_op_conv_loss: 0.5288 - val_avg_loss: 0.3923 - val_op_main_accuracy: 0.8650 - val_op_conv_accuracy: 0.8546 - val_avg_accuracy: 0.8574\n",
      "Epoch 193/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7112 - op_main_loss: 0.1711 - op_conv_loss: 0.1044 - avg_loss: 0.1310 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9589\n",
      "Epoch 00193: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7112 - op_main_loss: 0.1711 - op_conv_loss: 0.1044 - avg_loss: 0.1310 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9589 - val_loss: 1.3147 - val_op_main_loss: 0.3043 - val_op_conv_loss: 0.3962 - val_avg_loss: 0.3100 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8754\n",
      "Epoch 194/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7334 - op_main_loss: 0.1778 - op_conv_loss: 0.1138 - avg_loss: 0.1374 - op_main_accuracy: 0.9369 - op_conv_accuracy: 0.9570 - avg_accuracy: 0.9551\n",
      "Epoch 00194: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7334 - op_main_loss: 0.1778 - op_conv_loss: 0.1138 - avg_loss: 0.1374 - op_main_accuracy: 0.9369 - op_conv_accuracy: 0.9570 - avg_accuracy: 0.9551 - val_loss: 1.3348 - val_op_main_loss: 0.3204 - val_op_conv_loss: 0.3948 - val_avg_loss: 0.3152 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8886\n",
      "Epoch 195/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7186 - op_main_loss: 0.1772 - op_conv_loss: 0.1036 - avg_loss: 0.1333 - op_main_accuracy: 0.9411 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9566\n",
      "Epoch 00195: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7171 - op_main_loss: 0.1766 - op_conv_loss: 0.1032 - avg_loss: 0.1329 - op_main_accuracy: 0.9414 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9565 - val_loss: 1.3516 - val_op_main_loss: 0.3387 - val_op_conv_loss: 0.3811 - val_avg_loss: 0.3269 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8886\n",
      "Epoch 196/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7200 - op_main_loss: 0.1725 - op_conv_loss: 0.1088 - avg_loss: 0.1335 - op_main_accuracy: 0.9436 - op_conv_accuracy: 0.9588 - avg_accuracy: 0.9591\n",
      "Epoch 00196: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7211 - op_main_loss: 0.1731 - op_conv_loss: 0.1090 - avg_loss: 0.1338 - op_main_accuracy: 0.9428 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9586 - val_loss: 1.2996 - val_op_main_loss: 0.3097 - val_op_conv_loss: 0.3773 - val_avg_loss: 0.3080 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8895\n",
      "Epoch 197/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7094 - op_main_loss: 0.1712 - op_conv_loss: 0.1030 - avg_loss: 0.1304 - op_main_accuracy: 0.9428 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9591\n",
      "Epoch 00197: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7094 - op_main_loss: 0.1713 - op_conv_loss: 0.1030 - avg_loss: 0.1304 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9589 - val_loss: 1.3987 - val_op_main_loss: 0.3334 - val_op_conv_loss: 0.4241 - val_avg_loss: 0.3363 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8857\n",
      "Epoch 198/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.7174 - op_main_loss: 0.1743 - op_conv_loss: 0.1060 - avg_loss: 0.1326 - op_main_accuracy: 0.9358 - op_conv_accuracy: 0.9581 - avg_accuracy: 0.9574\n",
      "Epoch 00198: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7172 - op_main_loss: 0.1742 - op_conv_loss: 0.1059 - avg_loss: 0.1325 - op_main_accuracy: 0.9360 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9575 - val_loss: 1.4098 - val_op_main_loss: 0.3380 - val_op_conv_loss: 0.4253 - val_avg_loss: 0.3427 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8754\n",
      "Epoch 199/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7155 - op_main_loss: 0.1705 - op_conv_loss: 0.1089 - avg_loss: 0.1320 - op_main_accuracy: 0.9401 - op_conv_accuracy: 0.9553 - avg_accuracy: 0.9558\n",
      "Epoch 00199: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7176 - op_main_loss: 0.1711 - op_conv_loss: 0.1097 - avg_loss: 0.1327 - op_main_accuracy: 0.9397 - op_conv_accuracy: 0.9539 - avg_accuracy: 0.9553 - val_loss: 1.5459 - val_op_main_loss: 0.3591 - val_op_conv_loss: 0.5028 - val_avg_loss: 0.3798 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8574 - val_avg_accuracy: 0.8621\n",
      "Epoch 200/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7187 - op_main_loss: 0.1754 - op_conv_loss: 0.1062 - avg_loss: 0.1331 - op_main_accuracy: 0.9404 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9583\n",
      "Epoch 00200: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7216 - op_main_loss: 0.1760 - op_conv_loss: 0.1076 - avg_loss: 0.1340 - op_main_accuracy: 0.9402 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9579 - val_loss: 1.2796 - val_op_main_loss: 0.3046 - val_op_conv_loss: 0.3685 - val_avg_loss: 0.3029 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8933\n",
      "Epoch 201/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7153 - op_main_loss: 0.1692 - op_conv_loss: 0.1099 - avg_loss: 0.1323 - op_main_accuracy: 0.9403 - op_conv_accuracy: 0.9609 - avg_accuracy: 0.9576\n",
      "Epoch 00201: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7158 - op_main_loss: 0.1695 - op_conv_loss: 0.1100 - avg_loss: 0.1324 - op_main_accuracy: 0.9402 - op_conv_accuracy: 0.9608 - avg_accuracy: 0.9575 - val_loss: 1.3214 - val_op_main_loss: 0.3104 - val_op_conv_loss: 0.3950 - val_avg_loss: 0.3118 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8848\n",
      "Epoch 202/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7003 - op_main_loss: 0.1674 - op_conv_loss: 0.1012 - avg_loss: 0.1276 - op_main_accuracy: 0.9451 - op_conv_accuracy: 0.9602 - avg_accuracy: 0.9602\n",
      "Epoch 00202: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7000 - op_main_loss: 0.1673 - op_conv_loss: 0.1010 - avg_loss: 0.1275 - op_main_accuracy: 0.9452 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9605 - val_loss: 1.2965 - val_op_main_loss: 0.3097 - val_op_conv_loss: 0.3769 - val_avg_loss: 0.3065 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8905\n",
      "Epoch 203/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7169 - op_main_loss: 0.1710 - op_conv_loss: 0.1097 - avg_loss: 0.1327 - op_main_accuracy: 0.9416 - op_conv_accuracy: 0.9572 - avg_accuracy: 0.9555\n",
      "Epoch 00203: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7203 - op_main_loss: 0.1721 - op_conv_loss: 0.1109 - avg_loss: 0.1338 - op_main_accuracy: 0.9409 - op_conv_accuracy: 0.9568 - avg_accuracy: 0.9549 - val_loss: 1.3031 - val_op_main_loss: 0.3099 - val_op_conv_loss: 0.3805 - val_avg_loss: 0.3094 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8876\n",
      "Epoch 204/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.7343 - op_main_loss: 0.1799 - op_conv_loss: 0.1124 - avg_loss: 0.1388 - op_main_accuracy: 0.9345 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9534\n",
      "Epoch 00204: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7343 - op_main_loss: 0.1799 - op_conv_loss: 0.1124 - avg_loss: 0.1388 - op_main_accuracy: 0.9345 - op_conv_accuracy: 0.9582 - avg_accuracy: 0.9534 - val_loss: 1.4232 - val_op_main_loss: 0.3422 - val_op_conv_loss: 0.4298 - val_avg_loss: 0.3476 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8716\n",
      "Epoch 205/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7037 - op_main_loss: 0.1652 - op_conv_loss: 0.1062 - avg_loss: 0.1285 - op_main_accuracy: 0.9474 - op_conv_accuracy: 0.9595 - avg_accuracy: 0.9605\n",
      "Epoch 00205: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7031 - op_main_loss: 0.1649 - op_conv_loss: 0.1060 - avg_loss: 0.1283 - op_main_accuracy: 0.9475 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9605 - val_loss: 1.3205 - val_op_main_loss: 0.3210 - val_op_conv_loss: 0.3796 - val_avg_loss: 0.3159 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8839\n",
      "Epoch 206/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7063 - op_main_loss: 0.1695 - op_conv_loss: 0.1039 - avg_loss: 0.1293 - op_main_accuracy: 0.9401 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9608\n",
      "Epoch 00206: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7062 - op_main_loss: 0.1692 - op_conv_loss: 0.1040 - avg_loss: 0.1292 - op_main_accuracy: 0.9405 - op_conv_accuracy: 0.9594 - avg_accuracy: 0.9608 - val_loss: 1.4464 - val_op_main_loss: 0.3285 - val_op_conv_loss: 0.4678 - val_avg_loss: 0.3472 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8631 - val_avg_accuracy: 0.8772\n",
      "Epoch 207/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7212 - op_main_loss: 0.1721 - op_conv_loss: 0.1119 - avg_loss: 0.1343 - op_main_accuracy: 0.9404 - op_conv_accuracy: 0.9590 - avg_accuracy: 0.9575\n",
      "Epoch 00207: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7208 - op_main_loss: 0.1720 - op_conv_loss: 0.1117 - avg_loss: 0.1341 - op_main_accuracy: 0.9402 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9575 - val_loss: 1.5107 - val_op_main_loss: 0.3400 - val_op_conv_loss: 0.5032 - val_avg_loss: 0.3646 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8650\n",
      "Epoch 208/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7165 - op_main_loss: 0.1717 - op_conv_loss: 0.1086 - avg_loss: 0.1335 - op_main_accuracy: 0.9440 - op_conv_accuracy: 0.9595 - avg_accuracy: 0.9598\n",
      "Epoch 00208: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7189 - op_main_loss: 0.1722 - op_conv_loss: 0.1099 - avg_loss: 0.1342 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9594 - val_loss: 1.3136 - val_op_main_loss: 0.3110 - val_op_conv_loss: 0.3888 - val_avg_loss: 0.3111 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8857\n",
      "Epoch 209/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6694 - op_main_loss: 0.1565 - op_conv_loss: 0.0915 - avg_loss: 0.1185 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9642 - avg_accuracy: 0.9645\n",
      "Epoch 00209: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6678 - op_main_loss: 0.1560 - op_conv_loss: 0.0909 - avg_loss: 0.1179 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9648 - val_loss: 1.3646 - val_op_main_loss: 0.3334 - val_op_conv_loss: 0.4007 - val_avg_loss: 0.3271 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8914\n",
      "Epoch 210/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6892 - op_main_loss: 0.1661 - op_conv_loss: 0.0959 - avg_loss: 0.1243 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9625 - avg_accuracy: 0.9620\n",
      "Epoch 00210: val_avg_accuracy did not improve from 0.89518\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6940 - op_main_loss: 0.1676 - op_conv_loss: 0.0978 - avg_loss: 0.1258 - op_main_accuracy: 0.9416 - op_conv_accuracy: 0.9617 - avg_accuracy: 0.9615 - val_loss: 1.3985 - val_op_main_loss: 0.3394 - val_op_conv_loss: 0.4221 - val_avg_loss: 0.3344 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8791\n",
      "Epoch 211/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6962 - op_main_loss: 0.1655 - op_conv_loss: 0.1015 - avg_loss: 0.1265 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9591 - avg_accuracy: 0.9583\n",
      "Epoch 00211: val_avg_accuracy improved from 0.89518 to 0.89896, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6976 - op_main_loss: 0.1655 - op_conv_loss: 0.1025 - avg_loss: 0.1270 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9586 - avg_accuracy: 0.9579 - val_loss: 1.3069 - val_op_main_loss: 0.3149 - val_op_conv_loss: 0.3798 - val_avg_loss: 0.3096 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8990\n",
      "Epoch 212/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6898 - op_main_loss: 0.1624 - op_conv_loss: 0.1001 - avg_loss: 0.1248 - op_main_accuracy: 0.9469 - op_conv_accuracy: 0.9588 - avg_accuracy: 0.9583\n",
      "Epoch 00212: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6930 - op_main_loss: 0.1638 - op_conv_loss: 0.1008 - avg_loss: 0.1259 - op_main_accuracy: 0.9461 - op_conv_accuracy: 0.9584 - avg_accuracy: 0.9579 - val_loss: 1.2921 - val_op_main_loss: 0.3120 - val_op_conv_loss: 0.3729 - val_avg_loss: 0.3051 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8914\n",
      "Epoch 213/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7135 - op_main_loss: 0.1676 - op_conv_loss: 0.1120 - avg_loss: 0.1319 - op_main_accuracy: 0.9420 - op_conv_accuracy: 0.9566 - avg_accuracy: 0.9566\n",
      "Epoch 00213: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7121 - op_main_loss: 0.1671 - op_conv_loss: 0.1115 - avg_loss: 0.1314 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9568 - avg_accuracy: 0.9568 - val_loss: 1.6417 - val_op_main_loss: 0.3881 - val_op_conv_loss: 0.5420 - val_avg_loss: 0.4092 - val_op_main_accuracy: 0.8546 - val_op_conv_accuracy: 0.8480 - val_avg_accuracy: 0.8517\n",
      "Epoch 214/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6621 - op_main_loss: 0.1580 - op_conv_loss: 0.0863 - avg_loss: 0.1155 - op_main_accuracy: 0.9451 - op_conv_accuracy: 0.9673 - avg_accuracy: 0.9668\n",
      "Epoch 00214: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6630 - op_main_loss: 0.1581 - op_conv_loss: 0.0868 - avg_loss: 0.1158 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9664 - val_loss: 1.3311 - val_op_main_loss: 0.3186 - val_op_conv_loss: 0.3964 - val_avg_loss: 0.3139 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8933\n",
      "Epoch 215/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6920 - op_main_loss: 0.1671 - op_conv_loss: 0.0977 - avg_loss: 0.1256 - op_main_accuracy: 0.9426 - op_conv_accuracy: 0.9620 - avg_accuracy: 0.9617\n",
      "Epoch 00215: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6931 - op_main_loss: 0.1675 - op_conv_loss: 0.0979 - avg_loss: 0.1259 - op_main_accuracy: 0.9419 - op_conv_accuracy: 0.9617 - avg_accuracy: 0.9612 - val_loss: 1.4480 - val_op_main_loss: 0.3706 - val_op_conv_loss: 0.4184 - val_avg_loss: 0.3573 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8829\n",
      "Epoch 216/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7221 - op_main_loss: 0.1733 - op_conv_loss: 0.1129 - avg_loss: 0.1345 - op_main_accuracy: 0.9392 - op_conv_accuracy: 0.9559 - avg_accuracy: 0.9523\n",
      "Epoch 00216: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7217 - op_main_loss: 0.1734 - op_conv_loss: 0.1126 - avg_loss: 0.1344 - op_main_accuracy: 0.9393 - op_conv_accuracy: 0.9560 - avg_accuracy: 0.9525 - val_loss: 1.4453 - val_op_main_loss: 0.3425 - val_op_conv_loss: 0.4505 - val_avg_loss: 0.3512 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8735\n",
      "Epoch 217/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6729 - op_main_loss: 0.1571 - op_conv_loss: 0.0949 - avg_loss: 0.1197 - op_main_accuracy: 0.9499 - op_conv_accuracy: 0.9649 - avg_accuracy: 0.9659\n",
      "Epoch 00217: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6742 - op_main_loss: 0.1574 - op_conv_loss: 0.0956 - avg_loss: 0.1200 - op_main_accuracy: 0.9497 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9655 - val_loss: 1.4186 - val_op_main_loss: 0.3321 - val_op_conv_loss: 0.4433 - val_avg_loss: 0.3416 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8772\n",
      "Epoch 218/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6729 - op_main_loss: 0.1621 - op_conv_loss: 0.0900 - avg_loss: 0.1196 - op_main_accuracy: 0.9447 - op_conv_accuracy: 0.9647 - avg_accuracy: 0.9659\n",
      "Epoch 00218: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6755 - op_main_loss: 0.1626 - op_conv_loss: 0.0913 - avg_loss: 0.1204 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9650 - val_loss: 1.4463 - val_op_main_loss: 0.3662 - val_op_conv_loss: 0.4298 - val_avg_loss: 0.3488 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8839\n",
      "Epoch 219/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6903 - op_main_loss: 0.1647 - op_conv_loss: 0.0993 - avg_loss: 0.1250 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9620 - avg_accuracy: 0.9601\n",
      "Epoch 00219: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6903 - op_main_loss: 0.1647 - op_conv_loss: 0.0993 - avg_loss: 0.1250 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9620 - avg_accuracy: 0.9601 - val_loss: 1.2947 - val_op_main_loss: 0.3141 - val_op_conv_loss: 0.3711 - val_avg_loss: 0.3084 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8867\n",
      "Epoch 220/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6773 - op_main_loss: 0.1573 - op_conv_loss: 0.0983 - avg_loss: 0.1210 - op_main_accuracy: 0.9462 - op_conv_accuracy: 0.9632 - avg_accuracy: 0.9617\n",
      "Epoch 00220: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6783 - op_main_loss: 0.1580 - op_conv_loss: 0.0982 - avg_loss: 0.1213 - op_main_accuracy: 0.9457 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9615 - val_loss: 1.4553 - val_op_main_loss: 0.3464 - val_op_conv_loss: 0.4544 - val_avg_loss: 0.3542 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8820\n",
      "Epoch 221/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7032 - op_main_loss: 0.1683 - op_conv_loss: 0.1055 - avg_loss: 0.1291 - op_main_accuracy: 0.9409 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9582\n",
      "Epoch 00221: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7007 - op_main_loss: 0.1675 - op_conv_loss: 0.1044 - avg_loss: 0.1283 - op_main_accuracy: 0.9414 - op_conv_accuracy: 0.9601 - avg_accuracy: 0.9586 - val_loss: 1.3251 - val_op_main_loss: 0.3169 - val_op_conv_loss: 0.3932 - val_avg_loss: 0.3146 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8801\n",
      "Epoch 222/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.7218 - op_main_loss: 0.1750 - op_conv_loss: 0.1112 - avg_loss: 0.1349 - op_main_accuracy: 0.9363 - op_conv_accuracy: 0.9562 - avg_accuracy: 0.9564\n",
      "Epoch 00222: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7180 - op_main_loss: 0.1738 - op_conv_loss: 0.1098 - avg_loss: 0.1338 - op_main_accuracy: 0.9369 - op_conv_accuracy: 0.9565 - avg_accuracy: 0.9570 - val_loss: 1.3121 - val_op_main_loss: 0.3160 - val_op_conv_loss: 0.3852 - val_avg_loss: 0.3103 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8886\n",
      "Epoch 223/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6852 - op_main_loss: 0.1611 - op_conv_loss: 0.0996 - avg_loss: 0.1240 - op_main_accuracy: 0.9491 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9595\n",
      "Epoch 00223: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6867 - op_main_loss: 0.1620 - op_conv_loss: 0.0998 - avg_loss: 0.1245 - op_main_accuracy: 0.9483 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9594 - val_loss: 1.4250 - val_op_main_loss: 0.3516 - val_op_conv_loss: 0.4253 - val_avg_loss: 0.3481 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8801\n",
      "Epoch 224/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6884 - op_main_loss: 0.1618 - op_conv_loss: 0.1015 - avg_loss: 0.1251 - op_main_accuracy: 0.9491 - op_conv_accuracy: 0.9617 - avg_accuracy: 0.9639\n",
      "Epoch 00224: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6868 - op_main_loss: 0.1614 - op_conv_loss: 0.1008 - avg_loss: 0.1247 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9617 - avg_accuracy: 0.9643 - val_loss: 1.3179 - val_op_main_loss: 0.3149 - val_op_conv_loss: 0.3892 - val_avg_loss: 0.3137 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8914\n",
      "Epoch 225/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6868 - op_main_loss: 0.1630 - op_conv_loss: 0.0989 - avg_loss: 0.1246 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9646\n",
      "Epoch 00225: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6868 - op_main_loss: 0.1630 - op_conv_loss: 0.0989 - avg_loss: 0.1246 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9646 - val_loss: 1.3553 - val_op_main_loss: 0.3264 - val_op_conv_loss: 0.4038 - val_avg_loss: 0.3245 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8914\n",
      "Epoch 226/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6935 - op_main_loss: 0.1621 - op_conv_loss: 0.1044 - avg_loss: 0.1262 - op_main_accuracy: 0.9415 - op_conv_accuracy: 0.9588 - avg_accuracy: 0.9569\n",
      "Epoch 00226: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6928 - op_main_loss: 0.1618 - op_conv_loss: 0.1042 - avg_loss: 0.1260 - op_main_accuracy: 0.9416 - op_conv_accuracy: 0.9589 - avg_accuracy: 0.9570 - val_loss: 1.3748 - val_op_main_loss: 0.3297 - val_op_conv_loss: 0.4151 - val_avg_loss: 0.3294 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 227/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6743 - op_main_loss: 0.1601 - op_conv_loss: 0.0931 - avg_loss: 0.1207 - op_main_accuracy: 0.9467 - op_conv_accuracy: 0.9645 - avg_accuracy: 0.9652\n",
      "Epoch 00227: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6751 - op_main_loss: 0.1602 - op_conv_loss: 0.0935 - avg_loss: 0.1209 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9650 - val_loss: 1.4058 - val_op_main_loss: 0.3238 - val_op_conv_loss: 0.4494 - val_avg_loss: 0.3318 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8725\n",
      "Epoch 228/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.7021 - op_main_loss: 0.1701 - op_conv_loss: 0.1027 - avg_loss: 0.1286 - op_main_accuracy: 0.9415 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9574\n",
      "Epoch 00228: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7016 - op_main_loss: 0.1699 - op_conv_loss: 0.1026 - avg_loss: 0.1285 - op_main_accuracy: 0.9416 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9575 - val_loss: 1.3460 - val_op_main_loss: 0.3191 - val_op_conv_loss: 0.4042 - val_avg_loss: 0.3224 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8848\n",
      "Epoch 229/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6547 - op_main_loss: 0.1510 - op_conv_loss: 0.0890 - avg_loss: 0.1143 - op_main_accuracy: 0.9498 - op_conv_accuracy: 0.9647 - avg_accuracy: 0.9654\n",
      "Epoch 00229: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6555 - op_main_loss: 0.1511 - op_conv_loss: 0.0894 - avg_loss: 0.1146 - op_main_accuracy: 0.9497 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9648 - val_loss: 1.4116 - val_op_main_loss: 0.3416 - val_op_conv_loss: 0.4264 - val_avg_loss: 0.3433 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8791\n",
      "Epoch 230/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6797 - op_main_loss: 0.1609 - op_conv_loss: 0.0972 - avg_loss: 0.1221 - op_main_accuracy: 0.9482 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9626\n",
      "Epoch 00230: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6799 - op_main_loss: 0.1611 - op_conv_loss: 0.0972 - avg_loss: 0.1221 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9624 - val_loss: 1.5188 - val_op_main_loss: 0.3863 - val_op_conv_loss: 0.4530 - val_avg_loss: 0.3799 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8744 - val_avg_accuracy: 0.8744\n",
      "Epoch 231/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6866 - op_main_loss: 0.1633 - op_conv_loss: 0.1008 - avg_loss: 0.1229 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9629\n",
      "Epoch 00231: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6866 - op_main_loss: 0.1633 - op_conv_loss: 0.1008 - avg_loss: 0.1229 - op_main_accuracy: 0.9442 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9629 - val_loss: 1.4380 - val_op_main_loss: 0.3402 - val_op_conv_loss: 0.4494 - val_avg_loss: 0.3488 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8801\n",
      "Epoch 232/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.7024 - op_main_loss: 0.1683 - op_conv_loss: 0.1058 - avg_loss: 0.1291 - op_main_accuracy: 0.9397 - op_conv_accuracy: 0.9591 - avg_accuracy: 0.9578\n",
      "Epoch 00232: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7003 - op_main_loss: 0.1676 - op_conv_loss: 0.1050 - avg_loss: 0.1284 - op_main_accuracy: 0.9400 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9584 - val_loss: 1.3373 - val_op_main_loss: 0.3221 - val_op_conv_loss: 0.3951 - val_avg_loss: 0.3210 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8942\n",
      "Epoch 233/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6731 - op_main_loss: 0.1562 - op_conv_loss: 0.0973 - avg_loss: 0.1205 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9608 - avg_accuracy: 0.9605\n",
      "Epoch 00233: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6746 - op_main_loss: 0.1563 - op_conv_loss: 0.0983 - avg_loss: 0.1210 - op_main_accuracy: 0.9438 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9603 - val_loss: 1.3951 - val_op_main_loss: 0.3362 - val_op_conv_loss: 0.4243 - val_avg_loss: 0.3359 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8933\n",
      "Epoch 234/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.6717 - op_main_loss: 0.1587 - op_conv_loss: 0.0942 - avg_loss: 0.1198 - op_main_accuracy: 0.9469 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9608\n",
      "Epoch 00234: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6705 - op_main_loss: 0.1582 - op_conv_loss: 0.0938 - avg_loss: 0.1194 - op_main_accuracy: 0.9473 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9612 - val_loss: 1.4336 - val_op_main_loss: 0.3294 - val_op_conv_loss: 0.4625 - val_avg_loss: 0.3424 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8857\n",
      "Epoch 235/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6608 - op_main_loss: 0.1533 - op_conv_loss: 0.0920 - avg_loss: 0.1163 - op_main_accuracy: 0.9547 - op_conv_accuracy: 0.9632 - avg_accuracy: 0.9617\n",
      "Epoch 00235: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6612 - op_main_loss: 0.1535 - op_conv_loss: 0.0921 - avg_loss: 0.1164 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9617 - val_loss: 1.4099 - val_op_main_loss: 0.3386 - val_op_conv_loss: 0.4310 - val_avg_loss: 0.3421 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8820\n",
      "Epoch 236/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6650 - op_main_loss: 0.1545 - op_conv_loss: 0.0941 - avg_loss: 0.1182 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9612\n",
      "Epoch 00236: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6650 - op_main_loss: 0.1545 - op_conv_loss: 0.0941 - avg_loss: 0.1182 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9612 - val_loss: 1.5262 - val_op_main_loss: 0.3599 - val_op_conv_loss: 0.4920 - val_avg_loss: 0.3761 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8697 - val_avg_accuracy: 0.8697\n",
      "Epoch 237/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.7184 - op_main_loss: 0.1786 - op_conv_loss: 0.1075 - avg_loss: 0.1345 - op_main_accuracy: 0.9320 - op_conv_accuracy: 0.9584 - avg_accuracy: 0.9538\n",
      "Epoch 00237: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7204 - op_main_loss: 0.1789 - op_conv_loss: 0.1085 - avg_loss: 0.1352 - op_main_accuracy: 0.9312 - op_conv_accuracy: 0.9579 - avg_accuracy: 0.9530 - val_loss: 1.5541 - val_op_main_loss: 0.3594 - val_op_conv_loss: 0.5141 - val_avg_loss: 0.3831 - val_op_main_accuracy: 0.8706 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8621\n",
      "Epoch 238/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6787 - op_main_loss: 0.1612 - op_conv_loss: 0.0974 - avg_loss: 0.1219 - op_main_accuracy: 0.9440 - op_conv_accuracy: 0.9630 - avg_accuracy: 0.9611\n",
      "Epoch 00238: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6782 - op_main_loss: 0.1610 - op_conv_loss: 0.0972 - avg_loss: 0.1217 - op_main_accuracy: 0.9445 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9612 - val_loss: 1.3946 - val_op_main_loss: 0.3343 - val_op_conv_loss: 0.4229 - val_avg_loss: 0.3393 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8848\n",
      "Epoch 239/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6745 - op_main_loss: 0.1614 - op_conv_loss: 0.0939 - avg_loss: 0.1213 - op_main_accuracy: 0.9462 - op_conv_accuracy: 0.9663 - avg_accuracy: 0.9654\n",
      "Epoch 00239: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6734 - op_main_loss: 0.1609 - op_conv_loss: 0.0937 - avg_loss: 0.1210 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9657 - val_loss: 1.4484 - val_op_main_loss: 0.3433 - val_op_conv_loss: 0.4536 - val_avg_loss: 0.3538 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8763\n",
      "Epoch 240/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6500 - op_main_loss: 0.1492 - op_conv_loss: 0.0896 - avg_loss: 0.1131 - op_main_accuracy: 0.9503 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9638\n",
      "Epoch 00240: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6493 - op_main_loss: 0.1490 - op_conv_loss: 0.0894 - avg_loss: 0.1129 - op_main_accuracy: 0.9504 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9638 - val_loss: 1.4756 - val_op_main_loss: 0.3496 - val_op_conv_loss: 0.4675 - val_avg_loss: 0.3606 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8763\n",
      "Epoch 241/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6758 - op_main_loss: 0.1579 - op_conv_loss: 0.0989 - avg_loss: 0.1212 - op_main_accuracy: 0.9476 - op_conv_accuracy: 0.9642 - avg_accuracy: 0.9620\n",
      "Epoch 00241: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6760 - op_main_loss: 0.1582 - op_conv_loss: 0.0988 - avg_loss: 0.1213 - op_main_accuracy: 0.9475 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9620 - val_loss: 1.3003 - val_op_main_loss: 0.3032 - val_op_conv_loss: 0.3930 - val_avg_loss: 0.3065 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8848\n",
      "Epoch 242/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6597 - op_main_loss: 0.1542 - op_conv_loss: 0.0918 - avg_loss: 0.1163 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9630 - avg_accuracy: 0.9602\n",
      "Epoch 00242: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6587 - op_main_loss: 0.1537 - op_conv_loss: 0.0915 - avg_loss: 0.1160 - op_main_accuracy: 0.9485 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9603 - val_loss: 1.3240 - val_op_main_loss: 0.3060 - val_op_conv_loss: 0.4101 - val_avg_loss: 0.3107 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8905\n",
      "Epoch 243/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6541 - op_main_loss: 0.1533 - op_conv_loss: 0.0887 - avg_loss: 0.1151 - op_main_accuracy: 0.9481 - op_conv_accuracy: 0.9673 - avg_accuracy: 0.9651\n",
      "Epoch 00243: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6553 - op_main_loss: 0.1543 - op_conv_loss: 0.0886 - avg_loss: 0.1154 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9672 - avg_accuracy: 0.9646 - val_loss: 1.5251 - val_op_main_loss: 0.3687 - val_op_conv_loss: 0.4819 - val_avg_loss: 0.3776 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8687\n",
      "Epoch 244/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6610 - op_main_loss: 0.1543 - op_conv_loss: 0.0930 - avg_loss: 0.1169 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9638\n",
      "Epoch 00244: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6610 - op_main_loss: 0.1543 - op_conv_loss: 0.0930 - avg_loss: 0.1169 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9638 - val_loss: 1.4056 - val_op_main_loss: 0.3320 - val_op_conv_loss: 0.4386 - val_avg_loss: 0.3384 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8857\n",
      "Epoch 245/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6767 - op_main_loss: 0.1603 - op_conv_loss: 0.0981 - avg_loss: 0.1212 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9620\n",
      "Epoch 00245: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6767 - op_main_loss: 0.1603 - op_conv_loss: 0.0981 - avg_loss: 0.1212 - op_main_accuracy: 0.9433 - op_conv_accuracy: 0.9627 - avg_accuracy: 0.9620 - val_loss: 1.4821 - val_op_main_loss: 0.3732 - val_op_conv_loss: 0.4435 - val_avg_loss: 0.3681 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8791\n",
      "Epoch 246/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.6912 - op_main_loss: 0.1645 - op_conv_loss: 0.1033 - avg_loss: 0.1265 - op_main_accuracy: 0.9408 - op_conv_accuracy: 0.9578 - avg_accuracy: 0.9559\n",
      "Epoch 00246: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6915 - op_main_loss: 0.1642 - op_conv_loss: 0.1039 - avg_loss: 0.1265 - op_main_accuracy: 0.9407 - op_conv_accuracy: 0.9572 - avg_accuracy: 0.9553 - val_loss: 1.3452 - val_op_main_loss: 0.3172 - val_op_conv_loss: 0.4091 - val_avg_loss: 0.3222 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8961\n",
      "Epoch 247/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6490 - op_main_loss: 0.1517 - op_conv_loss: 0.0874 - avg_loss: 0.1134 - op_main_accuracy: 0.9489 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9676\n",
      "Epoch 00247: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6503 - op_main_loss: 0.1521 - op_conv_loss: 0.0880 - avg_loss: 0.1138 - op_main_accuracy: 0.9487 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9674 - val_loss: 1.3465 - val_op_main_loss: 0.3147 - val_op_conv_loss: 0.4185 - val_avg_loss: 0.3168 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8933\n",
      "Epoch 248/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6441 - op_main_loss: 0.1492 - op_conv_loss: 0.0864 - avg_loss: 0.1118 - op_main_accuracy: 0.9554 - op_conv_accuracy: 0.9680 - avg_accuracy: 0.9685\n",
      "Epoch 00248: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6437 - op_main_loss: 0.1496 - op_conv_loss: 0.0857 - avg_loss: 0.1116 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9683 - val_loss: 1.4503 - val_op_main_loss: 0.3340 - val_op_conv_loss: 0.4698 - val_avg_loss: 0.3501 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8782\n",
      "Epoch 249/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6474 - op_main_loss: 0.1484 - op_conv_loss: 0.0898 - avg_loss: 0.1128 - op_main_accuracy: 0.9528 - op_conv_accuracy: 0.9668 - avg_accuracy: 0.9654\n",
      "Epoch 00249: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6475 - op_main_loss: 0.1484 - op_conv_loss: 0.0897 - avg_loss: 0.1128 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9669 - avg_accuracy: 0.9653 - val_loss: 1.3823 - val_op_main_loss: 0.3217 - val_op_conv_loss: 0.4363 - val_avg_loss: 0.3279 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8952\n",
      "Epoch 250/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6568 - op_main_loss: 0.1530 - op_conv_loss: 0.0922 - avg_loss: 0.1156 - op_main_accuracy: 0.9476 - op_conv_accuracy: 0.9651 - avg_accuracy: 0.9627\n",
      "Epoch 00250: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6556 - op_main_loss: 0.1525 - op_conv_loss: 0.0919 - avg_loss: 0.1152 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9631 - val_loss: 1.3038 - val_op_main_loss: 0.3101 - val_op_conv_loss: 0.3895 - val_avg_loss: 0.3084 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8839\n",
      "Epoch 251/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6569 - op_main_loss: 0.1530 - op_conv_loss: 0.0922 - avg_loss: 0.1158 - op_main_accuracy: 0.9495 - op_conv_accuracy: 0.9608 - avg_accuracy: 0.9656\n",
      "Epoch 00251: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6535 - op_main_loss: 0.1516 - op_conv_loss: 0.0912 - avg_loss: 0.1148 - op_main_accuracy: 0.9504 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9662 - val_loss: 1.4267 - val_op_main_loss: 0.3360 - val_op_conv_loss: 0.4508 - val_avg_loss: 0.3439 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8763\n",
      "Epoch 252/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6453 - op_main_loss: 0.1476 - op_conv_loss: 0.0898 - avg_loss: 0.1119 - op_main_accuracy: 0.9520 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9660\n",
      "Epoch 00252: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6453 - op_main_loss: 0.1476 - op_conv_loss: 0.0898 - avg_loss: 0.1119 - op_main_accuracy: 0.9520 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9660 - val_loss: 1.4318 - val_op_main_loss: 0.3439 - val_op_conv_loss: 0.4450 - val_avg_loss: 0.3475 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8857\n",
      "Epoch 253/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6916 - op_main_loss: 0.1677 - op_conv_loss: 0.1025 - avg_loss: 0.1265 - op_main_accuracy: 0.9421 - op_conv_accuracy: 0.9605 - avg_accuracy: 0.9598\n",
      "Epoch 00253: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6917 - op_main_loss: 0.1678 - op_conv_loss: 0.1024 - avg_loss: 0.1267 - op_main_accuracy: 0.9423 - op_conv_accuracy: 0.9603 - avg_accuracy: 0.9598 - val_loss: 1.2905 - val_op_main_loss: 0.2988 - val_op_conv_loss: 0.3963 - val_avg_loss: 0.3020 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8961\n",
      "Epoch 254/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6592 - op_main_loss: 0.1538 - op_conv_loss: 0.0943 - avg_loss: 0.1173 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9657\n",
      "Epoch 00254: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6592 - op_main_loss: 0.1538 - op_conv_loss: 0.0943 - avg_loss: 0.1173 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9657 - val_loss: 1.6999 - val_op_main_loss: 0.4124 - val_op_conv_loss: 0.5613 - val_avg_loss: 0.4319 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8536 - val_avg_accuracy: 0.8593\n",
      "Epoch 255/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6720 - op_main_loss: 0.1606 - op_conv_loss: 0.0961 - avg_loss: 0.1211 - op_main_accuracy: 0.9447 - op_conv_accuracy: 0.9635 - avg_accuracy: 0.9630\n",
      "Epoch 00255: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6708 - op_main_loss: 0.1602 - op_conv_loss: 0.0957 - avg_loss: 0.1207 - op_main_accuracy: 0.9449 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9631 - val_loss: 1.3482 - val_op_main_loss: 0.3191 - val_op_conv_loss: 0.4109 - val_avg_loss: 0.3239 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 256/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6675 - op_main_loss: 0.1550 - op_conv_loss: 0.0978 - avg_loss: 0.1200 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9611 - avg_accuracy: 0.9618\n",
      "Epoch 00256: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6664 - op_main_loss: 0.1547 - op_conv_loss: 0.0974 - avg_loss: 0.1197 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9612 - avg_accuracy: 0.9620 - val_loss: 1.4168 - val_op_main_loss: 0.3316 - val_op_conv_loss: 0.4487 - val_avg_loss: 0.3416 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8876\n",
      "Epoch 257/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6495 - op_main_loss: 0.1531 - op_conv_loss: 0.0882 - avg_loss: 0.1138 - op_main_accuracy: 0.9503 - op_conv_accuracy: 0.9704 - avg_accuracy: 0.9695\n",
      "Epoch 00257: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6495 - op_main_loss: 0.1532 - op_conv_loss: 0.0882 - avg_loss: 0.1138 - op_main_accuracy: 0.9504 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9695 - val_loss: 1.3813 - val_op_main_loss: 0.3282 - val_op_conv_loss: 0.4240 - val_avg_loss: 0.3351 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8848\n",
      "Epoch 258/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6452 - op_main_loss: 0.1481 - op_conv_loss: 0.0897 - avg_loss: 0.1127 - op_main_accuracy: 0.9520 - op_conv_accuracy: 0.9629 - avg_accuracy: 0.9627\n",
      "Epoch 00258: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6423 - op_main_loss: 0.1472 - op_conv_loss: 0.0886 - avg_loss: 0.1118 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9631 - val_loss: 1.3132 - val_op_main_loss: 0.3178 - val_op_conv_loss: 0.3876 - val_avg_loss: 0.3133 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8914\n",
      "Epoch 259/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6378 - op_main_loss: 0.1456 - op_conv_loss: 0.0870 - avg_loss: 0.1107 - op_main_accuracy: 0.9503 - op_conv_accuracy: 0.9671 - avg_accuracy: 0.9643\n",
      "Epoch 00259: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6389 - op_main_loss: 0.1458 - op_conv_loss: 0.0876 - avg_loss: 0.1110 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9669 - avg_accuracy: 0.9641 - val_loss: 1.7091 - val_op_main_loss: 0.4080 - val_op_conv_loss: 0.5765 - val_avg_loss: 0.4303 - val_op_main_accuracy: 0.8621 - val_op_conv_accuracy: 0.8612 - val_avg_accuracy: 0.8650\n",
      "Epoch 260/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6734 - op_main_loss: 0.1599 - op_conv_loss: 0.0978 - avg_loss: 0.1216 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9633 - avg_accuracy: 0.9621\n",
      "Epoch 00260: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6733 - op_main_loss: 0.1602 - op_conv_loss: 0.0974 - avg_loss: 0.1215 - op_main_accuracy: 0.9461 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9622 - val_loss: 1.9562 - val_op_main_loss: 0.4547 - val_op_conv_loss: 0.7078 - val_avg_loss: 0.4999 - val_op_main_accuracy: 0.8461 - val_op_conv_accuracy: 0.8272 - val_avg_accuracy: 0.8357\n",
      "Epoch 261/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6416 - op_main_loss: 0.1479 - op_conv_loss: 0.0882 - avg_loss: 0.1116 - op_main_accuracy: 0.9509 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9636\n",
      "Epoch 00261: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6416 - op_main_loss: 0.1479 - op_conv_loss: 0.0882 - avg_loss: 0.1116 - op_main_accuracy: 0.9509 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9636 - val_loss: 1.4505 - val_op_main_loss: 0.3518 - val_op_conv_loss: 0.4484 - val_avg_loss: 0.3560 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8810\n",
      "Epoch 262/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6643 - op_main_loss: 0.1548 - op_conv_loss: 0.0964 - avg_loss: 0.1189 - op_main_accuracy: 0.9479 - op_conv_accuracy: 0.9609 - avg_accuracy: 0.9612\n",
      "Epoch 00262: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6638 - op_main_loss: 0.1547 - op_conv_loss: 0.0963 - avg_loss: 0.1188 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9610 - avg_accuracy: 0.9612 - val_loss: 1.3457 - val_op_main_loss: 0.3418 - val_op_conv_loss: 0.3863 - val_avg_loss: 0.3233 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8971\n",
      "Epoch 263/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6446 - op_main_loss: 0.1490 - op_conv_loss: 0.0894 - avg_loss: 0.1122 - op_main_accuracy: 0.9485 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9660\n",
      "Epoch 00263: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6446 - op_main_loss: 0.1490 - op_conv_loss: 0.0894 - avg_loss: 0.1122 - op_main_accuracy: 0.9485 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9660 - val_loss: 1.3742 - val_op_main_loss: 0.3275 - val_op_conv_loss: 0.4240 - val_avg_loss: 0.3290 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8886\n",
      "Epoch 264/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6539 - op_main_loss: 0.1524 - op_conv_loss: 0.0924 - avg_loss: 0.1158 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9656 - avg_accuracy: 0.9654\n",
      "Epoch 00264: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6525 - op_main_loss: 0.1517 - op_conv_loss: 0.0921 - avg_loss: 0.1153 - op_main_accuracy: 0.9520 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9657 - val_loss: 1.3683 - val_op_main_loss: 0.3386 - val_op_conv_loss: 0.4039 - val_avg_loss: 0.3325 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8933\n",
      "Epoch 265/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6604 - op_main_loss: 0.1520 - op_conv_loss: 0.0974 - avg_loss: 0.1174 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9598 - avg_accuracy: 0.9615\n",
      "Epoch 00265: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6602 - op_main_loss: 0.1518 - op_conv_loss: 0.0975 - avg_loss: 0.1173 - op_main_accuracy: 0.9509 - op_conv_accuracy: 0.9596 - avg_accuracy: 0.9617 - val_loss: 1.3334 - val_op_main_loss: 0.3195 - val_op_conv_loss: 0.4009 - val_avg_loss: 0.3194 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8914\n",
      "Epoch 266/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6467 - op_main_loss: 0.1505 - op_conv_loss: 0.0897 - avg_loss: 0.1133 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9660\n",
      "Epoch 00266: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6467 - op_main_loss: 0.1505 - op_conv_loss: 0.0897 - avg_loss: 0.1133 - op_main_accuracy: 0.9480 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9660 - val_loss: 1.3687 - val_op_main_loss: 0.3352 - val_op_conv_loss: 0.4087 - val_avg_loss: 0.3319 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8895\n",
      "Epoch 267/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6348 - op_main_loss: 0.1466 - op_conv_loss: 0.0855 - avg_loss: 0.1098 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9705\n",
      "Epoch 00267: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6348 - op_main_loss: 0.1466 - op_conv_loss: 0.0855 - avg_loss: 0.1098 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9705 - val_loss: 1.4610 - val_op_main_loss: 0.3358 - val_op_conv_loss: 0.4791 - val_avg_loss: 0.3532 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8735\n",
      "Epoch 268/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6548 - op_main_loss: 0.1539 - op_conv_loss: 0.0921 - avg_loss: 0.1164 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9635 - avg_accuracy: 0.9632\n",
      "Epoch 00268: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6575 - op_main_loss: 0.1557 - op_conv_loss: 0.0924 - avg_loss: 0.1171 - op_main_accuracy: 0.9459 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9629 - val_loss: 1.3241 - val_op_main_loss: 0.3068 - val_op_conv_loss: 0.4128 - val_avg_loss: 0.3123 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8990\n",
      "Epoch 269/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6397 - op_main_loss: 0.1481 - op_conv_loss: 0.0876 - avg_loss: 0.1114 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9668 - avg_accuracy: 0.9637\n",
      "Epoch 00269: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6366 - op_main_loss: 0.1470 - op_conv_loss: 0.0866 - avg_loss: 0.1104 - op_main_accuracy: 0.9513 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9643 - val_loss: 1.3213 - val_op_main_loss: 0.3096 - val_op_conv_loss: 0.4061 - val_avg_loss: 0.3131 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8924\n",
      "Epoch 270/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6523 - op_main_loss: 0.1550 - op_conv_loss: 0.0898 - avg_loss: 0.1153 - op_main_accuracy: 0.9469 - op_conv_accuracy: 0.9661 - avg_accuracy: 0.9666\n",
      "Epoch 00270: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6504 - op_main_loss: 0.1546 - op_conv_loss: 0.0888 - avg_loss: 0.1147 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9669 - val_loss: 1.3324 - val_op_main_loss: 0.3154 - val_op_conv_loss: 0.4092 - val_avg_loss: 0.3156 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8942\n",
      "Epoch 271/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6421 - op_main_loss: 0.1468 - op_conv_loss: 0.0907 - avg_loss: 0.1120 - op_main_accuracy: 0.9498 - op_conv_accuracy: 0.9663 - avg_accuracy: 0.9649\n",
      "Epoch 00271: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6452 - op_main_loss: 0.1474 - op_conv_loss: 0.0922 - avg_loss: 0.1130 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9643 - val_loss: 1.5306 - val_op_main_loss: 0.3806 - val_op_conv_loss: 0.4774 - val_avg_loss: 0.3804 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8782\n",
      "Epoch 272/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6282 - op_main_loss: 0.1424 - op_conv_loss: 0.0857 - avg_loss: 0.1086 - op_main_accuracy: 0.9571 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9661\n",
      "Epoch 00272: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6274 - op_main_loss: 0.1422 - op_conv_loss: 0.0853 - avg_loss: 0.1084 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9662 - val_loss: 1.3594 - val_op_main_loss: 0.3292 - val_op_conv_loss: 0.4110 - val_avg_loss: 0.3280 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8924\n",
      "Epoch 273/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6445 - op_main_loss: 0.1486 - op_conv_loss: 0.0914 - avg_loss: 0.1135 - op_main_accuracy: 0.9499 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9642\n",
      "Epoch 00273: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6433 - op_main_loss: 0.1482 - op_conv_loss: 0.0910 - avg_loss: 0.1131 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9643 - val_loss: 1.3201 - val_op_main_loss: 0.3102 - val_op_conv_loss: 0.4061 - val_avg_loss: 0.3132 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8914\n",
      "Epoch 274/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6366 - op_main_loss: 0.1490 - op_conv_loss: 0.0855 - avg_loss: 0.1112 - op_main_accuracy: 0.9522 - op_conv_accuracy: 0.9671 - avg_accuracy: 0.9678\n",
      "Epoch 00274: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6382 - op_main_loss: 0.1498 - op_conv_loss: 0.0858 - avg_loss: 0.1117 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9669 - avg_accuracy: 0.9674 - val_loss: 1.3441 - val_op_main_loss: 0.3222 - val_op_conv_loss: 0.4091 - val_avg_loss: 0.3215 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8895\n",
      "Epoch 275/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6471 - op_main_loss: 0.1544 - op_conv_loss: 0.0872 - avg_loss: 0.1138 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9643\n",
      "Epoch 00275: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6471 - op_main_loss: 0.1544 - op_conv_loss: 0.0872 - avg_loss: 0.1138 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9643 - val_loss: 1.2975 - val_op_main_loss: 0.3087 - val_op_conv_loss: 0.3885 - val_avg_loss: 0.3083 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8942\n",
      "Epoch 276/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6238 - op_main_loss: 0.1410 - op_conv_loss: 0.0844 - avg_loss: 0.1063 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9693\n",
      "Epoch 00276: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6238 - op_main_loss: 0.1410 - op_conv_loss: 0.0844 - avg_loss: 0.1063 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9693 - val_loss: 1.3039 - val_op_main_loss: 0.3123 - val_op_conv_loss: 0.3894 - val_avg_loss: 0.3099 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8980\n",
      "Epoch 277/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6450 - op_main_loss: 0.1460 - op_conv_loss: 0.0937 - avg_loss: 0.1133 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9642 - avg_accuracy: 0.9628\n",
      "Epoch 00277: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6445 - op_main_loss: 0.1456 - op_conv_loss: 0.0938 - avg_loss: 0.1131 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9627 - val_loss: 1.3382 - val_op_main_loss: 0.3132 - val_op_conv_loss: 0.4170 - val_avg_loss: 0.3160 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8942\n",
      "Epoch 278/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6065 - op_main_loss: 0.1383 - op_conv_loss: 0.0757 - avg_loss: 0.1006 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9733\n",
      "Epoch 00278: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6065 - op_main_loss: 0.1383 - op_conv_loss: 0.0757 - avg_loss: 0.1006 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9733 - val_loss: 1.3252 - val_op_main_loss: 0.3171 - val_op_conv_loss: 0.4006 - val_avg_loss: 0.3158 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8867\n",
      "Epoch 279/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6463 - op_main_loss: 0.1507 - op_conv_loss: 0.0906 - avg_loss: 0.1139 - op_main_accuracy: 0.9479 - op_conv_accuracy: 0.9661 - avg_accuracy: 0.9634\n",
      "Epoch 00279: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6470 - op_main_loss: 0.1508 - op_conv_loss: 0.0909 - avg_loss: 0.1141 - op_main_accuracy: 0.9475 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9631 - val_loss: 1.2778 - val_op_main_loss: 0.3032 - val_op_conv_loss: 0.3809 - val_avg_loss: 0.3035 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8971\n",
      "Epoch 280/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6291 - op_main_loss: 0.1448 - op_conv_loss: 0.0849 - avg_loss: 0.1093 - op_main_accuracy: 0.9535 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9685\n",
      "Epoch 00280: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6282 - op_main_loss: 0.1443 - op_conv_loss: 0.0850 - avg_loss: 0.1089 - op_main_accuracy: 0.9539 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9688 - val_loss: 1.3297 - val_op_main_loss: 0.3127 - val_op_conv_loss: 0.4105 - val_avg_loss: 0.3161 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8990\n",
      "Epoch 281/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6282 - op_main_loss: 0.1456 - op_conv_loss: 0.0840 - avg_loss: 0.1084 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9707\n",
      "Epoch 00281: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6313 - op_main_loss: 0.1466 - op_conv_loss: 0.0851 - avg_loss: 0.1094 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9702 - val_loss: 1.6300 - val_op_main_loss: 0.4057 - val_op_conv_loss: 0.5229 - val_avg_loss: 0.4107 - val_op_main_accuracy: 0.8678 - val_op_conv_accuracy: 0.8659 - val_avg_accuracy: 0.8640\n",
      "Epoch 282/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6295 - op_main_loss: 0.1456 - op_conv_loss: 0.0843 - avg_loss: 0.1089 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9662\n",
      "Epoch 00282: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6295 - op_main_loss: 0.1456 - op_conv_loss: 0.0843 - avg_loss: 0.1089 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9662 - val_loss: 1.3194 - val_op_main_loss: 0.3161 - val_op_conv_loss: 0.4007 - val_avg_loss: 0.3122 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8942\n",
      "Epoch 283/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6265 - op_main_loss: 0.1415 - op_conv_loss: 0.0873 - avg_loss: 0.1075 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9663 - avg_accuracy: 0.9666\n",
      "Epoch 00283: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6240 - op_main_loss: 0.1410 - op_conv_loss: 0.0861 - avg_loss: 0.1067 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9669 - avg_accuracy: 0.9672 - val_loss: 1.3859 - val_op_main_loss: 0.3440 - val_op_conv_loss: 0.4176 - val_avg_loss: 0.3344 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8942\n",
      "Epoch 284/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6271 - op_main_loss: 0.1424 - op_conv_loss: 0.0867 - avg_loss: 0.1081 - op_main_accuracy: 0.9520 - op_conv_accuracy: 0.9656 - avg_accuracy: 0.9639\n",
      "Epoch 00284: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6252 - op_main_loss: 0.1420 - op_conv_loss: 0.0858 - avg_loss: 0.1075 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9643 - val_loss: 1.3233 - val_op_main_loss: 0.3132 - val_op_conv_loss: 0.4056 - val_avg_loss: 0.3148 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8895\n",
      "Epoch 285/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6241 - op_main_loss: 0.1400 - op_conv_loss: 0.0867 - avg_loss: 0.1075 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9637 - avg_accuracy: 0.9645\n",
      "Epoch 00285: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6228 - op_main_loss: 0.1395 - op_conv_loss: 0.0864 - avg_loss: 0.1071 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9648 - val_loss: 1.7660 - val_op_main_loss: 0.4754 - val_op_conv_loss: 0.5400 - val_avg_loss: 0.4609 - val_op_main_accuracy: 0.8517 - val_op_conv_accuracy: 0.8640 - val_avg_accuracy: 0.8612\n",
      "Epoch 286/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6218 - op_main_loss: 0.1430 - op_conv_loss: 0.0829 - avg_loss: 0.1066 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9678\n",
      "Epoch 00286: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6250 - op_main_loss: 0.1442 - op_conv_loss: 0.0839 - avg_loss: 0.1076 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9672 - val_loss: 2.0135 - val_op_main_loss: 0.4746 - val_op_conv_loss: 0.7316 - val_avg_loss: 0.5182 - val_op_main_accuracy: 0.8555 - val_op_conv_accuracy: 0.8310 - val_avg_accuracy: 0.8385\n",
      "Epoch 287/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6238 - op_main_loss: 0.1439 - op_conv_loss: 0.0834 - avg_loss: 0.1071 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9655\n",
      "Epoch 00287: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6238 - op_main_loss: 0.1439 - op_conv_loss: 0.0834 - avg_loss: 0.1071 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9655 - val_loss: 1.3537 - val_op_main_loss: 0.3304 - val_op_conv_loss: 0.4078 - val_avg_loss: 0.3260 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8942\n",
      "Epoch 288/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6525 - op_main_loss: 0.1525 - op_conv_loss: 0.0945 - avg_loss: 0.1160 - op_main_accuracy: 0.9460 - op_conv_accuracy: 0.9633 - avg_accuracy: 0.9612\n",
      "Epoch 00288: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6528 - op_main_loss: 0.1527 - op_conv_loss: 0.0945 - avg_loss: 0.1161 - op_main_accuracy: 0.9459 - op_conv_accuracy: 0.9634 - avg_accuracy: 0.9610 - val_loss: 1.3390 - val_op_main_loss: 0.3145 - val_op_conv_loss: 0.4142 - val_avg_loss: 0.3208 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8914\n",
      "Epoch 289/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6147 - op_main_loss: 0.1387 - op_conv_loss: 0.0818 - avg_loss: 0.1044 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9690\n",
      "Epoch 00289: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6207 - op_main_loss: 0.1405 - op_conv_loss: 0.0842 - avg_loss: 0.1062 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9683 - val_loss: 1.3287 - val_op_main_loss: 0.3125 - val_op_conv_loss: 0.4131 - val_avg_loss: 0.3137 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8942\n",
      "Epoch 290/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6210 - op_main_loss: 0.1407 - op_conv_loss: 0.0846 - avg_loss: 0.1067 - op_main_accuracy: 0.9548 - op_conv_accuracy: 0.9647 - avg_accuracy: 0.9666\n",
      "Epoch 00290: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6239 - op_main_loss: 0.1417 - op_conv_loss: 0.0857 - avg_loss: 0.1076 - op_main_accuracy: 0.9539 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9662 - val_loss: 1.3225 - val_op_main_loss: 0.3047 - val_op_conv_loss: 0.4184 - val_avg_loss: 0.3106 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 291/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5942 - op_main_loss: 0.1334 - op_conv_loss: 0.0740 - avg_loss: 0.0981 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9724\n",
      "Epoch 00291: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5963 - op_main_loss: 0.1340 - op_conv_loss: 0.0748 - avg_loss: 0.0987 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9724 - val_loss: 1.4585 - val_op_main_loss: 0.3650 - val_op_conv_loss: 0.4453 - val_avg_loss: 0.3592 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8867\n",
      "Epoch 292/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6322 - op_main_loss: 0.1416 - op_conv_loss: 0.0923 - avg_loss: 0.1097 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9657\n",
      "Epoch 00292: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6322 - op_main_loss: 0.1416 - op_conv_loss: 0.0923 - avg_loss: 0.1097 - op_main_accuracy: 0.9544 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9657 - val_loss: 1.5194 - val_op_main_loss: 0.3756 - val_op_conv_loss: 0.4781 - val_avg_loss: 0.3772 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8810\n",
      "Epoch 293/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6477 - op_main_loss: 0.1540 - op_conv_loss: 0.0912 - avg_loss: 0.1147 - op_main_accuracy: 0.9477 - op_conv_accuracy: 0.9663 - avg_accuracy: 0.9637\n",
      "Epoch 00293: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6467 - op_main_loss: 0.1536 - op_conv_loss: 0.0909 - avg_loss: 0.1143 - op_main_accuracy: 0.9478 - op_conv_accuracy: 0.9667 - avg_accuracy: 0.9638 - val_loss: 1.3907 - val_op_main_loss: 0.3406 - val_op_conv_loss: 0.4221 - val_avg_loss: 0.3403 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8829\n",
      "Epoch 294/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6347 - op_main_loss: 0.1502 - op_conv_loss: 0.0855 - avg_loss: 0.1113 - op_main_accuracy: 0.9472 - op_conv_accuracy: 0.9685 - avg_accuracy: 0.9634\n",
      "Epoch 00294: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6346 - op_main_loss: 0.1505 - op_conv_loss: 0.0851 - avg_loss: 0.1113 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9638 - val_loss: 1.3943 - val_op_main_loss: 0.3273 - val_op_conv_loss: 0.4417 - val_avg_loss: 0.3368 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8791\n",
      "Epoch 295/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6175 - op_main_loss: 0.1386 - op_conv_loss: 0.0851 - avg_loss: 0.1053 - op_main_accuracy: 0.9555 - op_conv_accuracy: 0.9678 - avg_accuracy: 0.9688\n",
      "Epoch 00295: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6165 - op_main_loss: 0.1384 - op_conv_loss: 0.0846 - avg_loss: 0.1050 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9690 - val_loss: 1.8097 - val_op_main_loss: 0.4436 - val_op_conv_loss: 0.6136 - val_avg_loss: 0.4645 - val_op_main_accuracy: 0.8574 - val_op_conv_accuracy: 0.8565 - val_avg_accuracy: 0.8555\n",
      "Epoch 296/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6093 - op_main_loss: 0.1376 - op_conv_loss: 0.0800 - avg_loss: 0.1032 - op_main_accuracy: 0.9541 - op_conv_accuracy: 0.9656 - avg_accuracy: 0.9654\n",
      "Epoch 00296: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6088 - op_main_loss: 0.1377 - op_conv_loss: 0.0795 - avg_loss: 0.1030 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9653 - val_loss: 1.3200 - val_op_main_loss: 0.3196 - val_op_conv_loss: 0.3967 - val_avg_loss: 0.3147 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8933\n",
      "Epoch 297/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6185 - op_main_loss: 0.1417 - op_conv_loss: 0.0821 - avg_loss: 0.1056 - op_main_accuracy: 0.9578 - op_conv_accuracy: 0.9729 - avg_accuracy: 0.9700\n",
      "Epoch 00297: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6184 - op_main_loss: 0.1416 - op_conv_loss: 0.0821 - avg_loss: 0.1056 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9698 - val_loss: 1.3635 - val_op_main_loss: 0.3081 - val_op_conv_loss: 0.4434 - val_avg_loss: 0.3232 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8839\n",
      "Epoch 298/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6112 - op_main_loss: 0.1422 - op_conv_loss: 0.0769 - avg_loss: 0.1034 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9726\n",
      "Epoch 00298: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6112 - op_main_loss: 0.1422 - op_conv_loss: 0.0769 - avg_loss: 0.1034 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9726 - val_loss: 1.3690 - val_op_main_loss: 0.3341 - val_op_conv_loss: 0.4160 - val_avg_loss: 0.3303 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8914\n",
      "Epoch 299/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6139 - op_main_loss: 0.1414 - op_conv_loss: 0.0798 - avg_loss: 0.1043 - op_main_accuracy: 0.9513 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9675\n",
      "Epoch 00299: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6162 - op_main_loss: 0.1418 - op_conv_loss: 0.0810 - avg_loss: 0.1050 - op_main_accuracy: 0.9513 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9674 - val_loss: 1.2728 - val_op_main_loss: 0.3047 - val_op_conv_loss: 0.3783 - val_avg_loss: 0.3018 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8971\n",
      "Epoch 300/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6209 - op_main_loss: 0.1395 - op_conv_loss: 0.0865 - avg_loss: 0.1068 - op_main_accuracy: 0.9553 - op_conv_accuracy: 0.9663 - avg_accuracy: 0.9678\n",
      "Epoch 00300: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6209 - op_main_loss: 0.1391 - op_conv_loss: 0.0869 - avg_loss: 0.1067 - op_main_accuracy: 0.9551 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9676 - val_loss: 1.3704 - val_op_main_loss: 0.3112 - val_op_conv_loss: 0.4459 - val_avg_loss: 0.3254 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8876\n",
      "Epoch 301/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6231 - op_main_loss: 0.1428 - op_conv_loss: 0.0846 - avg_loss: 0.1077 - op_main_accuracy: 0.9494 - op_conv_accuracy: 0.9661 - avg_accuracy: 0.9620\n",
      "Epoch 00301: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6234 - op_main_loss: 0.1431 - op_conv_loss: 0.0845 - avg_loss: 0.1078 - op_main_accuracy: 0.9492 - op_conv_accuracy: 0.9657 - avg_accuracy: 0.9620 - val_loss: 1.3657 - val_op_main_loss: 0.3200 - val_op_conv_loss: 0.4274 - val_avg_loss: 0.3302 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8848\n",
      "Epoch 302/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6288 - op_main_loss: 0.1431 - op_conv_loss: 0.0883 - avg_loss: 0.1092 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9652 - avg_accuracy: 0.9645\n",
      "Epoch 00302: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6281 - op_main_loss: 0.1429 - op_conv_loss: 0.0881 - avg_loss: 0.1090 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9643 - val_loss: 1.4428 - val_op_main_loss: 0.3551 - val_op_conv_loss: 0.4423 - val_avg_loss: 0.3574 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8829\n",
      "Epoch 303/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6036 - op_main_loss: 0.1365 - op_conv_loss: 0.0783 - avg_loss: 0.1007 - op_main_accuracy: 0.9564 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9692\n",
      "Epoch 00303: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6066 - op_main_loss: 0.1372 - op_conv_loss: 0.0796 - avg_loss: 0.1016 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9686 - val_loss: 1.5017 - val_op_main_loss: 0.3487 - val_op_conv_loss: 0.4989 - val_avg_loss: 0.3659 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8820\n",
      "Epoch 304/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6289 - op_main_loss: 0.1427 - op_conv_loss: 0.0901 - avg_loss: 0.1084 - op_main_accuracy: 0.9527 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9638\n",
      "Epoch 00304: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6289 - op_main_loss: 0.1427 - op_conv_loss: 0.0901 - avg_loss: 0.1084 - op_main_accuracy: 0.9527 - op_conv_accuracy: 0.9662 - avg_accuracy: 0.9638 - val_loss: 1.7407 - val_op_main_loss: 0.4396 - val_op_conv_loss: 0.5632 - val_avg_loss: 0.4509 - val_op_main_accuracy: 0.8536 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8565\n",
      "Epoch 305/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6275 - op_main_loss: 0.1428 - op_conv_loss: 0.0877 - avg_loss: 0.1092 - op_main_accuracy: 0.9496 - op_conv_accuracy: 0.9632 - avg_accuracy: 0.9608\n",
      "Epoch 00305: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6264 - op_main_loss: 0.1425 - op_conv_loss: 0.0872 - avg_loss: 0.1089 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9610 - val_loss: 1.4246 - val_op_main_loss: 0.3288 - val_op_conv_loss: 0.4637 - val_avg_loss: 0.3441 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8886\n",
      "Epoch 306/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6247 - op_main_loss: 0.1420 - op_conv_loss: 0.0868 - avg_loss: 0.1083 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9627\n",
      "Epoch 00306: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6247 - op_main_loss: 0.1420 - op_conv_loss: 0.0868 - avg_loss: 0.1083 - op_main_accuracy: 0.9501 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9627 - val_loss: 1.2876 - val_op_main_loss: 0.2991 - val_op_conv_loss: 0.3941 - val_avg_loss: 0.3073 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8952\n",
      "Epoch 307/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6505 - op_main_loss: 0.1526 - op_conv_loss: 0.0939 - avg_loss: 0.1165 - op_main_accuracy: 0.9472 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9628\n",
      "Epoch 00307: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6504 - op_main_loss: 0.1525 - op_conv_loss: 0.0939 - avg_loss: 0.1164 - op_main_accuracy: 0.9473 - op_conv_accuracy: 0.9638 - avg_accuracy: 0.9629 - val_loss: 1.4408 - val_op_main_loss: 0.3454 - val_op_conv_loss: 0.4543 - val_avg_loss: 0.3534 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8829\n",
      "Epoch 308/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6207 - op_main_loss: 0.1393 - op_conv_loss: 0.0864 - avg_loss: 0.1069 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9673 - avg_accuracy: 0.9647\n",
      "Epoch 00308: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6214 - op_main_loss: 0.1394 - op_conv_loss: 0.0867 - avg_loss: 0.1071 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9674 - avg_accuracy: 0.9648 - val_loss: 1.5332 - val_op_main_loss: 0.4048 - val_op_conv_loss: 0.4523 - val_avg_loss: 0.3879 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8782\n",
      "Epoch 309/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5926 - op_main_loss: 0.1306 - op_conv_loss: 0.0761 - avg_loss: 0.0979 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9730 - avg_accuracy: 0.9721\n",
      "Epoch 00309: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5928 - op_main_loss: 0.1305 - op_conv_loss: 0.0763 - avg_loss: 0.0980 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9721 - val_loss: 1.3647 - val_op_main_loss: 0.3224 - val_op_conv_loss: 0.4291 - val_avg_loss: 0.3250 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8933\n",
      "Epoch 310/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6035 - op_main_loss: 0.1365 - op_conv_loss: 0.0778 - avg_loss: 0.1017 - op_main_accuracy: 0.9583 - op_conv_accuracy: 0.9718 - avg_accuracy: 0.9714\n",
      "Epoch 00310: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6050 - op_main_loss: 0.1369 - op_conv_loss: 0.0784 - avg_loss: 0.1022 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9712 - val_loss: 1.4816 - val_op_main_loss: 0.3391 - val_op_conv_loss: 0.4971 - val_avg_loss: 0.3580 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8754\n",
      "Epoch 311/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6156 - op_main_loss: 0.1371 - op_conv_loss: 0.0866 - avg_loss: 0.1050 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9676\n",
      "Epoch 00311: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6156 - op_main_loss: 0.1371 - op_conv_loss: 0.0866 - avg_loss: 0.1050 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9676 - val_loss: 1.2990 - val_op_main_loss: 0.3077 - val_op_conv_loss: 0.3945 - val_avg_loss: 0.3102 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8971\n",
      "Epoch 312/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6246 - op_main_loss: 0.1429 - op_conv_loss: 0.0873 - avg_loss: 0.1078 - op_main_accuracy: 0.9528 - op_conv_accuracy: 0.9678 - avg_accuracy: 0.9656\n",
      "Epoch 00312: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6273 - op_main_loss: 0.1437 - op_conv_loss: 0.0884 - avg_loss: 0.1086 - op_main_accuracy: 0.9520 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9653 - val_loss: 1.4096 - val_op_main_loss: 0.3294 - val_op_conv_loss: 0.4532 - val_avg_loss: 0.3404 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8848\n",
      "Epoch 313/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6019 - op_main_loss: 0.1371 - op_conv_loss: 0.0769 - avg_loss: 0.1013 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9719\n",
      "Epoch 00313: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6074 - op_main_loss: 0.1385 - op_conv_loss: 0.0792 - avg_loss: 0.1030 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9709 - val_loss: 1.3227 - val_op_main_loss: 0.3083 - val_op_conv_loss: 0.4140 - val_avg_loss: 0.3139 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8933\n",
      "Epoch 314/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6044 - op_main_loss: 0.1346 - op_conv_loss: 0.0812 - avg_loss: 0.1023 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9685 - avg_accuracy: 0.9702\n",
      "Epoch 00314: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6064 - op_main_loss: 0.1352 - op_conv_loss: 0.0819 - avg_loss: 0.1030 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9700 - val_loss: 1.3707 - val_op_main_loss: 0.3407 - val_op_conv_loss: 0.4079 - val_avg_loss: 0.3360 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8914\n",
      "Epoch 315/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6036 - op_main_loss: 0.1343 - op_conv_loss: 0.0816 - avg_loss: 0.1019 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9683\n",
      "Epoch 00315: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6035 - op_main_loss: 0.1343 - op_conv_loss: 0.0816 - avg_loss: 0.1019 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9683 - val_loss: 1.3872 - val_op_main_loss: 0.3396 - val_op_conv_loss: 0.4218 - val_avg_loss: 0.3403 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8905\n",
      "Epoch 316/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5979 - op_main_loss: 0.1330 - op_conv_loss: 0.0788 - avg_loss: 0.1007 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9692 - avg_accuracy: 0.9695\n",
      "Epoch 00316: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5966 - op_main_loss: 0.1325 - op_conv_loss: 0.0783 - avg_loss: 0.1003 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9698 - val_loss: 1.5647 - val_op_main_loss: 0.4260 - val_op_conv_loss: 0.4530 - val_avg_loss: 0.4003 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8697\n",
      "Epoch 317/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6082 - op_main_loss: 0.1378 - op_conv_loss: 0.0816 - avg_loss: 0.1038 - op_main_accuracy: 0.9548 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9683\n",
      "Epoch 00317: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6062 - op_main_loss: 0.1371 - op_conv_loss: 0.0809 - avg_loss: 0.1031 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9688 - val_loss: 1.3199 - val_op_main_loss: 0.3046 - val_op_conv_loss: 0.4191 - val_avg_loss: 0.3118 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8952\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.6021 - op_main_loss: 0.1374 - op_conv_loss: 0.0788 - avg_loss: 0.1012 - op_main_accuracy: 0.9539 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9702\n",
      "Epoch 00318: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6021 - op_main_loss: 0.1374 - op_conv_loss: 0.0788 - avg_loss: 0.1012 - op_main_accuracy: 0.9539 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9702 - val_loss: 1.3406 - val_op_main_loss: 0.3272 - val_op_conv_loss: 0.4065 - val_avg_loss: 0.3219 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8990 - val_avg_accuracy: 0.8971\n",
      "Epoch 319/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6079 - op_main_loss: 0.1380 - op_conv_loss: 0.0813 - avg_loss: 0.1034 - op_main_accuracy: 0.9526 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9695\n",
      "Epoch 00319: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6074 - op_main_loss: 0.1380 - op_conv_loss: 0.0810 - avg_loss: 0.1033 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9695 - val_loss: 1.4704 - val_op_main_loss: 0.3409 - val_op_conv_loss: 0.4889 - val_avg_loss: 0.3552 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8791\n",
      "Epoch 320/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6324 - op_main_loss: 0.1425 - op_conv_loss: 0.0938 - avg_loss: 0.1110 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9646\n",
      "Epoch 00320: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6324 - op_main_loss: 0.1425 - op_conv_loss: 0.0938 - avg_loss: 0.1110 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9641 - avg_accuracy: 0.9646 - val_loss: 1.2929 - val_op_main_loss: 0.3145 - val_op_conv_loss: 0.3867 - val_avg_loss: 0.3070 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8971\n",
      "Epoch 321/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5824 - op_main_loss: 0.1279 - op_conv_loss: 0.0738 - avg_loss: 0.0958 - op_main_accuracy: 0.9611 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9743\n",
      "Epoch 00321: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5802 - op_main_loss: 0.1269 - op_conv_loss: 0.0732 - avg_loss: 0.0951 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9740 - val_loss: 1.4638 - val_op_main_loss: 0.3507 - val_op_conv_loss: 0.4711 - val_avg_loss: 0.3570 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8867\n",
      "Epoch 322/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6148 - op_main_loss: 0.1365 - op_conv_loss: 0.0878 - avg_loss: 0.1055 - op_main_accuracy: 0.9552 - op_conv_accuracy: 0.9668 - avg_accuracy: 0.9673\n",
      "Epoch 00322: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6118 - op_main_loss: 0.1360 - op_conv_loss: 0.0863 - avg_loss: 0.1045 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9681 - val_loss: 1.3869 - val_op_main_loss: 0.3219 - val_op_conv_loss: 0.4532 - val_avg_loss: 0.3274 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8867\n",
      "Epoch 323/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5812 - op_main_loss: 0.1268 - op_conv_loss: 0.0750 - avg_loss: 0.0953 - op_main_accuracy: 0.9637 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9726\n",
      "Epoch 00323: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5793 - op_main_loss: 0.1262 - op_conv_loss: 0.0743 - avg_loss: 0.0947 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9728 - val_loss: 1.3947 - val_op_main_loss: 0.3364 - val_op_conv_loss: 0.4351 - val_avg_loss: 0.3391 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8857\n",
      "Epoch 324/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6215 - op_main_loss: 0.1408 - op_conv_loss: 0.0889 - avg_loss: 0.1081 - op_main_accuracy: 0.9524 - op_conv_accuracy: 0.9649 - avg_accuracy: 0.9644\n",
      "Epoch 00324: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6218 - op_main_loss: 0.1408 - op_conv_loss: 0.0892 - avg_loss: 0.1082 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9650 - avg_accuracy: 0.9646 - val_loss: 1.5708 - val_op_main_loss: 0.3972 - val_op_conv_loss: 0.4934 - val_avg_loss: 0.3965 - val_op_main_accuracy: 0.8687 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8763\n",
      "Epoch 325/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5993 - op_main_loss: 0.1359 - op_conv_loss: 0.0783 - avg_loss: 0.1013 - op_main_accuracy: 0.9571 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9685\n",
      "Epoch 00325: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6030 - op_main_loss: 0.1367 - op_conv_loss: 0.0802 - avg_loss: 0.1024 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9676 - val_loss: 1.3208 - val_op_main_loss: 0.3170 - val_op_conv_loss: 0.4039 - val_avg_loss: 0.3163 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8952\n",
      "Epoch 326/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6084 - op_main_loss: 0.1364 - op_conv_loss: 0.0847 - avg_loss: 0.1043 - op_main_accuracy: 0.9573 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9697\n",
      "Epoch 00326: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6073 - op_main_loss: 0.1361 - op_conv_loss: 0.0842 - avg_loss: 0.1040 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9700 - val_loss: 1.3664 - val_op_main_loss: 0.3272 - val_op_conv_loss: 0.4271 - val_avg_loss: 0.3285 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8933\n",
      "Epoch 327/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5964 - op_main_loss: 0.1342 - op_conv_loss: 0.0778 - avg_loss: 0.1007 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9672\n",
      "Epoch 00327: val_avg_accuracy did not improve from 0.89896\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5964 - op_main_loss: 0.1342 - op_conv_loss: 0.0778 - avg_loss: 0.1007 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9672 - val_loss: 1.3957 - val_op_main_loss: 0.3396 - val_op_conv_loss: 0.4322 - val_avg_loss: 0.3402 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8829\n",
      "Epoch 328/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6051 - op_main_loss: 0.1335 - op_conv_loss: 0.0848 - avg_loss: 0.1029 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9674\n",
      "Epoch 00328: val_avg_accuracy improved from 0.89896 to 0.89991, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.6051 - op_main_loss: 0.1335 - op_conv_loss: 0.0848 - avg_loss: 0.1029 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9674 - val_loss: 1.3266 - val_op_main_loss: 0.3179 - val_op_conv_loss: 0.4078 - val_avg_loss: 0.3174 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8999 - val_avg_accuracy: 0.8999\n",
      "Epoch 329/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5919 - op_main_loss: 0.1346 - op_conv_loss: 0.0750 - avg_loss: 0.0992 - op_main_accuracy: 0.9564 - op_conv_accuracy: 0.9729 - avg_accuracy: 0.9692\n",
      "Epoch 00329: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5915 - op_main_loss: 0.1351 - op_conv_loss: 0.0743 - avg_loss: 0.0990 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9695 - val_loss: 1.3731 - val_op_main_loss: 0.3322 - val_op_conv_loss: 0.4241 - val_avg_loss: 0.3339 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8914\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.6170 - op_main_loss: 0.1396 - op_conv_loss: 0.0870 - avg_loss: 0.1074 - op_main_accuracy: 0.9525 - op_conv_accuracy: 0.9646 - avg_accuracy: 0.9641\n",
      "Epoch 00330: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6167 - op_main_loss: 0.1400 - op_conv_loss: 0.0864 - avg_loss: 0.1072 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9648 - avg_accuracy: 0.9646 - val_loss: 1.3686 - val_op_main_loss: 0.3274 - val_op_conv_loss: 0.4288 - val_avg_loss: 0.3292 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8754 - val_avg_accuracy: 0.8772\n",
      "Epoch 331/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6103 - op_main_loss: 0.1406 - op_conv_loss: 0.0816 - avg_loss: 0.1046 - op_main_accuracy: 0.9495 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9659\n",
      "Epoch 00331: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6085 - op_main_loss: 0.1401 - op_conv_loss: 0.0809 - avg_loss: 0.1040 - op_main_accuracy: 0.9504 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9662 - val_loss: 1.4074 - val_op_main_loss: 0.3534 - val_op_conv_loss: 0.4269 - val_avg_loss: 0.3429 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8876\n",
      "Epoch 332/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6128 - op_main_loss: 0.1387 - op_conv_loss: 0.0848 - avg_loss: 0.1054 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9692 - avg_accuracy: 0.9692\n",
      "Epoch 00332: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6142 - op_main_loss: 0.1394 - op_conv_loss: 0.0850 - avg_loss: 0.1059 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9693 - val_loss: 1.3680 - val_op_main_loss: 0.3385 - val_op_conv_loss: 0.4158 - val_avg_loss: 0.3301 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8914\n",
      "Epoch 333/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6007 - op_main_loss: 0.1344 - op_conv_loss: 0.0813 - avg_loss: 0.1014 - op_main_accuracy: 0.9552 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9700\n",
      "Epoch 00333: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6017 - op_main_loss: 0.1346 - op_conv_loss: 0.0818 - avg_loss: 0.1017 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9698 - val_loss: 1.3635 - val_op_main_loss: 0.3250 - val_op_conv_loss: 0.4282 - val_avg_loss: 0.3272 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8905\n",
      "Epoch 334/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5959 - op_main_loss: 0.1335 - op_conv_loss: 0.0800 - avg_loss: 0.0992 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9714\n",
      "Epoch 00334: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5953 - op_main_loss: 0.1336 - op_conv_loss: 0.0795 - avg_loss: 0.0990 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9716 - val_loss: 1.4722 - val_op_main_loss: 0.3818 - val_op_conv_loss: 0.4436 - val_avg_loss: 0.3635 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8820\n",
      "Epoch 335/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6425 - op_main_loss: 0.1517 - op_conv_loss: 0.0920 - avg_loss: 0.1147 - op_main_accuracy: 0.9469 - op_conv_accuracy: 0.9635 - avg_accuracy: 0.9594\n",
      "Epoch 00335: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6408 - op_main_loss: 0.1512 - op_conv_loss: 0.0913 - avg_loss: 0.1142 - op_main_accuracy: 0.9471 - op_conv_accuracy: 0.9636 - avg_accuracy: 0.9596 - val_loss: 1.3382 - val_op_main_loss: 0.3162 - val_op_conv_loss: 0.4177 - val_avg_loss: 0.3200 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8914\n",
      "Epoch 336/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5909 - op_main_loss: 0.1322 - op_conv_loss: 0.0764 - avg_loss: 0.0982 - op_main_accuracy: 0.9588 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9709\n",
      "Epoch 00336: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5929 - op_main_loss: 0.1326 - op_conv_loss: 0.0775 - avg_loss: 0.0988 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9702 - val_loss: 1.3535 - val_op_main_loss: 0.3155 - val_op_conv_loss: 0.4305 - val_avg_loss: 0.3240 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8924\n",
      "Epoch 337/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5928 - op_main_loss: 0.1337 - op_conv_loss: 0.0758 - avg_loss: 0.0995 - op_main_accuracy: 0.9587 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9716\n",
      "Epoch 00337: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5936 - op_main_loss: 0.1342 - op_conv_loss: 0.0759 - avg_loss: 0.0998 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9714 - val_loss: 1.3796 - val_op_main_loss: 0.3177 - val_op_conv_loss: 0.4520 - val_avg_loss: 0.3265 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8782\n",
      "Epoch 338/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5972 - op_main_loss: 0.1349 - op_conv_loss: 0.0792 - avg_loss: 0.1003 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9678\n",
      "Epoch 00338: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5948 - op_main_loss: 0.1340 - op_conv_loss: 0.0783 - avg_loss: 0.0995 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9686 - val_loss: 1.3154 - val_op_main_loss: 0.3146 - val_op_conv_loss: 0.4003 - val_avg_loss: 0.3176 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8961\n",
      "Epoch 339/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5967 - op_main_loss: 0.1321 - op_conv_loss: 0.0817 - avg_loss: 0.1000 - op_main_accuracy: 0.9581 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9697\n",
      "Epoch 00339: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5976 - op_main_loss: 0.1328 - op_conv_loss: 0.0817 - avg_loss: 0.1002 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9695 - val_loss: 1.6315 - val_op_main_loss: 0.4100 - val_op_conv_loss: 0.5255 - val_avg_loss: 0.4131 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8687 - val_avg_accuracy: 0.8687\n",
      "Epoch 340/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.6142 - op_main_loss: 0.1405 - op_conv_loss: 0.0854 - avg_loss: 0.1054 - op_main_accuracy: 0.9513 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9656\n",
      "Epoch 00340: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6158 - op_main_loss: 0.1411 - op_conv_loss: 0.0858 - avg_loss: 0.1059 - op_main_accuracy: 0.9511 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9653 - val_loss: 1.3869 - val_op_main_loss: 0.3289 - val_op_conv_loss: 0.4396 - val_avg_loss: 0.3356 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8905\n",
      "Epoch 341/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5830 - op_main_loss: 0.1290 - op_conv_loss: 0.0746 - avg_loss: 0.0965 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9743 - avg_accuracy: 0.9729\n",
      "Epoch 00341: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5838 - op_main_loss: 0.1291 - op_conv_loss: 0.0750 - avg_loss: 0.0968 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9728 - val_loss: 1.3302 - val_op_main_loss: 0.3182 - val_op_conv_loss: 0.4094 - val_avg_loss: 0.3196 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8914\n",
      "Epoch 342/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.5978 - op_main_loss: 0.1329 - op_conv_loss: 0.0810 - avg_loss: 0.1009 - op_main_accuracy: 0.9588 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9671\n",
      "Epoch 00342: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5973 - op_main_loss: 0.1327 - op_conv_loss: 0.0809 - avg_loss: 0.1007 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9672 - val_loss: 1.3371 - val_op_main_loss: 0.3158 - val_op_conv_loss: 0.4187 - val_avg_loss: 0.3201 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8876\n",
      "Epoch 343/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5952 - op_main_loss: 0.1334 - op_conv_loss: 0.0789 - avg_loss: 0.1004 - op_main_accuracy: 0.9550 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9692\n",
      "Epoch 00343: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5950 - op_main_loss: 0.1334 - op_conv_loss: 0.0788 - avg_loss: 0.1003 - op_main_accuracy: 0.9551 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9693 - val_loss: 1.6465 - val_op_main_loss: 0.4235 - val_op_conv_loss: 0.5162 - val_avg_loss: 0.4243 - val_op_main_accuracy: 0.8612 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8631\n",
      "Epoch 344/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5746 - op_main_loss: 0.1268 - op_conv_loss: 0.0712 - avg_loss: 0.0942 - op_main_accuracy: 0.9606 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9728\n",
      "Epoch 00344: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5755 - op_main_loss: 0.1271 - op_conv_loss: 0.0715 - avg_loss: 0.0945 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9728 - val_loss: 1.3881 - val_op_main_loss: 0.3243 - val_op_conv_loss: 0.4469 - val_avg_loss: 0.3346 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8895\n",
      "Epoch 345/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5790 - op_main_loss: 0.1272 - op_conv_loss: 0.0745 - avg_loss: 0.0953 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9721\n",
      "Epoch 00345: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5806 - op_main_loss: 0.1279 - op_conv_loss: 0.0748 - avg_loss: 0.0958 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9716 - val_loss: 1.3660 - val_op_main_loss: 0.3213 - val_op_conv_loss: 0.4356 - val_avg_loss: 0.3271 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8933\n",
      "Epoch 346/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6012 - op_main_loss: 0.1375 - op_conv_loss: 0.0798 - avg_loss: 0.1023 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9668\n",
      "Epoch 00346: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6015 - op_main_loss: 0.1375 - op_conv_loss: 0.0800 - avg_loss: 0.1024 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9667 - val_loss: 1.5251 - val_op_main_loss: 0.3598 - val_op_conv_loss: 0.5057 - val_avg_loss: 0.3780 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8763\n",
      "Epoch 347/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5854 - op_main_loss: 0.1323 - op_conv_loss: 0.0739 - avg_loss: 0.0976 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9674\n",
      "Epoch 00347: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5854 - op_main_loss: 0.1323 - op_conv_loss: 0.0739 - avg_loss: 0.0976 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9674 - val_loss: 1.3016 - val_op_main_loss: 0.3023 - val_op_conv_loss: 0.4080 - val_avg_loss: 0.3095 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8942\n",
      "Epoch 348/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5889 - op_main_loss: 0.1299 - op_conv_loss: 0.0789 - avg_loss: 0.0984 - op_main_accuracy: 0.9564 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9711\n",
      "Epoch 00348: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5889 - op_main_loss: 0.1299 - op_conv_loss: 0.0788 - avg_loss: 0.0984 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9712 - val_loss: 1.3740 - val_op_main_loss: 0.3205 - val_op_conv_loss: 0.4441 - val_avg_loss: 0.3276 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8867\n",
      "Epoch 349/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5923 - op_main_loss: 0.1328 - op_conv_loss: 0.0790 - avg_loss: 0.0988 - op_main_accuracy: 0.9545 - op_conv_accuracy: 0.9704 - avg_accuracy: 0.9688\n",
      "Epoch 00349: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5918 - op_main_loss: 0.1331 - op_conv_loss: 0.0783 - avg_loss: 0.0987 - op_main_accuracy: 0.9546 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9693 - val_loss: 1.4027 - val_op_main_loss: 0.3311 - val_op_conv_loss: 0.4475 - val_avg_loss: 0.3428 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8801\n",
      "Epoch 350/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.6109 - op_main_loss: 0.1411 - op_conv_loss: 0.0833 - avg_loss: 0.1051 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9662\n",
      "Epoch 00350: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6109 - op_main_loss: 0.1411 - op_conv_loss: 0.0833 - avg_loss: 0.1051 - op_main_accuracy: 0.9490 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9662 - val_loss: 1.3493 - val_op_main_loss: 0.3134 - val_op_conv_loss: 0.4297 - val_avg_loss: 0.3253 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8886\n",
      "Epoch 351/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6290 - op_main_loss: 0.1405 - op_conv_loss: 0.0962 - avg_loss: 0.1109 - op_main_accuracy: 0.9535 - op_conv_accuracy: 0.9628 - avg_accuracy: 0.9621\n",
      "Epoch 00351: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6273 - op_main_loss: 0.1400 - op_conv_loss: 0.0956 - avg_loss: 0.1104 - op_main_accuracy: 0.9539 - op_conv_accuracy: 0.9631 - avg_accuracy: 0.9624 - val_loss: 1.6894 - val_op_main_loss: 0.3601 - val_op_conv_loss: 0.6379 - val_avg_loss: 0.4100 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8489 - val_avg_accuracy: 0.8593\n",
      "Epoch 352/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5976 - op_main_loss: 0.1338 - op_conv_loss: 0.0805 - avg_loss: 0.1019 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9680 - avg_accuracy: 0.9673\n",
      "Epoch 00352: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5959 - op_main_loss: 0.1331 - op_conv_loss: 0.0799 - avg_loss: 0.1013 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9676 - val_loss: 1.3177 - val_op_main_loss: 0.3032 - val_op_conv_loss: 0.4175 - val_avg_loss: 0.3156 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8914\n",
      "Epoch 353/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5764 - op_main_loss: 0.1244 - op_conv_loss: 0.0760 - avg_loss: 0.0945 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9717\n",
      "Epoch 00353: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5805 - op_main_loss: 0.1260 - op_conv_loss: 0.0771 - avg_loss: 0.0959 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9709 - val_loss: 1.2985 - val_op_main_loss: 0.3138 - val_op_conv_loss: 0.3898 - val_avg_loss: 0.3134 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8933\n",
      "Epoch 354/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.5994 - op_main_loss: 0.1345 - op_conv_loss: 0.0820 - avg_loss: 0.1015 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9690\n",
      "Epoch 00354: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5971 - op_main_loss: 0.1333 - op_conv_loss: 0.0817 - avg_loss: 0.1008 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9688 - val_loss: 1.4324 - val_op_main_loss: 0.3289 - val_op_conv_loss: 0.4774 - val_avg_loss: 0.3450 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8839\n",
      "Epoch 355/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5825 - op_main_loss: 0.1288 - op_conv_loss: 0.0758 - avg_loss: 0.0962 - op_main_accuracy: 0.9588 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9730\n",
      "Epoch 00355: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5820 - op_main_loss: 0.1286 - op_conv_loss: 0.0757 - avg_loss: 0.0960 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9731 - val_loss: 1.3585 - val_op_main_loss: 0.3278 - val_op_conv_loss: 0.4231 - val_avg_loss: 0.3259 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8961\n",
      "Epoch 356/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5712 - op_main_loss: 0.1235 - op_conv_loss: 0.0736 - avg_loss: 0.0927 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9714\n",
      "Epoch 00356: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5716 - op_main_loss: 0.1243 - op_conv_loss: 0.0731 - avg_loss: 0.0929 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9714 - val_loss: 1.4282 - val_op_main_loss: 0.3469 - val_op_conv_loss: 0.4519 - val_avg_loss: 0.3484 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8829\n",
      "Epoch 357/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5788 - op_main_loss: 0.1293 - op_conv_loss: 0.0736 - avg_loss: 0.0951 - op_main_accuracy: 0.9578 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9721\n",
      "Epoch 00357: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5810 - op_main_loss: 0.1298 - op_conv_loss: 0.0747 - avg_loss: 0.0958 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9714 - val_loss: 1.4451 - val_op_main_loss: 0.3454 - val_op_conv_loss: 0.4652 - val_avg_loss: 0.3540 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8839\n",
      "Epoch 358/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5879 - op_main_loss: 0.1331 - op_conv_loss: 0.0759 - avg_loss: 0.0988 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9719\n",
      "Epoch 00358: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5860 - op_main_loss: 0.1322 - op_conv_loss: 0.0756 - avg_loss: 0.0982 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9721 - val_loss: 1.4435 - val_op_main_loss: 0.3407 - val_op_conv_loss: 0.4696 - val_avg_loss: 0.3534 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8829\n",
      "Epoch 359/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6092 - op_main_loss: 0.1386 - op_conv_loss: 0.0862 - avg_loss: 0.1045 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9688\n",
      "Epoch 00359: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6102 - op_main_loss: 0.1389 - op_conv_loss: 0.0866 - avg_loss: 0.1047 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9686 - val_loss: 1.3096 - val_op_main_loss: 0.3138 - val_op_conv_loss: 0.3987 - val_avg_loss: 0.3174 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8980\n",
      "Epoch 360/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5854 - op_main_loss: 0.1310 - op_conv_loss: 0.0769 - avg_loss: 0.0975 - op_main_accuracy: 0.9538 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9712\n",
      "Epoch 00360: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5902 - op_main_loss: 0.1329 - op_conv_loss: 0.0784 - avg_loss: 0.0990 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9702 - val_loss: 1.5278 - val_op_main_loss: 0.3696 - val_op_conv_loss: 0.4952 - val_avg_loss: 0.3834 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8754\n",
      "Epoch 361/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6285 - op_main_loss: 0.1444 - op_conv_loss: 0.0932 - avg_loss: 0.1114 - op_main_accuracy: 0.9466 - op_conv_accuracy: 0.9651 - avg_accuracy: 0.9659\n",
      "Epoch 00361: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6310 - op_main_loss: 0.1455 - op_conv_loss: 0.0937 - avg_loss: 0.1122 - op_main_accuracy: 0.9459 - op_conv_accuracy: 0.9643 - avg_accuracy: 0.9653 - val_loss: 1.4186 - val_op_main_loss: 0.3265 - val_op_conv_loss: 0.4730 - val_avg_loss: 0.3397 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8905\n",
      "Epoch 362/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5847 - op_main_loss: 0.1287 - op_conv_loss: 0.0787 - avg_loss: 0.0979 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9702\n",
      "Epoch 00362: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5847 - op_main_loss: 0.1287 - op_conv_loss: 0.0787 - avg_loss: 0.0979 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9702 - val_loss: 1.3714 - val_op_main_loss: 0.3360 - val_op_conv_loss: 0.4206 - val_avg_loss: 0.3354 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8905\n",
      "Epoch 363/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5835 - op_main_loss: 0.1287 - op_conv_loss: 0.0777 - avg_loss: 0.0972 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9729\n",
      "Epoch 00363: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5865 - op_main_loss: 0.1298 - op_conv_loss: 0.0786 - avg_loss: 0.0981 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9721 - val_loss: 2.4040 - val_op_main_loss: 0.6104 - val_op_conv_loss: 0.8699 - val_avg_loss: 0.6437 - val_op_main_accuracy: 0.8291 - val_op_conv_accuracy: 0.8159 - val_avg_accuracy: 0.8196\n",
      "Epoch 364/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.6010 - op_main_loss: 0.1362 - op_conv_loss: 0.0819 - avg_loss: 0.1028 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9688\n",
      "Epoch 00364: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6001 - op_main_loss: 0.1362 - op_conv_loss: 0.0814 - avg_loss: 0.1025 - op_main_accuracy: 0.9558 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9688 - val_loss: 1.3476 - val_op_main_loss: 0.3059 - val_op_conv_loss: 0.4450 - val_avg_loss: 0.3170 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8848\n",
      "Epoch 365/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5878 - op_main_loss: 0.1281 - op_conv_loss: 0.0816 - avg_loss: 0.0982 - op_main_accuracy: 0.9592 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9704\n",
      "Epoch 00365: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5861 - op_main_loss: 0.1276 - op_conv_loss: 0.0810 - avg_loss: 0.0977 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9707 - val_loss: 1.3595 - val_op_main_loss: 0.3111 - val_op_conv_loss: 0.4435 - val_avg_loss: 0.3253 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8857\n",
      "Epoch 366/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.5863 - op_main_loss: 0.1295 - op_conv_loss: 0.0786 - avg_loss: 0.0987 - op_main_accuracy: 0.9574 - op_conv_accuracy: 0.9685 - avg_accuracy: 0.9671\n",
      "Epoch 00366: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5883 - op_main_loss: 0.1305 - op_conv_loss: 0.0790 - avg_loss: 0.0993 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9667 - val_loss: 1.2675 - val_op_main_loss: 0.2996 - val_op_conv_loss: 0.3848 - val_avg_loss: 0.3040 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8952\n",
      "Epoch 367/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5823 - op_main_loss: 0.1286 - op_conv_loss: 0.0781 - avg_loss: 0.0961 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9714\n",
      "Epoch 00367: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5823 - op_main_loss: 0.1286 - op_conv_loss: 0.0781 - avg_loss: 0.0961 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9714 - val_loss: 1.5791 - val_op_main_loss: 0.4089 - val_op_conv_loss: 0.4876 - val_avg_loss: 0.4022 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8744\n",
      "Epoch 368/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5712 - op_main_loss: 0.1238 - op_conv_loss: 0.0739 - avg_loss: 0.0933 - op_main_accuracy: 0.9625 - op_conv_accuracy: 0.9699 - avg_accuracy: 0.9707\n",
      "Epoch 00368: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5700 - op_main_loss: 0.1235 - op_conv_loss: 0.0734 - avg_loss: 0.0930 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9709 - val_loss: 1.3533 - val_op_main_loss: 0.3280 - val_op_conv_loss: 0.4165 - val_avg_loss: 0.3288 - val_op_main_accuracy: 0.8952 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8933\n",
      "Epoch 369/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5966 - op_main_loss: 0.1331 - op_conv_loss: 0.0821 - avg_loss: 0.1018 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9672\n",
      "Epoch 00369: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5966 - op_main_loss: 0.1331 - op_conv_loss: 0.0821 - avg_loss: 0.1018 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9672 - val_loss: 1.4704 - val_op_main_loss: 0.3485 - val_op_conv_loss: 0.4794 - val_avg_loss: 0.3636 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8791\n",
      "Epoch 370/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5703 - op_main_loss: 0.1270 - op_conv_loss: 0.0711 - avg_loss: 0.0939 - op_main_accuracy: 0.9595 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9723\n",
      "Epoch 00370: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5707 - op_main_loss: 0.1271 - op_conv_loss: 0.0711 - avg_loss: 0.0940 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9735 - avg_accuracy: 0.9724 - val_loss: 1.2846 - val_op_main_loss: 0.3118 - val_op_conv_loss: 0.3859 - val_avg_loss: 0.3089 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8924\n",
      "Epoch 371/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5941 - op_main_loss: 0.1331 - op_conv_loss: 0.0825 - avg_loss: 0.1004 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9702\n",
      "Epoch 00371: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5938 - op_main_loss: 0.1330 - op_conv_loss: 0.0824 - avg_loss: 0.1003 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9702 - val_loss: 1.5096 - val_op_main_loss: 0.3592 - val_op_conv_loss: 0.5003 - val_avg_loss: 0.3720 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8810\n",
      "Epoch 372/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.6012 - op_main_loss: 0.1336 - op_conv_loss: 0.0873 - avg_loss: 0.1024 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9671\n",
      "Epoch 00372: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6011 - op_main_loss: 0.1337 - op_conv_loss: 0.0871 - avg_loss: 0.1024 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9664 - avg_accuracy: 0.9672 - val_loss: 1.2701 - val_op_main_loss: 0.2990 - val_op_conv_loss: 0.3906 - val_avg_loss: 0.3028 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8961\n",
      "Epoch 373/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5699 - op_main_loss: 0.1275 - op_conv_loss: 0.0709 - avg_loss: 0.0941 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9735\n",
      "Epoch 00373: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5699 - op_main_loss: 0.1275 - op_conv_loss: 0.0709 - avg_loss: 0.0941 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9735 - val_loss: 1.3594 - val_op_main_loss: 0.3356 - val_op_conv_loss: 0.4128 - val_avg_loss: 0.3336 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8905\n",
      "Epoch 374/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5750 - op_main_loss: 0.1267 - op_conv_loss: 0.0751 - avg_loss: 0.0956 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9698\n",
      "Epoch 00374: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5750 - op_main_loss: 0.1267 - op_conv_loss: 0.0751 - avg_loss: 0.0956 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9698 - avg_accuracy: 0.9698 - val_loss: 1.3117 - val_op_main_loss: 0.3138 - val_op_conv_loss: 0.4052 - val_avg_loss: 0.3153 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8971\n",
      "Epoch 375/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5969 - op_main_loss: 0.1349 - op_conv_loss: 0.0825 - avg_loss: 0.1019 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9688\n",
      "Epoch 00375: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5993 - op_main_loss: 0.1353 - op_conv_loss: 0.0840 - avg_loss: 0.1025 - op_main_accuracy: 0.9532 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9683 - val_loss: 1.3328 - val_op_main_loss: 0.3205 - val_op_conv_loss: 0.4138 - val_avg_loss: 0.3210 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8942\n",
      "Epoch 376/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5646 - op_main_loss: 0.1221 - op_conv_loss: 0.0733 - avg_loss: 0.0919 - op_main_accuracy: 0.9635 - op_conv_accuracy: 0.9723 - avg_accuracy: 0.9740\n",
      "Epoch 00376: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5651 - op_main_loss: 0.1223 - op_conv_loss: 0.0735 - avg_loss: 0.0921 - op_main_accuracy: 0.9634 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9738 - val_loss: 1.5578 - val_op_main_loss: 0.3738 - val_op_conv_loss: 0.5182 - val_avg_loss: 0.3884 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8763\n",
      "Epoch 377/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5808 - op_main_loss: 0.1275 - op_conv_loss: 0.0787 - avg_loss: 0.0972 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9698\n",
      "Epoch 00377: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5808 - op_main_loss: 0.1275 - op_conv_loss: 0.0787 - avg_loss: 0.0972 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9698 - val_loss: 1.4880 - val_op_main_loss: 0.3666 - val_op_conv_loss: 0.4712 - val_avg_loss: 0.3730 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8782\n",
      "Epoch 378/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/133 [============================>.] - ETA: 0s - loss: 0.5893 - op_main_loss: 0.1340 - op_conv_loss: 0.0782 - avg_loss: 0.0999 - op_main_accuracy: 0.9550 - op_conv_accuracy: 0.9718 - avg_accuracy: 0.9704\n",
      "Epoch 00378: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5892 - op_main_loss: 0.1339 - op_conv_loss: 0.0782 - avg_loss: 0.0999 - op_main_accuracy: 0.9551 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9705 - val_loss: 1.3539 - val_op_main_loss: 0.3206 - val_op_conv_loss: 0.4284 - val_avg_loss: 0.3278 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8961\n",
      "Epoch 379/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6070 - op_main_loss: 0.1395 - op_conv_loss: 0.0847 - avg_loss: 0.1055 - op_main_accuracy: 0.9521 - op_conv_accuracy: 0.9680 - avg_accuracy: 0.9668\n",
      "Epoch 00379: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6076 - op_main_loss: 0.1399 - op_conv_loss: 0.0847 - avg_loss: 0.1057 - op_main_accuracy: 0.9520 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9669 - val_loss: 1.3380 - val_op_main_loss: 0.3030 - val_op_conv_loss: 0.4402 - val_avg_loss: 0.3172 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8810\n",
      "Epoch 380/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5829 - op_main_loss: 0.1290 - op_conv_loss: 0.0784 - avg_loss: 0.0976 - op_main_accuracy: 0.9583 - op_conv_accuracy: 0.9692 - avg_accuracy: 0.9673\n",
      "Epoch 00380: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5811 - op_main_loss: 0.1288 - op_conv_loss: 0.0775 - avg_loss: 0.0970 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9674 - val_loss: 2.3253 - val_op_main_loss: 0.5661 - val_op_conv_loss: 0.8680 - val_avg_loss: 0.6135 - val_op_main_accuracy: 0.8310 - val_op_conv_accuracy: 0.8178 - val_avg_accuracy: 0.8253\n",
      "Epoch 381/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5922 - op_main_loss: 0.1338 - op_conv_loss: 0.0799 - avg_loss: 0.1009 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9681\n",
      "Epoch 00381: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5922 - op_main_loss: 0.1338 - op_conv_loss: 0.0799 - avg_loss: 0.1009 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9681 - val_loss: 1.3346 - val_op_main_loss: 0.3099 - val_op_conv_loss: 0.4268 - val_avg_loss: 0.3203 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8801\n",
      "Epoch 382/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5938 - op_main_loss: 0.1363 - op_conv_loss: 0.0780 - avg_loss: 0.1015 - op_main_accuracy: 0.9512 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9692\n",
      "Epoch 00382: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5936 - op_main_loss: 0.1363 - op_conv_loss: 0.0779 - avg_loss: 0.1014 - op_main_accuracy: 0.9513 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9695 - val_loss: 1.3013 - val_op_main_loss: 0.3022 - val_op_conv_loss: 0.4101 - val_avg_loss: 0.3110 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8905\n",
      "Epoch 383/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5524 - op_main_loss: 0.1183 - op_conv_loss: 0.0682 - avg_loss: 0.0878 - op_main_accuracy: 0.9635 - op_conv_accuracy: 0.9769 - avg_accuracy: 0.9755\n",
      "Epoch 00383: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5529 - op_main_loss: 0.1182 - op_conv_loss: 0.0687 - avg_loss: 0.0880 - op_main_accuracy: 0.9634 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9752 - val_loss: 1.4205 - val_op_main_loss: 0.3418 - val_op_conv_loss: 0.4506 - val_avg_loss: 0.3503 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8914\n",
      "Epoch 384/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5594 - op_main_loss: 0.1208 - op_conv_loss: 0.0707 - avg_loss: 0.0901 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9753 - avg_accuracy: 0.9738\n",
      "Epoch 00384: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5594 - op_main_loss: 0.1209 - op_conv_loss: 0.0707 - avg_loss: 0.0901 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9742 - val_loss: 1.3645 - val_op_main_loss: 0.3290 - val_op_conv_loss: 0.4269 - val_avg_loss: 0.3311 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8735 - val_avg_accuracy: 0.8801\n",
      "Epoch 385/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5714 - op_main_loss: 0.1251 - op_conv_loss: 0.0747 - avg_loss: 0.0941 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9721\n",
      "Epoch 00385: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5726 - op_main_loss: 0.1256 - op_conv_loss: 0.0750 - avg_loss: 0.0945 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9716 - val_loss: 1.3745 - val_op_main_loss: 0.3310 - val_op_conv_loss: 0.4293 - val_avg_loss: 0.3366 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8876\n",
      "Epoch 386/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5653 - op_main_loss: 0.1240 - op_conv_loss: 0.0720 - avg_loss: 0.0919 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9741 - avg_accuracy: 0.9724\n",
      "Epoch 00386: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5639 - op_main_loss: 0.1233 - op_conv_loss: 0.0717 - avg_loss: 0.0915 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9724 - val_loss: 1.3827 - val_op_main_loss: 0.3251 - val_op_conv_loss: 0.4445 - val_avg_loss: 0.3357 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8876\n",
      "Epoch 387/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5674 - op_main_loss: 0.1244 - op_conv_loss: 0.0728 - avg_loss: 0.0933 - op_main_accuracy: 0.9629 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9700\n",
      "Epoch 00387: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5674 - op_main_loss: 0.1244 - op_conv_loss: 0.0728 - avg_loss: 0.0933 - op_main_accuracy: 0.9629 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9700 - val_loss: 1.3806 - val_op_main_loss: 0.3277 - val_op_conv_loss: 0.4431 - val_avg_loss: 0.3330 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8924\n",
      "Epoch 388/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5832 - op_main_loss: 0.1289 - op_conv_loss: 0.0796 - avg_loss: 0.0980 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9702\n",
      "Epoch 00388: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5845 - op_main_loss: 0.1291 - op_conv_loss: 0.0803 - avg_loss: 0.0983 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9700 - val_loss: 1.3313 - val_op_main_loss: 0.3231 - val_op_conv_loss: 0.4067 - val_avg_loss: 0.3252 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8895\n",
      "Epoch 389/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5799 - op_main_loss: 0.1301 - op_conv_loss: 0.0758 - avg_loss: 0.0972 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9685\n",
      "Epoch 00389: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5806 - op_main_loss: 0.1302 - op_conv_loss: 0.0762 - avg_loss: 0.0974 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9686 - val_loss: 1.3760 - val_op_main_loss: 0.3412 - val_op_conv_loss: 0.4197 - val_avg_loss: 0.3381 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8886\n",
      "Epoch 390/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.5599 - op_main_loss: 0.1196 - op_conv_loss: 0.0725 - avg_loss: 0.0909 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9704\n",
      "Epoch 00390: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5621 - op_main_loss: 0.1202 - op_conv_loss: 0.0734 - avg_loss: 0.0915 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9702 - val_loss: 1.4098 - val_op_main_loss: 0.3301 - val_op_conv_loss: 0.4578 - val_avg_loss: 0.3452 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8905\n",
      "Epoch 391/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6004 - op_main_loss: 0.1328 - op_conv_loss: 0.0882 - avg_loss: 0.1030 - op_main_accuracy: 0.9547 - op_conv_accuracy: 0.9678 - avg_accuracy: 0.9678\n",
      "Epoch 00391: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5999 - op_main_loss: 0.1327 - op_conv_loss: 0.0880 - avg_loss: 0.1028 - op_main_accuracy: 0.9549 - op_conv_accuracy: 0.9676 - avg_accuracy: 0.9681 - val_loss: 1.4275 - val_op_main_loss: 0.3296 - val_op_conv_loss: 0.4747 - val_avg_loss: 0.3471 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8829\n",
      "Epoch 392/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5717 - op_main_loss: 0.1261 - op_conv_loss: 0.0748 - avg_loss: 0.0945 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9724\n",
      "Epoch 00392: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5717 - op_main_loss: 0.1261 - op_conv_loss: 0.0748 - avg_loss: 0.0945 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9724 - val_loss: 1.3646 - val_op_main_loss: 0.3151 - val_op_conv_loss: 0.4457 - val_avg_loss: 0.3276 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8924\n",
      "Epoch 393/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5739 - op_main_loss: 0.1270 - op_conv_loss: 0.0752 - avg_loss: 0.0955 - op_main_accuracy: 0.9566 - op_conv_accuracy: 0.9685 - avg_accuracy: 0.9695\n",
      "Epoch 00393: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5751 - op_main_loss: 0.1275 - op_conv_loss: 0.0755 - avg_loss: 0.0959 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9690 - val_loss: 1.4288 - val_op_main_loss: 0.3384 - val_op_conv_loss: 0.4623 - val_avg_loss: 0.3518 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8914\n",
      "Epoch 394/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5892 - op_main_loss: 0.1327 - op_conv_loss: 0.0801 - avg_loss: 0.1000 - op_main_accuracy: 0.9531 - op_conv_accuracy: 0.9673 - avg_accuracy: 0.9673\n",
      "Epoch 00394: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5862 - op_main_loss: 0.1316 - op_conv_loss: 0.0791 - avg_loss: 0.0990 - op_main_accuracy: 0.9537 - op_conv_accuracy: 0.9679 - avg_accuracy: 0.9679 - val_loss: 1.4210 - val_op_main_loss: 0.3424 - val_op_conv_loss: 0.4511 - val_avg_loss: 0.3509 - val_op_main_accuracy: 0.8801 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8952\n",
      "Epoch 395/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5783 - op_main_loss: 0.1254 - op_conv_loss: 0.0797 - avg_loss: 0.0971 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9671 - avg_accuracy: 0.9668\n",
      "Epoch 00395: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.5795 - op_main_loss: 0.1256 - op_conv_loss: 0.0803 - avg_loss: 0.0974 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9669 - avg_accuracy: 0.9667 - val_loss: 1.3099 - val_op_main_loss: 0.3070 - val_op_conv_loss: 0.4129 - val_avg_loss: 0.3140 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8839\n",
      "Epoch 396/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5684 - op_main_loss: 0.1236 - op_conv_loss: 0.0749 - avg_loss: 0.0935 - op_main_accuracy: 0.9580 - op_conv_accuracy: 0.9704 - avg_accuracy: 0.9711\n",
      "Epoch 00396: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5681 - op_main_loss: 0.1235 - op_conv_loss: 0.0748 - avg_loss: 0.0934 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9705 - avg_accuracy: 0.9712 - val_loss: 1.3047 - val_op_main_loss: 0.3083 - val_op_conv_loss: 0.4062 - val_avg_loss: 0.3139 - val_op_main_accuracy: 0.8980 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8942\n",
      "Epoch 397/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5641 - op_main_loss: 0.1245 - op_conv_loss: 0.0716 - avg_loss: 0.0919 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9721\n",
      "Epoch 00397: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 0.5677 - op_main_loss: 0.1255 - op_conv_loss: 0.0733 - avg_loss: 0.0929 - op_main_accuracy: 0.9556 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9716 - val_loss: 1.3569 - val_op_main_loss: 0.3238 - val_op_conv_loss: 0.4272 - val_avg_loss: 0.3298 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8820\n",
      "Epoch 398/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5658 - op_main_loss: 0.1263 - op_conv_loss: 0.0711 - avg_loss: 0.0924 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9735\n",
      "Epoch 00398: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5658 - op_main_loss: 0.1263 - op_conv_loss: 0.0711 - avg_loss: 0.0924 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9735 - val_loss: 1.5917 - val_op_main_loss: 0.3414 - val_op_conv_loss: 0.5946 - val_avg_loss: 0.3797 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8640 - val_avg_accuracy: 0.8763\n",
      "Epoch 399/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5902 - op_main_loss: 0.1333 - op_conv_loss: 0.0804 - avg_loss: 0.1004 - op_main_accuracy: 0.9543 - op_conv_accuracy: 0.9685 - avg_accuracy: 0.9685\n",
      "Epoch 00399: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5900 - op_main_loss: 0.1333 - op_conv_loss: 0.0803 - avg_loss: 0.1004 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9686 - val_loss: 1.3351 - val_op_main_loss: 0.3153 - val_op_conv_loss: 0.4195 - val_avg_loss: 0.3241 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8895\n",
      "Epoch 400/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5963 - op_main_loss: 0.1323 - op_conv_loss: 0.0849 - avg_loss: 0.1027 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9648 - avg_accuracy: 0.9653\n",
      "Epoch 00400: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5963 - op_main_loss: 0.1323 - op_conv_loss: 0.0849 - avg_loss: 0.1027 - op_main_accuracy: 0.9518 - op_conv_accuracy: 0.9648 - avg_accuracy: 0.9653 - val_loss: 1.3512 - val_op_main_loss: 0.3143 - val_op_conv_loss: 0.4328 - val_avg_loss: 0.3280 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8914\n",
      "Epoch 401/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5622 - op_main_loss: 0.1219 - op_conv_loss: 0.0725 - avg_loss: 0.0917 - op_main_accuracy: 0.9600 - op_conv_accuracy: 0.9732 - avg_accuracy: 0.9721\n",
      "Epoch 00401: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5622 - op_main_loss: 0.1219 - op_conv_loss: 0.0724 - avg_loss: 0.0917 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9721 - val_loss: 1.3519 - val_op_main_loss: 0.3346 - val_op_conv_loss: 0.4089 - val_avg_loss: 0.3324 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.8961\n",
      "Epoch 402/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.5627 - op_main_loss: 0.1197 - op_conv_loss: 0.0753 - avg_loss: 0.0918 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9685\n",
      "Epoch 00402: val_avg_accuracy did not improve from 0.89991\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5643 - op_main_loss: 0.1205 - op_conv_loss: 0.0757 - avg_loss: 0.0923 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9679 - val_loss: 1.5311 - val_op_main_loss: 0.3866 - val_op_conv_loss: 0.4790 - val_avg_loss: 0.3903 - val_op_main_accuracy: 0.8744 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8801\n",
      "Epoch 403/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5477 - op_main_loss: 0.1191 - op_conv_loss: 0.0658 - avg_loss: 0.0877 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9759\n",
      "Epoch 00403: val_avg_accuracy improved from 0.89991 to 0.90179, saving model to ./weight_cp\\weight_lstm2.hdf5\n",
      "133/133 [==============================] - 2s 16ms/step - loss: 0.5477 - op_main_loss: 0.1191 - op_conv_loss: 0.0658 - avg_loss: 0.0877 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9759 - val_loss: 1.3779 - val_op_main_loss: 0.3430 - val_op_conv_loss: 0.4189 - val_avg_loss: 0.3410 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8942 - val_avg_accuracy: 0.9018\n",
      "Epoch 404/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5629 - op_main_loss: 0.1228 - op_conv_loss: 0.0730 - avg_loss: 0.0921 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9719\n",
      "Epoch 00404: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5631 - op_main_loss: 0.1228 - op_conv_loss: 0.0731 - avg_loss: 0.0921 - op_main_accuracy: 0.9610 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9721 - val_loss: 1.3829 - val_op_main_loss: 0.3256 - val_op_conv_loss: 0.4465 - val_avg_loss: 0.3358 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8914\n",
      "Epoch 405/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5936 - op_main_loss: 0.1347 - op_conv_loss: 0.0825 - avg_loss: 0.1014 - op_main_accuracy: 0.9531 - op_conv_accuracy: 0.9690 - avg_accuracy: 0.9661\n",
      "Epoch 00405: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5924 - op_main_loss: 0.1342 - op_conv_loss: 0.0822 - avg_loss: 0.1011 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9693 - avg_accuracy: 0.9664 - val_loss: 1.4055 - val_op_main_loss: 0.3466 - val_op_conv_loss: 0.4332 - val_avg_loss: 0.3509 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8820\n",
      "Epoch 406/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5479 - op_main_loss: 0.1174 - op_conv_loss: 0.0682 - avg_loss: 0.0873 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9734 - avg_accuracy: 0.9743\n",
      "Epoch 00406: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5491 - op_main_loss: 0.1177 - op_conv_loss: 0.0688 - avg_loss: 0.0877 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9740 - val_loss: 1.4420 - val_op_main_loss: 0.3389 - val_op_conv_loss: 0.4712 - val_avg_loss: 0.3567 - val_op_main_accuracy: 0.8820 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8848\n",
      "Epoch 407/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5482 - op_main_loss: 0.1174 - op_conv_loss: 0.0678 - avg_loss: 0.0877 - op_main_accuracy: 0.9651 - op_conv_accuracy: 0.9729 - avg_accuracy: 0.9755\n",
      "Epoch 00407: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5554 - op_main_loss: 0.1192 - op_conv_loss: 0.0711 - avg_loss: 0.0898 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9745 - val_loss: 1.4964 - val_op_main_loss: 0.3932 - val_op_conv_loss: 0.4480 - val_avg_loss: 0.3799 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8754\n",
      "Epoch 408/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5809 - op_main_loss: 0.1302 - op_conv_loss: 0.0782 - avg_loss: 0.0976 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9712\n",
      "Epoch 00408: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5809 - op_main_loss: 0.1302 - op_conv_loss: 0.0782 - avg_loss: 0.0976 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9712 - val_loss: 1.3411 - val_op_main_loss: 0.3169 - val_op_conv_loss: 0.4247 - val_avg_loss: 0.3246 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8895\n",
      "Epoch 409/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5694 - op_main_loss: 0.1249 - op_conv_loss: 0.0748 - avg_loss: 0.0946 - op_main_accuracy: 0.9567 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9712\n",
      "Epoch 00409: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5707 - op_main_loss: 0.1254 - op_conv_loss: 0.0752 - avg_loss: 0.0950 - op_main_accuracy: 0.9563 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9707 - val_loss: 1.4749 - val_op_main_loss: 0.3452 - val_op_conv_loss: 0.4902 - val_avg_loss: 0.3644 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8801\n",
      "Epoch 410/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5607 - op_main_loss: 0.1214 - op_conv_loss: 0.0724 - avg_loss: 0.0918 - op_main_accuracy: 0.9623 - op_conv_accuracy: 0.9697 - avg_accuracy: 0.9700\n",
      "Epoch 00410: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5591 - op_main_loss: 0.1207 - op_conv_loss: 0.0720 - avg_loss: 0.0913 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9702 - val_loss: 1.5667 - val_op_main_loss: 0.3578 - val_op_conv_loss: 0.5483 - val_avg_loss: 0.3858 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8725\n",
      "Epoch 411/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5341 - op_main_loss: 0.1124 - op_conv_loss: 0.0641 - avg_loss: 0.0829 - op_main_accuracy: 0.9650 - op_conv_accuracy: 0.9783 - avg_accuracy: 0.9773\n",
      "Epoch 00411: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5341 - op_main_loss: 0.1124 - op_conv_loss: 0.0641 - avg_loss: 0.0829 - op_main_accuracy: 0.9650 - op_conv_accuracy: 0.9783 - avg_accuracy: 0.9773 - val_loss: 1.4141 - val_op_main_loss: 0.3293 - val_op_conv_loss: 0.4641 - val_avg_loss: 0.3465 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8801\n",
      "Epoch 412/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5436 - op_main_loss: 0.1172 - op_conv_loss: 0.0660 - avg_loss: 0.0864 - op_main_accuracy: 0.9644 - op_conv_accuracy: 0.9753 - avg_accuracy: 0.9741\n",
      "Epoch 00412: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5493 - op_main_loss: 0.1189 - op_conv_loss: 0.0683 - avg_loss: 0.0881 - op_main_accuracy: 0.9629 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9728 - val_loss: 1.3974 - val_op_main_loss: 0.3397 - val_op_conv_loss: 0.4409 - val_avg_loss: 0.3431 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8924\n",
      "Epoch 413/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6005 - op_main_loss: 0.1357 - op_conv_loss: 0.0869 - avg_loss: 0.1040 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9654 - avg_accuracy: 0.9676\n",
      "Epoch 00413: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6015 - op_main_loss: 0.1362 - op_conv_loss: 0.0870 - avg_loss: 0.1043 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9653 - avg_accuracy: 0.9676 - val_loss: 1.4064 - val_op_main_loss: 0.3276 - val_op_conv_loss: 0.4589 - val_avg_loss: 0.3455 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8839\n",
      "Epoch 414/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5497 - op_main_loss: 0.1182 - op_conv_loss: 0.0685 - avg_loss: 0.0885 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9740\n",
      "Epoch 00414: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5497 - op_main_loss: 0.1182 - op_conv_loss: 0.0685 - avg_loss: 0.0885 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9740 - val_loss: 1.3391 - val_op_main_loss: 0.3185 - val_op_conv_loss: 0.4212 - val_avg_loss: 0.3251 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8924\n",
      "Epoch 415/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5766 - op_main_loss: 0.1321 - op_conv_loss: 0.0735 - avg_loss: 0.0961 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9711\n",
      "Epoch 00415: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5767 - op_main_loss: 0.1321 - op_conv_loss: 0.0736 - avg_loss: 0.0962 - op_main_accuracy: 0.9530 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9712 - val_loss: 1.3292 - val_op_main_loss: 0.3155 - val_op_conv_loss: 0.4171 - val_avg_loss: 0.3213 - val_op_main_accuracy: 0.8961 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 416/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5546 - op_main_loss: 0.1170 - op_conv_loss: 0.0735 - avg_loss: 0.0891 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9692\n",
      "Epoch 00416: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5584 - op_main_loss: 0.1177 - op_conv_loss: 0.0755 - avg_loss: 0.0901 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9686 - val_loss: 1.3666 - val_op_main_loss: 0.3274 - val_op_conv_loss: 0.4302 - val_avg_loss: 0.3346 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8942\n",
      "Epoch 417/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5559 - op_main_loss: 0.1182 - op_conv_loss: 0.0734 - avg_loss: 0.0900 - op_main_accuracy: 0.9634 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9721\n",
      "Epoch 00417: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5573 - op_main_loss: 0.1190 - op_conv_loss: 0.0736 - avg_loss: 0.0905 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9719 - val_loss: 1.7015 - val_op_main_loss: 0.4134 - val_op_conv_loss: 0.5800 - val_avg_loss: 0.4344 - val_op_main_accuracy: 0.8659 - val_op_conv_accuracy: 0.8612 - val_avg_accuracy: 0.8650\n",
      "Epoch 418/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5666 - op_main_loss: 0.1254 - op_conv_loss: 0.0739 - avg_loss: 0.0939 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9714\n",
      "Epoch 00418: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5666 - op_main_loss: 0.1254 - op_conv_loss: 0.0739 - avg_loss: 0.0939 - op_main_accuracy: 0.9568 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9714 - val_loss: 1.3352 - val_op_main_loss: 0.3143 - val_op_conv_loss: 0.4273 - val_avg_loss: 0.3201 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8933\n",
      "Epoch 419/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5678 - op_main_loss: 0.1261 - op_conv_loss: 0.0740 - avg_loss: 0.0942 - op_main_accuracy: 0.9562 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9717\n",
      "Epoch 00419: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5659 - op_main_loss: 0.1251 - op_conv_loss: 0.0737 - avg_loss: 0.0936 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9719 - val_loss: 1.3181 - val_op_main_loss: 0.3040 - val_op_conv_loss: 0.4226 - val_avg_loss: 0.3181 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8876\n",
      "Epoch 420/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5622 - op_main_loss: 0.1208 - op_conv_loss: 0.0751 - avg_loss: 0.0929 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9721\n",
      "Epoch 00420: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5622 - op_main_loss: 0.1208 - op_conv_loss: 0.0751 - avg_loss: 0.0929 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9721 - val_loss: 1.3481 - val_op_main_loss: 0.3138 - val_op_conv_loss: 0.4385 - val_avg_loss: 0.3225 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8933\n",
      "Epoch 421/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5656 - op_main_loss: 0.1254 - op_conv_loss: 0.0735 - avg_loss: 0.0934 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9733\n",
      "Epoch 00421: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5656 - op_main_loss: 0.1254 - op_conv_loss: 0.0735 - avg_loss: 0.0934 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9733 - val_loss: 1.3324 - val_op_main_loss: 0.3211 - val_op_conv_loss: 0.4149 - val_avg_loss: 0.3231 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8905\n",
      "Epoch 422/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5448 - op_main_loss: 0.1148 - op_conv_loss: 0.0699 - avg_loss: 0.0868 - op_main_accuracy: 0.9678 - op_conv_accuracy: 0.9758 - avg_accuracy: 0.9760\n",
      "Epoch 00422: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5433 - op_main_loss: 0.1144 - op_conv_loss: 0.0693 - avg_loss: 0.0864 - op_main_accuracy: 0.9676 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9761 - val_loss: 1.3601 - val_op_main_loss: 0.3179 - val_op_conv_loss: 0.4391 - val_avg_loss: 0.3299 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8905\n",
      "Epoch 423/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5640 - op_main_loss: 0.1222 - op_conv_loss: 0.0764 - avg_loss: 0.0925 - op_main_accuracy: 0.9602 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9718\n",
      "Epoch 00423: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5642 - op_main_loss: 0.1222 - op_conv_loss: 0.0765 - avg_loss: 0.0925 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9716 - val_loss: 1.3490 - val_op_main_loss: 0.3302 - val_op_conv_loss: 0.4194 - val_avg_loss: 0.3268 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8914\n",
      "Epoch 424/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5965 - op_main_loss: 0.1377 - op_conv_loss: 0.0823 - avg_loss: 0.1033 - op_main_accuracy: 0.9519 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9666\n",
      "Epoch 00424: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5955 - op_main_loss: 0.1374 - op_conv_loss: 0.0819 - avg_loss: 0.1030 - op_main_accuracy: 0.9523 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9669 - val_loss: 1.4656 - val_op_main_loss: 0.3681 - val_op_conv_loss: 0.4542 - val_avg_loss: 0.3700 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8820\n",
      "Epoch 425/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5596 - op_main_loss: 0.1210 - op_conv_loss: 0.0736 - avg_loss: 0.0919 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9709\n",
      "Epoch 00425: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5590 - op_main_loss: 0.1212 - op_conv_loss: 0.0730 - avg_loss: 0.0917 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9714 - val_loss: 1.3096 - val_op_main_loss: 0.3145 - val_op_conv_loss: 0.4041 - val_avg_loss: 0.3179 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8999\n",
      "Epoch 426/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5515 - op_main_loss: 0.1185 - op_conv_loss: 0.0710 - avg_loss: 0.0889 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9750\n",
      "Epoch 00426: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5515 - op_main_loss: 0.1185 - op_conv_loss: 0.0710 - avg_loss: 0.0889 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9750 - val_loss: 1.3800 - val_op_main_loss: 0.3153 - val_op_conv_loss: 0.4587 - val_avg_loss: 0.3329 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8725 - val_avg_accuracy: 0.8791\n",
      "Epoch 427/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5731 - op_main_loss: 0.1240 - op_conv_loss: 0.0798 - avg_loss: 0.0963 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9680 - avg_accuracy: 0.9683\n",
      "Epoch 00427: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5725 - op_main_loss: 0.1239 - op_conv_loss: 0.0796 - avg_loss: 0.0962 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9686 - avg_accuracy: 0.9681 - val_loss: 1.5063 - val_op_main_loss: 0.3586 - val_op_conv_loss: 0.5011 - val_avg_loss: 0.3737 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8754\n",
      "Epoch 428/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5565 - op_main_loss: 0.1241 - op_conv_loss: 0.0689 - avg_loss: 0.0904 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9705\n",
      "Epoch 00428: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5565 - op_main_loss: 0.1241 - op_conv_loss: 0.0689 - avg_loss: 0.0904 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9705 - val_loss: 1.5757 - val_op_main_loss: 0.3690 - val_op_conv_loss: 0.5409 - val_avg_loss: 0.3930 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8744 - val_avg_accuracy: 0.8725\n",
      "Epoch 429/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5572 - op_main_loss: 0.1195 - op_conv_loss: 0.0744 - avg_loss: 0.0906 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9695\n",
      "Epoch 00429: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5574 - op_main_loss: 0.1195 - op_conv_loss: 0.0745 - avg_loss: 0.0907 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9690 - val_loss: 1.5120 - val_op_main_loss: 0.3355 - val_op_conv_loss: 0.5406 - val_avg_loss: 0.3636 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8744 - val_avg_accuracy: 0.8772\n",
      "Epoch 430/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5524 - op_main_loss: 0.1196 - op_conv_loss: 0.0707 - avg_loss: 0.0898 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9740\n",
      "Epoch 00430: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5524 - op_main_loss: 0.1196 - op_conv_loss: 0.0707 - avg_loss: 0.0898 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9740 - val_loss: 1.3774 - val_op_main_loss: 0.3318 - val_op_conv_loss: 0.4360 - val_avg_loss: 0.3375 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8886\n",
      "Epoch 431/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5279 - op_main_loss: 0.1121 - op_conv_loss: 0.0611 - avg_loss: 0.0824 - op_main_accuracy: 0.9661 - op_conv_accuracy: 0.9764 - avg_accuracy: 0.9776\n",
      "Epoch 00431: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5293 - op_main_loss: 0.1127 - op_conv_loss: 0.0614 - avg_loss: 0.0828 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9776 - val_loss: 1.3779 - val_op_main_loss: 0.3282 - val_op_conv_loss: 0.4448 - val_avg_loss: 0.3324 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8961\n",
      "Epoch 432/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5747 - op_main_loss: 0.1202 - op_conv_loss: 0.0858 - avg_loss: 0.0961 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9675 - avg_accuracy: 0.9649\n",
      "Epoch 00432: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5732 - op_main_loss: 0.1198 - op_conv_loss: 0.0852 - avg_loss: 0.0957 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9653 - val_loss: 1.5656 - val_op_main_loss: 0.3934 - val_op_conv_loss: 0.5051 - val_avg_loss: 0.3953 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8801\n",
      "Epoch 433/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5499 - op_main_loss: 0.1181 - op_conv_loss: 0.0703 - avg_loss: 0.0899 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9693\n",
      "Epoch 00433: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5499 - op_main_loss: 0.1181 - op_conv_loss: 0.0703 - avg_loss: 0.0899 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9712 - avg_accuracy: 0.9693 - val_loss: 1.3700 - val_op_main_loss: 0.3184 - val_op_conv_loss: 0.4494 - val_avg_loss: 0.3305 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8848\n",
      "Epoch 434/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5417 - op_main_loss: 0.1190 - op_conv_loss: 0.0641 - avg_loss: 0.0863 - op_main_accuracy: 0.9571 - op_conv_accuracy: 0.9749 - avg_accuracy: 0.9737\n",
      "Epoch 00434: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5415 - op_main_loss: 0.1189 - op_conv_loss: 0.0640 - avg_loss: 0.0863 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9738 - val_loss: 1.4628 - val_op_main_loss: 0.3480 - val_op_conv_loss: 0.4827 - val_avg_loss: 0.3599 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8772 - val_avg_accuracy: 0.8839\n",
      "Epoch 435/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5541 - op_main_loss: 0.1185 - op_conv_loss: 0.0734 - avg_loss: 0.0904 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9742\n",
      "Epoch 00435: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5541 - op_main_loss: 0.1185 - op_conv_loss: 0.0734 - avg_loss: 0.0904 - op_main_accuracy: 0.9605 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9742 - val_loss: 1.3394 - val_op_main_loss: 0.3191 - val_op_conv_loss: 0.4283 - val_avg_loss: 0.3205 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8942\n",
      "Epoch 436/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5782 - op_main_loss: 0.1267 - op_conv_loss: 0.0824 - avg_loss: 0.0978 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9688\n",
      "Epoch 00436: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5782 - op_main_loss: 0.1267 - op_conv_loss: 0.0824 - avg_loss: 0.0978 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9688 - avg_accuracy: 0.9688 - val_loss: 1.6662 - val_op_main_loss: 0.3816 - val_op_conv_loss: 0.6006 - val_avg_loss: 0.4133 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8669 - val_avg_accuracy: 0.8716\n",
      "Epoch 437/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5515 - op_main_loss: 0.1173 - op_conv_loss: 0.0733 - avg_loss: 0.0903 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9716\n",
      "Epoch 00437: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5515 - op_main_loss: 0.1173 - op_conv_loss: 0.0733 - avg_loss: 0.0903 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9716 - val_loss: 1.3752 - val_op_main_loss: 0.3229 - val_op_conv_loss: 0.4469 - val_avg_loss: 0.3347 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8895\n",
      "Epoch 438/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - ETA: 0s - loss: 0.5370 - op_main_loss: 0.1137 - op_conv_loss: 0.0670 - avg_loss: 0.0858 - op_main_accuracy: 0.9648 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9731\n",
      "Epoch 00438: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5370 - op_main_loss: 0.1137 - op_conv_loss: 0.0670 - avg_loss: 0.0858 - op_main_accuracy: 0.9648 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9731 - val_loss: 1.4030 - val_op_main_loss: 0.3352 - val_op_conv_loss: 0.4505 - val_avg_loss: 0.3471 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8933\n",
      "Epoch 439/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5456 - op_main_loss: 0.1181 - op_conv_loss: 0.0691 - avg_loss: 0.0880 - op_main_accuracy: 0.9602 - op_conv_accuracy: 0.9751 - avg_accuracy: 0.9751\n",
      "Epoch 00439: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5454 - op_main_loss: 0.1181 - op_conv_loss: 0.0690 - avg_loss: 0.0880 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9752 - val_loss: 1.3251 - val_op_main_loss: 0.3320 - val_op_conv_loss: 0.3987 - val_avg_loss: 0.3241 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8952\n",
      "Epoch 440/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5325 - op_main_loss: 0.1113 - op_conv_loss: 0.0670 - avg_loss: 0.0840 - op_main_accuracy: 0.9683 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9764\n",
      "Epoch 00440: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5335 - op_main_loss: 0.1117 - op_conv_loss: 0.0673 - avg_loss: 0.0843 - op_main_accuracy: 0.9676 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9761 - val_loss: 1.5986 - val_op_main_loss: 0.3946 - val_op_conv_loss: 0.5273 - val_avg_loss: 0.4067 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8725\n",
      "Epoch 441/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5519 - op_main_loss: 0.1231 - op_conv_loss: 0.0691 - avg_loss: 0.0899 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9738\n",
      "Epoch 00441: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5519 - op_main_loss: 0.1231 - op_conv_loss: 0.0691 - avg_loss: 0.0899 - op_main_accuracy: 0.9570 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9738 - val_loss: 1.3992 - val_op_main_loss: 0.3179 - val_op_conv_loss: 0.4780 - val_avg_loss: 0.3330 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8839\n",
      "Epoch 442/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5899 - op_main_loss: 0.1344 - op_conv_loss: 0.0841 - avg_loss: 0.1013 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9667\n",
      "Epoch 00442: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5899 - op_main_loss: 0.1344 - op_conv_loss: 0.0841 - avg_loss: 0.1013 - op_main_accuracy: 0.9542 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9667 - val_loss: 1.5127 - val_op_main_loss: 0.3331 - val_op_conv_loss: 0.5381 - val_avg_loss: 0.3709 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8678 - val_avg_accuracy: 0.8782\n",
      "Epoch 443/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5646 - op_main_loss: 0.1183 - op_conv_loss: 0.0813 - avg_loss: 0.0941 - op_main_accuracy: 0.9611 - op_conv_accuracy: 0.9659 - avg_accuracy: 0.9671\n",
      "Epoch 00443: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5637 - op_main_loss: 0.1180 - op_conv_loss: 0.0809 - avg_loss: 0.0938 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9660 - avg_accuracy: 0.9672 - val_loss: 1.4565 - val_op_main_loss: 0.3424 - val_op_conv_loss: 0.4862 - val_avg_loss: 0.3569 - val_op_main_accuracy: 0.8924 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8867\n",
      "Epoch 444/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5627 - op_main_loss: 0.1209 - op_conv_loss: 0.0773 - avg_loss: 0.0937 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9699 - avg_accuracy: 0.9673\n",
      "Epoch 00444: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5623 - op_main_loss: 0.1208 - op_conv_loss: 0.0772 - avg_loss: 0.0936 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9674 - val_loss: 1.3764 - val_op_main_loss: 0.3309 - val_op_conv_loss: 0.4364 - val_avg_loss: 0.3389 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8942\n",
      "Epoch 445/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5424 - op_main_loss: 0.1176 - op_conv_loss: 0.0677 - avg_loss: 0.0870 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9753 - avg_accuracy: 0.9748\n",
      "Epoch 00445: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5434 - op_main_loss: 0.1178 - op_conv_loss: 0.0683 - avg_loss: 0.0873 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9745 - val_loss: 1.3080 - val_op_main_loss: 0.3116 - val_op_conv_loss: 0.4112 - val_avg_loss: 0.3151 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8895\n",
      "Epoch 446/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5713 - op_main_loss: 0.1239 - op_conv_loss: 0.0809 - avg_loss: 0.0959 - op_main_accuracy: 0.9560 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9661\n",
      "Epoch 00446: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5703 - op_main_loss: 0.1234 - op_conv_loss: 0.0807 - avg_loss: 0.0956 - op_main_accuracy: 0.9565 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9662 - val_loss: 1.4655 - val_op_main_loss: 0.3307 - val_op_conv_loss: 0.5085 - val_avg_loss: 0.3554 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8621 - val_avg_accuracy: 0.8650\n",
      "Epoch 447/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5319 - op_main_loss: 0.1146 - op_conv_loss: 0.0621 - avg_loss: 0.0842 - op_main_accuracy: 0.9645 - op_conv_accuracy: 0.9730 - avg_accuracy: 0.9742\n",
      "Epoch 00447: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5343 - op_main_loss: 0.1150 - op_conv_loss: 0.0635 - avg_loss: 0.0847 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9740 - val_loss: 1.3988 - val_op_main_loss: 0.3271 - val_op_conv_loss: 0.4614 - val_avg_loss: 0.3394 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8886\n",
      "Epoch 448/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5476 - op_main_loss: 0.1154 - op_conv_loss: 0.0727 - avg_loss: 0.0890 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9702\n",
      "Epoch 00448: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5464 - op_main_loss: 0.1150 - op_conv_loss: 0.0722 - avg_loss: 0.0886 - op_main_accuracy: 0.9603 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9707 - val_loss: 1.3753 - val_op_main_loss: 0.3325 - val_op_conv_loss: 0.4346 - val_avg_loss: 0.3377 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8839\n",
      "Epoch 449/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5368 - op_main_loss: 0.1144 - op_conv_loss: 0.0665 - avg_loss: 0.0857 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9733\n",
      "Epoch 00449: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5368 - op_main_loss: 0.1144 - op_conv_loss: 0.0665 - avg_loss: 0.0857 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9733 - val_loss: 1.3737 - val_op_main_loss: 0.3303 - val_op_conv_loss: 0.4406 - val_avg_loss: 0.3325 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8820\n",
      "Epoch 450/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.5779 - op_main_loss: 0.1291 - op_conv_loss: 0.0803 - avg_loss: 0.0982 - op_main_accuracy: 0.9606 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9704\n",
      "Epoch 00450: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5769 - op_main_loss: 0.1287 - op_conv_loss: 0.0800 - avg_loss: 0.0979 - op_main_accuracy: 0.9608 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9707 - val_loss: 1.3703 - val_op_main_loss: 0.3162 - val_op_conv_loss: 0.4558 - val_avg_loss: 0.3279 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8763\n",
      "Epoch 451/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5262 - op_main_loss: 0.1094 - op_conv_loss: 0.0639 - avg_loss: 0.0824 - op_main_accuracy: 0.9671 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9756\n",
      "Epoch 00451: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5268 - op_main_loss: 0.1095 - op_conv_loss: 0.0642 - avg_loss: 0.0826 - op_main_accuracy: 0.9669 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9754 - val_loss: 1.4721 - val_op_main_loss: 0.3546 - val_op_conv_loss: 0.4814 - val_avg_loss: 0.3655 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8857\n",
      "Epoch 452/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5300 - op_main_loss: 0.1103 - op_conv_loss: 0.0664 - avg_loss: 0.0830 - op_main_accuracy: 0.9626 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9718\n",
      "Epoch 00452: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5300 - op_main_loss: 0.1102 - op_conv_loss: 0.0664 - avg_loss: 0.0830 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9721 - avg_accuracy: 0.9719 - val_loss: 1.3463 - val_op_main_loss: 0.3188 - val_op_conv_loss: 0.4342 - val_avg_loss: 0.3233 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8857\n",
      "Epoch 453/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5619 - op_main_loss: 0.1241 - op_conv_loss: 0.0748 - avg_loss: 0.0928 - op_main_accuracy: 0.9583 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9700\n",
      "Epoch 00453: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5622 - op_main_loss: 0.1241 - op_conv_loss: 0.0750 - avg_loss: 0.0929 - op_main_accuracy: 0.9584 - op_conv_accuracy: 0.9714 - avg_accuracy: 0.9695 - val_loss: 1.3610 - val_op_main_loss: 0.3247 - val_op_conv_loss: 0.4332 - val_avg_loss: 0.3329 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8933 - val_avg_accuracy: 0.8952\n",
      "Epoch 454/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5328 - op_main_loss: 0.1117 - op_conv_loss: 0.0668 - avg_loss: 0.0843 - op_main_accuracy: 0.9709 - op_conv_accuracy: 0.9773 - avg_accuracy: 0.9771\n",
      "Epoch 00454: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5331 - op_main_loss: 0.1119 - op_conv_loss: 0.0669 - avg_loss: 0.0844 - op_main_accuracy: 0.9705 - op_conv_accuracy: 0.9773 - avg_accuracy: 0.9771 - val_loss: 1.4680 - val_op_main_loss: 0.3484 - val_op_conv_loss: 0.4852 - val_avg_loss: 0.3646 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8839\n",
      "Epoch 455/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5178 - op_main_loss: 0.1077 - op_conv_loss: 0.0600 - avg_loss: 0.0801 - op_main_accuracy: 0.9678 - op_conv_accuracy: 0.9760 - avg_accuracy: 0.9758\n",
      "Epoch 00455: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5199 - op_main_loss: 0.1078 - op_conv_loss: 0.0616 - avg_loss: 0.0805 - op_main_accuracy: 0.9676 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9752 - val_loss: 1.3705 - val_op_main_loss: 0.3274 - val_op_conv_loss: 0.4398 - val_avg_loss: 0.3336 - val_op_main_accuracy: 0.8914 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8876\n",
      "Epoch 456/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5327 - op_main_loss: 0.1113 - op_conv_loss: 0.0676 - avg_loss: 0.0844 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9763 - avg_accuracy: 0.9753\n",
      "Epoch 00456: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5328 - op_main_loss: 0.1112 - op_conv_loss: 0.0678 - avg_loss: 0.0845 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9752 - val_loss: 1.3943 - val_op_main_loss: 0.3405 - val_op_conv_loss: 0.4420 - val_avg_loss: 0.3428 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8905\n",
      "Epoch 457/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5432 - op_main_loss: 0.1146 - op_conv_loss: 0.0720 - avg_loss: 0.0878 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9744\n",
      "Epoch 00457: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5431 - op_main_loss: 0.1146 - op_conv_loss: 0.0720 - avg_loss: 0.0877 - op_main_accuracy: 0.9631 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9745 - val_loss: 1.3421 - val_op_main_loss: 0.3219 - val_op_conv_loss: 0.4254 - val_avg_loss: 0.3259 - val_op_main_accuracy: 0.8942 - val_op_conv_accuracy: 0.8952 - val_avg_accuracy: 0.8990\n",
      "Epoch 458/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5700 - op_main_loss: 0.1272 - op_conv_loss: 0.0787 - avg_loss: 0.0957 - op_main_accuracy: 0.9571 - op_conv_accuracy: 0.9711 - avg_accuracy: 0.9676\n",
      "Epoch 00458: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5700 - op_main_loss: 0.1271 - op_conv_loss: 0.0788 - avg_loss: 0.0957 - op_main_accuracy: 0.9572 - op_conv_accuracy: 0.9709 - avg_accuracy: 0.9676 - val_loss: 1.3970 - val_op_main_loss: 0.3253 - val_op_conv_loss: 0.4633 - val_avg_loss: 0.3405 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8914 - val_avg_accuracy: 0.8886\n",
      "Epoch 459/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5676 - op_main_loss: 0.1231 - op_conv_loss: 0.0801 - avg_loss: 0.0963 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9683\n",
      "Epoch 00459: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5676 - op_main_loss: 0.1231 - op_conv_loss: 0.0801 - avg_loss: 0.0963 - op_main_accuracy: 0.9579 - op_conv_accuracy: 0.9683 - avg_accuracy: 0.9683 - val_loss: 1.5886 - val_op_main_loss: 0.3687 - val_op_conv_loss: 0.5561 - val_avg_loss: 0.3952 - val_op_main_accuracy: 0.8716 - val_op_conv_accuracy: 0.8716 - val_avg_accuracy: 0.8735\n",
      "Epoch 460/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5685 - op_main_loss: 0.1204 - op_conv_loss: 0.0840 - avg_loss: 0.0952 - op_main_accuracy: 0.9590 - op_conv_accuracy: 0.9654 - avg_accuracy: 0.9669\n",
      "Epoch 00460: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5680 - op_main_loss: 0.1202 - op_conv_loss: 0.0838 - avg_loss: 0.0950 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9655 - avg_accuracy: 0.9669 - val_loss: 1.5875 - val_op_main_loss: 0.3863 - val_op_conv_loss: 0.5280 - val_avg_loss: 0.4038 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8706 - val_avg_accuracy: 0.8763\n",
      "Epoch 461/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5370 - op_main_loss: 0.1157 - op_conv_loss: 0.0657 - avg_loss: 0.0863 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9742\n",
      "Epoch 00461: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5370 - op_main_loss: 0.1157 - op_conv_loss: 0.0657 - avg_loss: 0.0863 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9742 - val_loss: 1.5045 - val_op_main_loss: 0.3691 - val_op_conv_loss: 0.4872 - val_avg_loss: 0.3792 - val_op_main_accuracy: 0.8791 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8839\n",
      "Epoch 462/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.5305 - op_main_loss: 0.1106 - op_conv_loss: 0.0674 - avg_loss: 0.0840 - op_main_accuracy: 0.9664 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9733\n",
      "Epoch 00462: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5324 - op_main_loss: 0.1111 - op_conv_loss: 0.0682 - avg_loss: 0.0846 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9731 - val_loss: 1.7818 - val_op_main_loss: 0.4265 - val_op_conv_loss: 0.6337 - val_avg_loss: 0.4533 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8593 - val_avg_accuracy: 0.8640\n",
      "Epoch 463/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5286 - op_main_loss: 0.1112 - op_conv_loss: 0.0656 - avg_loss: 0.0836 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9749 - avg_accuracy: 0.9735\n",
      "Epoch 00463: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5284 - op_main_loss: 0.1112 - op_conv_loss: 0.0655 - avg_loss: 0.0836 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9735 - val_loss: 1.5801 - val_op_main_loss: 0.3920 - val_op_conv_loss: 0.5174 - val_avg_loss: 0.4028 - val_op_main_accuracy: 0.8763 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8791\n",
      "Epoch 464/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5564 - op_main_loss: 0.1198 - op_conv_loss: 0.0764 - avg_loss: 0.0922 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9683\n",
      "Epoch 00464: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5564 - op_main_loss: 0.1198 - op_conv_loss: 0.0764 - avg_loss: 0.0922 - op_main_accuracy: 0.9589 - op_conv_accuracy: 0.9695 - avg_accuracy: 0.9683 - val_loss: 1.4836 - val_op_main_loss: 0.3619 - val_op_conv_loss: 0.4805 - val_avg_loss: 0.3736 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8810\n",
      "Epoch 465/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5427 - op_main_loss: 0.1182 - op_conv_loss: 0.0686 - avg_loss: 0.0882 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9748 - avg_accuracy: 0.9738\n",
      "Epoch 00465: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5425 - op_main_loss: 0.1179 - op_conv_loss: 0.0689 - avg_loss: 0.0881 - op_main_accuracy: 0.9617 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9735 - val_loss: 1.4196 - val_op_main_loss: 0.3363 - val_op_conv_loss: 0.4675 - val_avg_loss: 0.3478 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8848\n",
      "Epoch 466/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5533 - op_main_loss: 0.1191 - op_conv_loss: 0.0750 - avg_loss: 0.0917 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9746 - avg_accuracy: 0.9726\n",
      "Epoch 00466: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5551 - op_main_loss: 0.1197 - op_conv_loss: 0.0756 - avg_loss: 0.0922 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9724 - val_loss: 1.4236 - val_op_main_loss: 0.3576 - val_op_conv_loss: 0.4405 - val_avg_loss: 0.3580 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8810 - val_avg_accuracy: 0.8886\n",
      "Epoch 467/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5381 - op_main_loss: 0.1156 - op_conv_loss: 0.0680 - avg_loss: 0.0870 - op_main_accuracy: 0.9634 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9738\n",
      "Epoch 00467: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5430 - op_main_loss: 0.1174 - op_conv_loss: 0.0694 - avg_loss: 0.0886 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9728 - val_loss: 1.4025 - val_op_main_loss: 0.3258 - val_op_conv_loss: 0.4678 - val_avg_loss: 0.3414 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8829\n",
      "Epoch 468/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5395 - op_main_loss: 0.1154 - op_conv_loss: 0.0692 - avg_loss: 0.0874 - op_main_accuracy: 0.9640 - op_conv_accuracy: 0.9749 - avg_accuracy: 0.9740\n",
      "Epoch 00468: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5393 - op_main_loss: 0.1154 - op_conv_loss: 0.0691 - avg_loss: 0.0873 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9750 - avg_accuracy: 0.9740 - val_loss: 1.4032 - val_op_main_loss: 0.3348 - val_op_conv_loss: 0.4572 - val_avg_loss: 0.3435 - val_op_main_accuracy: 0.8971 - val_op_conv_accuracy: 0.8895 - val_avg_accuracy: 0.8933\n",
      "Epoch 469/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5487 - op_main_loss: 0.1215 - op_conv_loss: 0.0698 - avg_loss: 0.0899 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9750\n",
      "Epoch 00469: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5487 - op_main_loss: 0.1215 - op_conv_loss: 0.0698 - avg_loss: 0.0899 - op_main_accuracy: 0.9586 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9750 - val_loss: 1.4374 - val_op_main_loss: 0.3272 - val_op_conv_loss: 0.4910 - val_avg_loss: 0.3518 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8857\n",
      "Epoch 470/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5710 - op_main_loss: 0.1248 - op_conv_loss: 0.0827 - avg_loss: 0.0959 - op_main_accuracy: 0.9574 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9699\n",
      "Epoch 00470: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5714 - op_main_loss: 0.1248 - op_conv_loss: 0.0829 - avg_loss: 0.0960 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9700 - avg_accuracy: 0.9698 - val_loss: 1.4004 - val_op_main_loss: 0.3261 - val_op_conv_loss: 0.4651 - val_avg_loss: 0.3411 - val_op_main_accuracy: 0.8829 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8867\n",
      "Epoch 471/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5272 - op_main_loss: 0.1112 - op_conv_loss: 0.0649 - avg_loss: 0.0828 - op_main_accuracy: 0.9661 - op_conv_accuracy: 0.9797 - avg_accuracy: 0.9783\n",
      "Epoch 00471: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5262 - op_main_loss: 0.1108 - op_conv_loss: 0.0646 - avg_loss: 0.0825 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9799 - avg_accuracy: 0.9785 - val_loss: 1.3744 - val_op_main_loss: 0.3291 - val_op_conv_loss: 0.4408 - val_avg_loss: 0.3363 - val_op_main_accuracy: 0.8990 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8914\n",
      "Epoch 472/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5273 - op_main_loss: 0.1098 - op_conv_loss: 0.0665 - avg_loss: 0.0832 - op_main_accuracy: 0.9652 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9737\n",
      "Epoch 00472: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5288 - op_main_loss: 0.1102 - op_conv_loss: 0.0671 - avg_loss: 0.0837 - op_main_accuracy: 0.9650 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9735 - val_loss: 1.5205 - val_op_main_loss: 0.3597 - val_op_conv_loss: 0.5148 - val_avg_loss: 0.3787 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8820\n",
      "Epoch 473/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5728 - op_main_loss: 0.1296 - op_conv_loss: 0.0793 - avg_loss: 0.0966 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9719\n",
      "Epoch 00473: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5728 - op_main_loss: 0.1296 - op_conv_loss: 0.0793 - avg_loss: 0.0966 - op_main_accuracy: 0.9534 - op_conv_accuracy: 0.9702 - avg_accuracy: 0.9719 - val_loss: 1.4936 - val_op_main_loss: 0.3199 - val_op_conv_loss: 0.5522 - val_avg_loss: 0.3541 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8687 - val_avg_accuracy: 0.8763\n",
      "Epoch 474/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.5252 - op_main_loss: 0.1108 - op_conv_loss: 0.0637 - avg_loss: 0.0832 - op_main_accuracy: 0.9676 - op_conv_accuracy: 0.9730 - avg_accuracy: 0.9752\n",
      "Epoch 00474: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5241 - op_main_loss: 0.1104 - op_conv_loss: 0.0634 - avg_loss: 0.0829 - op_main_accuracy: 0.9679 - op_conv_accuracy: 0.9733 - avg_accuracy: 0.9754 - val_loss: 1.4433 - val_op_main_loss: 0.3503 - val_op_conv_loss: 0.4675 - val_avg_loss: 0.3581 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8924\n",
      "Epoch 475/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5349 - op_main_loss: 0.1176 - op_conv_loss: 0.0644 - avg_loss: 0.0855 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9760 - avg_accuracy: 0.9746\n",
      "Epoch 00475: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5340 - op_main_loss: 0.1170 - op_conv_loss: 0.0644 - avg_loss: 0.0852 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9761 - avg_accuracy: 0.9747 - val_loss: 1.5425 - val_op_main_loss: 0.3455 - val_op_conv_loss: 0.5513 - val_avg_loss: 0.3781 - val_op_main_accuracy: 0.8772 - val_op_conv_accuracy: 0.8782 - val_avg_accuracy: 0.8772\n",
      "Epoch 476/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5343 - op_main_loss: 0.1132 - op_conv_loss: 0.0676 - avg_loss: 0.0859 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9735\n",
      "Epoch 00476: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5343 - op_main_loss: 0.1132 - op_conv_loss: 0.0676 - avg_loss: 0.0859 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9735 - val_loss: 1.4660 - val_op_main_loss: 0.3327 - val_op_conv_loss: 0.5106 - val_avg_loss: 0.3550 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8782\n",
      "Epoch 477/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5347 - op_main_loss: 0.1154 - op_conv_loss: 0.0657 - avg_loss: 0.0860 - op_main_accuracy: 0.9635 - op_conv_accuracy: 0.9730 - avg_accuracy: 0.9730\n",
      "Epoch 00477: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5345 - op_main_loss: 0.1154 - op_conv_loss: 0.0657 - avg_loss: 0.0860 - op_main_accuracy: 0.9636 - op_conv_accuracy: 0.9731 - avg_accuracy: 0.9731 - val_loss: 1.3647 - val_op_main_loss: 0.3288 - val_op_conv_loss: 0.4337 - val_avg_loss: 0.3349 - val_op_main_accuracy: 0.8933 - val_op_conv_accuracy: 0.8961 - val_avg_accuracy: 0.8933\n",
      "Epoch 478/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5371 - op_main_loss: 0.1111 - op_conv_loss: 0.0725 - avg_loss: 0.0861 - op_main_accuracy: 0.9641 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9734\n",
      "Epoch 00478: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5359 - op_main_loss: 0.1108 - op_conv_loss: 0.0720 - avg_loss: 0.0858 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9735 - val_loss: 1.3875 - val_op_main_loss: 0.3401 - val_op_conv_loss: 0.4358 - val_avg_loss: 0.3447 - val_op_main_accuracy: 0.8876 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8924\n",
      "Epoch 479/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5501 - op_main_loss: 0.1201 - op_conv_loss: 0.0724 - avg_loss: 0.0904 - op_main_accuracy: 0.9600 - op_conv_accuracy: 0.9718 - avg_accuracy: 0.9721\n",
      "Epoch 00479: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5520 - op_main_loss: 0.1206 - op_conv_loss: 0.0731 - avg_loss: 0.0910 - op_main_accuracy: 0.9598 - op_conv_accuracy: 0.9716 - avg_accuracy: 0.9719 - val_loss: 1.7387 - val_op_main_loss: 0.3946 - val_op_conv_loss: 0.6387 - val_avg_loss: 0.4375 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8602 - val_avg_accuracy: 0.8669\n",
      "Epoch 480/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5281 - op_main_loss: 0.1143 - op_conv_loss: 0.0633 - avg_loss: 0.0828 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9728\n",
      "Epoch 00480: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5270 - op_main_loss: 0.1137 - op_conv_loss: 0.0632 - avg_loss: 0.0824 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9728 - val_loss: 1.4535 - val_op_main_loss: 0.3616 - val_op_conv_loss: 0.4586 - val_avg_loss: 0.3655 - val_op_main_accuracy: 0.8782 - val_op_conv_accuracy: 0.8905 - val_avg_accuracy: 0.8886\n",
      "Epoch 481/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5212 - op_main_loss: 0.1063 - op_conv_loss: 0.0656 - avg_loss: 0.0817 - op_main_accuracy: 0.9671 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9752\n",
      "Epoch 00481: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5204 - op_main_loss: 0.1060 - op_conv_loss: 0.0653 - avg_loss: 0.0814 - op_main_accuracy: 0.9672 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9754 - val_loss: 1.3750 - val_op_main_loss: 0.3304 - val_op_conv_loss: 0.4415 - val_avg_loss: 0.3356 - val_op_main_accuracy: 0.8886 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8914\n",
      "Epoch 482/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5306 - op_main_loss: 0.1104 - op_conv_loss: 0.0681 - avg_loss: 0.0847 - op_main_accuracy: 0.9618 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9740\n",
      "Epoch 00482: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5314 - op_main_loss: 0.1110 - op_conv_loss: 0.0680 - avg_loss: 0.0849 - op_main_accuracy: 0.9615 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9738 - val_loss: 1.5516 - val_op_main_loss: 0.3749 - val_op_conv_loss: 0.5191 - val_avg_loss: 0.3902 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8829 - val_avg_accuracy: 0.8810\n",
      "Epoch 483/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5253 - op_main_loss: 0.1102 - op_conv_loss: 0.0649 - avg_loss: 0.0824 - op_main_accuracy: 0.9638 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9759\n",
      "Epoch 00483: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5253 - op_main_loss: 0.1102 - op_conv_loss: 0.0649 - avg_loss: 0.0824 - op_main_accuracy: 0.9638 - op_conv_accuracy: 0.9747 - avg_accuracy: 0.9759 - val_loss: 1.5165 - val_op_main_loss: 0.3630 - val_op_conv_loss: 0.5048 - val_avg_loss: 0.3808 - val_op_main_accuracy: 0.8867 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8810\n",
      "Epoch 484/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5302 - op_main_loss: 0.1127 - op_conv_loss: 0.0650 - avg_loss: 0.0844 - op_main_accuracy: 0.9601 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9724\n",
      "Epoch 00484: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5310 - op_main_loss: 0.1133 - op_conv_loss: 0.0650 - avg_loss: 0.0847 - op_main_accuracy: 0.9596 - op_conv_accuracy: 0.9724 - avg_accuracy: 0.9721 - val_loss: 1.4356 - val_op_main_loss: 0.3459 - val_op_conv_loss: 0.4667 - val_avg_loss: 0.3552 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8905\n",
      "Epoch 485/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5187 - op_main_loss: 0.1064 - op_conv_loss: 0.0641 - avg_loss: 0.0808 - op_main_accuracy: 0.9685 - op_conv_accuracy: 0.9758 - avg_accuracy: 0.9753\n",
      "Epoch 00485: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5203 - op_main_loss: 0.1074 - op_conv_loss: 0.0642 - avg_loss: 0.0813 - op_main_accuracy: 0.9679 - op_conv_accuracy: 0.9759 - avg_accuracy: 0.9747 - val_loss: 1.5047 - val_op_main_loss: 0.3488 - val_op_conv_loss: 0.5178 - val_avg_loss: 0.3710 - val_op_main_accuracy: 0.8839 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8810\n",
      "Epoch 486/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/133 [============================>.] - ETA: 0s - loss: 0.5638 - op_main_loss: 0.1222 - op_conv_loss: 0.0804 - avg_loss: 0.0948 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9680 - avg_accuracy: 0.9678\n",
      "Epoch 00486: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5648 - op_main_loss: 0.1227 - op_conv_loss: 0.0804 - avg_loss: 0.0951 - op_main_accuracy: 0.9577 - op_conv_accuracy: 0.9681 - avg_accuracy: 0.9679 - val_loss: 1.5378 - val_op_main_loss: 0.3805 - val_op_conv_loss: 0.4999 - val_avg_loss: 0.3907 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8839 - val_avg_accuracy: 0.8857\n",
      "Epoch 487/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5298 - op_main_loss: 0.1094 - op_conv_loss: 0.0688 - avg_loss: 0.0848 - op_main_accuracy: 0.9661 - op_conv_accuracy: 0.9745 - avg_accuracy: 0.9754\n",
      "Epoch 00487: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5324 - op_main_loss: 0.1101 - op_conv_loss: 0.0698 - avg_loss: 0.0857 - op_main_accuracy: 0.9655 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9750 - val_loss: 1.4544 - val_op_main_loss: 0.3420 - val_op_conv_loss: 0.4896 - val_avg_loss: 0.3561 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8867 - val_avg_accuracy: 0.8895\n",
      "Epoch 488/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5238 - op_main_loss: 0.1078 - op_conv_loss: 0.0663 - avg_loss: 0.0830 - op_main_accuracy: 0.9647 - op_conv_accuracy: 0.9706 - avg_accuracy: 0.9709\n",
      "Epoch 00488: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5235 - op_main_loss: 0.1076 - op_conv_loss: 0.0662 - avg_loss: 0.0829 - op_main_accuracy: 0.9648 - op_conv_accuracy: 0.9707 - avg_accuracy: 0.9709 - val_loss: 2.1115 - val_op_main_loss: 0.4489 - val_op_conv_loss: 0.8772 - val_avg_loss: 0.5184 - val_op_main_accuracy: 0.8669 - val_op_conv_accuracy: 0.8234 - val_avg_accuracy: 0.8414\n",
      "Epoch 489/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5521 - op_main_loss: 0.1165 - op_conv_loss: 0.0780 - avg_loss: 0.0912 - op_main_accuracy: 0.9574 - op_conv_accuracy: 0.9718 - avg_accuracy: 0.9716\n",
      "Epoch 00489: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5517 - op_main_loss: 0.1164 - op_conv_loss: 0.0779 - avg_loss: 0.0911 - op_main_accuracy: 0.9575 - op_conv_accuracy: 0.9719 - avg_accuracy: 0.9716 - val_loss: 1.4377 - val_op_main_loss: 0.3398 - val_op_conv_loss: 0.4752 - val_avg_loss: 0.3569 - val_op_main_accuracy: 0.8810 - val_op_conv_accuracy: 0.8801 - val_avg_accuracy: 0.8867\n",
      "Epoch 490/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5401 - op_main_loss: 0.1148 - op_conv_loss: 0.0712 - avg_loss: 0.0881 - op_main_accuracy: 0.9652 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9738\n",
      "Epoch 00490: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5399 - op_main_loss: 0.1146 - op_conv_loss: 0.0711 - avg_loss: 0.0880 - op_main_accuracy: 0.9655 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9740 - val_loss: 1.5743 - val_op_main_loss: 0.4291 - val_op_conv_loss: 0.4699 - val_avg_loss: 0.4090 - val_op_main_accuracy: 0.8725 - val_op_conv_accuracy: 0.8886 - val_avg_accuracy: 0.8810\n",
      "Epoch 491/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5329 - op_main_loss: 0.1122 - op_conv_loss: 0.0689 - avg_loss: 0.0852 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9742\n",
      "Epoch 00491: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5325 - op_main_loss: 0.1120 - op_conv_loss: 0.0688 - avg_loss: 0.0850 - op_main_accuracy: 0.9643 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9742 - val_loss: 1.4694 - val_op_main_loss: 0.3560 - val_op_conv_loss: 0.4811 - val_avg_loss: 0.3657 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8924\n",
      "Epoch 492/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5290 - op_main_loss: 0.1131 - op_conv_loss: 0.0652 - avg_loss: 0.0843 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9731\n",
      "Epoch 00492: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5290 - op_main_loss: 0.1131 - op_conv_loss: 0.0652 - avg_loss: 0.0843 - op_main_accuracy: 0.9627 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9731 - val_loss: 1.8677 - val_op_main_loss: 0.4498 - val_op_conv_loss: 0.6719 - val_avg_loss: 0.4801 - val_op_main_accuracy: 0.8697 - val_op_conv_accuracy: 0.8574 - val_avg_accuracy: 0.8621\n",
      "Epoch 493/500\n",
      "132/133 [============================>.] - ETA: 0s - loss: 0.5138 - op_main_loss: 0.1090 - op_conv_loss: 0.0598 - avg_loss: 0.0790 - op_main_accuracy: 0.9619 - op_conv_accuracy: 0.9796 - avg_accuracy: 0.9763\n",
      "Epoch 00493: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5135 - op_main_loss: 0.1089 - op_conv_loss: 0.0597 - avg_loss: 0.0789 - op_main_accuracy: 0.9620 - op_conv_accuracy: 0.9797 - avg_accuracy: 0.9764 - val_loss: 1.4984 - val_op_main_loss: 0.3650 - val_op_conv_loss: 0.4908 - val_avg_loss: 0.3768 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8857 - val_avg_accuracy: 0.8886\n",
      "Epoch 494/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5413 - op_main_loss: 0.1177 - op_conv_loss: 0.0704 - avg_loss: 0.0868 - op_main_accuracy: 0.9593 - op_conv_accuracy: 0.9758 - avg_accuracy: 0.9750\n",
      "Epoch 00494: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5464 - op_main_loss: 0.1201 - op_conv_loss: 0.0715 - avg_loss: 0.0884 - op_main_accuracy: 0.9582 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9742 - val_loss: 1.6499 - val_op_main_loss: 0.4143 - val_op_conv_loss: 0.5439 - val_avg_loss: 0.4252 - val_op_main_accuracy: 0.8754 - val_op_conv_accuracy: 0.8763 - val_avg_accuracy: 0.8754\n",
      "Epoch 495/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.5507 - op_main_loss: 0.1196 - op_conv_loss: 0.0735 - avg_loss: 0.0908 - op_main_accuracy: 0.9591 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9719\n",
      "Epoch 00495: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5504 - op_main_loss: 0.1196 - op_conv_loss: 0.0732 - avg_loss: 0.0908 - op_main_accuracy: 0.9594 - op_conv_accuracy: 0.9742 - avg_accuracy: 0.9724 - val_loss: 1.4121 - val_op_main_loss: 0.3322 - val_op_conv_loss: 0.4703 - val_avg_loss: 0.3427 - val_op_main_accuracy: 0.8905 - val_op_conv_accuracy: 0.8876 - val_avg_accuracy: 0.8905\n",
      "Epoch 496/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5422 - op_main_loss: 0.1128 - op_conv_loss: 0.0745 - avg_loss: 0.0886 - op_main_accuracy: 0.9622 - op_conv_accuracy: 0.9726 - avg_accuracy: 0.9719\n",
      "Epoch 00496: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5413 - op_main_loss: 0.1126 - op_conv_loss: 0.0741 - avg_loss: 0.0883 - op_main_accuracy: 0.9624 - op_conv_accuracy: 0.9728 - avg_accuracy: 0.9721 - val_loss: 1.3779 - val_op_main_loss: 0.3290 - val_op_conv_loss: 0.4432 - val_avg_loss: 0.3398 - val_op_main_accuracy: 0.8895 - val_op_conv_accuracy: 0.8924 - val_avg_accuracy: 0.8905\n",
      "Epoch 497/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5305 - op_main_loss: 0.1112 - op_conv_loss: 0.0691 - avg_loss: 0.0843 - op_main_accuracy: 0.9666 - op_conv_accuracy: 0.9754 - avg_accuracy: 0.9733\n",
      "Epoch 00497: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5328 - op_main_loss: 0.1120 - op_conv_loss: 0.0698 - avg_loss: 0.0851 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9752 - avg_accuracy: 0.9731 - val_loss: 1.6736 - val_op_main_loss: 0.3987 - val_op_conv_loss: 0.5839 - val_avg_loss: 0.4253 - val_op_main_accuracy: 0.8735 - val_op_conv_accuracy: 0.8650 - val_avg_accuracy: 0.8697\n",
      "Epoch 498/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/133 [============================>.] - ETA: 0s - loss: 0.5274 - op_main_loss: 0.1081 - op_conv_loss: 0.0698 - avg_loss: 0.0835 - op_main_accuracy: 0.9658 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9734\n",
      "Epoch 00498: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5288 - op_main_loss: 0.1085 - op_conv_loss: 0.0703 - avg_loss: 0.0840 - op_main_accuracy: 0.9662 - op_conv_accuracy: 0.9738 - avg_accuracy: 0.9731 - val_loss: 1.4535 - val_op_main_loss: 0.3496 - val_op_conv_loss: 0.4752 - val_avg_loss: 0.3630 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8848 - val_avg_accuracy: 0.8924\n",
      "Epoch 499/500\n",
      "129/133 [============================>.] - ETA: 0s - loss: 0.5109 - op_main_loss: 0.1064 - op_conv_loss: 0.0599 - avg_loss: 0.0787 - op_main_accuracy: 0.9671 - op_conv_accuracy: 0.9753 - avg_accuracy: 0.9750\n",
      "Epoch 00499: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5119 - op_main_loss: 0.1074 - op_conv_loss: 0.0596 - avg_loss: 0.0790 - op_main_accuracy: 0.9667 - op_conv_accuracy: 0.9757 - avg_accuracy: 0.9754 - val_loss: 1.5212 - val_op_main_loss: 0.3625 - val_op_conv_loss: 0.5120 - val_avg_loss: 0.3809 - val_op_main_accuracy: 0.8848 - val_op_conv_accuracy: 0.8791 - val_avg_accuracy: 0.8839\n",
      "Epoch 500/500\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.5346 - op_main_loss: 0.1161 - op_conv_loss: 0.0664 - avg_loss: 0.0861 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9728\n",
      "Epoch 00500: val_avg_accuracy did not improve from 0.90179\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5346 - op_main_loss: 0.1161 - op_conv_loss: 0.0664 - avg_loss: 0.0861 - op_main_accuracy: 0.9612 - op_conv_accuracy: 0.9740 - avg_accuracy: 0.9728 - val_loss: 1.5186 - val_op_main_loss: 0.3571 - val_op_conv_loss: 0.5176 - val_avg_loss: 0.3780 - val_op_main_accuracy: 0.8857 - val_op_conv_accuracy: 0.8820 - val_avg_accuracy: 0.8839\n"
     ]
    }
   ],
   "source": [
    "def ms_lstm(w2v):\n",
    "    inputs = Input(shape=(X_train[0].shape[-1],))\n",
    "\n",
    "    embedding_layer = gensim_to_keras_embedding(w2v)\n",
    "    \n",
    "    embedding = embedding_layer(inputs)\n",
    "\n",
    "    lstm1 = LSTM(lstm_units,return_sequences=True, return_state=True, kernel_regularizer=l2(w_decay),recurrent_regularizer=l2(w_decay), dropout=dropout_rate)(embedding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = Dense(units=1, activation='sigmoid', name='op_main')(lstm1[1])\n",
    "    \n",
    "\n",
    "    output_td_gap = GlobalAveragePooling1D(data_format='channels_first')(lstm1[0])\n",
    "    \n",
    "    output_td = TimeDistributed(Dense(units=1, activation='sigmoid'))(lstm1[0])\n",
    "    output_td = Flatten()(output_td)\n",
    "    \n",
    "    output_td = Multiply()([output_td_gap, output_td])\n",
    "    \n",
    "    output_td = Activation('relu', name='before_split')(output_td)\n",
    "    \n",
    "    output_td_splits = tf.split(output_td, 10, axis=-1)\n",
    "    \n",
    "    features = concatenate([output_td_splits[0], output_td_splits[1], output_td_splits[-2], output_td_splits[-1]])\n",
    "    \n",
    "    print(features.shape)\n",
    "    \n",
    "    output_td = Reshape((8, 10, 1))(features)\n",
    "    \n",
    "    output_td = Conv2D(2, 8, padding='same', strides=1, activation='relu', kernel_regularizer=l2(w_decay))(output_td)\n",
    "    output_td = BatchNormalization()(output_td)\n",
    "    output_td = Flatten()(output_td)\n",
    "   \n",
    "\n",
    "    output_td = Dense(units=1, activation='sigmoid', name='op_conv')(output_td)\n",
    "    \n",
    "    \n",
    "    \n",
    "    avg = tf.keras.layers.Average(name='avg')([output, output_td])\n",
    "    \n",
    "\n",
    "    model = Model(inputs, [output, output_td, avg])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = ms_lstm(w2v_model)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('./weight_cp/weight_lstm2.hdf5', save_freq=\"epoch\",  verbose=1, monitor='val_avg_accuracy', save_best_only=True,\n",
    "    save_weights_only=False)\n",
    "\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(0.0001)\n",
    "model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "model.summary()\n",
    "history1 = model.fit(X_train, y_train, epochs=epochs_to_run, validation_data=(X_val, y_val), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f4c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF Multi-Supervised LSTM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       661\n",
      "           1       0.92      0.87      0.89       662\n",
      "\n",
      "    accuracy                           0.90      1323\n",
      "   macro avg       0.90      0.90      0.90      1323\n",
      "weighted avg       0.90      0.90      0.90      1323\n",
      "\n",
      "0.8972033257747544\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./weight_cp/weight_lstm2.hdf5')\n",
    "predictionss = model.predict(X_test)\n",
    "predictions = np.where(predictionss[-1] > 0.5, 1, 0)\n",
    "y_pred = []\n",
    "for p in predictions:\n",
    "    y_pred.append(p[0])\n",
    "y_pred = np.array(y_pred)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"CLASSIFICATION REPORT OF Multi-Supervised LSTM\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebe85b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 80)\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 100)     14114800    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200, 50), (N 30200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 200, 1)       51          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 200)          0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 200)          0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 200)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "before_split (Activation)       (None, 200)          0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_1 (TensorFlow [(None, 20), (None,  0           before_split[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 80)           0           tf_op_layer_split_1[0][0]        \n",
      "                                                                 tf_op_layer_split_1[0][1]        \n",
      "                                                                 tf_op_layer_split_1[0][8]        \n",
      "                                                                 tf_op_layer_split_1[0][9]        \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 8, 10, 1)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8, 10, 2)     130         reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8, 10, 2)     8           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 160)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "op_conv (Dense)                 (None, 1)            161         flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,145,350\n",
      "Trainable params: 30,546\n",
      "Non-trainable params: 14,114,804\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 8.8787 - accuracy: 0.4859\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.49575, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 17ms/step - loss: 8.8684 - accuracy: 0.4856 - val_loss: 7.7607 - val_accuracy: 0.4958\n",
      "Epoch 2/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 6.8722 - accuracy: 0.4874\n",
      "Epoch 00002: val_accuracy did not improve from 0.49575\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 6.8643 - accuracy: 0.4875 - val_loss: 6.0169 - val_accuracy: 0.4882\n",
      "Epoch 3/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 5.3345 - accuracy: 0.5010\n",
      "Epoch 00003: val_accuracy improved from 0.49575 to 0.52691, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 5.3285 - accuracy: 0.5005 - val_loss: 4.6809 - val_accuracy: 0.5269\n",
      "Epoch 4/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 4.1597 - accuracy: 0.5017\n",
      "Epoch 00004: val_accuracy did not improve from 0.52691\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 4.1551 - accuracy: 0.5017 - val_loss: 3.6599 - val_accuracy: 0.5212\n",
      "Epoch 5/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 3.2625 - accuracy: 0.5024\n",
      "Epoch 00005: val_accuracy improved from 0.52691 to 0.52880, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 3.2591 - accuracy: 0.5019 - val_loss: 2.8826 - val_accuracy: 0.5288\n",
      "Epoch 6/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 2.5810 - accuracy: 0.5236\n",
      "Epoch 00006: val_accuracy improved from 0.52880 to 0.53447, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 2.5784 - accuracy: 0.5241 - val_loss: 2.2946 - val_accuracy: 0.5345\n",
      "Epoch 7/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 2.0683 - accuracy: 0.5310\n",
      "Epoch 00007: val_accuracy improved from 0.53447 to 0.54769, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 2.0663 - accuracy: 0.5302 - val_loss: 1.8535 - val_accuracy: 0.5477\n",
      "Epoch 8/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.6840 - accuracy: 0.5479\n",
      "Epoch 00008: val_accuracy improved from 0.54769 to 0.55146, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.6826 - accuracy: 0.5463 - val_loss: 1.5263 - val_accuracy: 0.5515\n",
      "Epoch 9/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.3985 - accuracy: 0.5728\n",
      "Epoch 00009: val_accuracy improved from 0.55146 to 0.60151, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.3974 - accuracy: 0.5733 - val_loss: 1.2795 - val_accuracy: 0.6015\n",
      "Epoch 10/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.1837 - accuracy: 0.6195\n",
      "Epoch 00010: val_accuracy improved from 0.60151 to 0.66195, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.1832 - accuracy: 0.6174 - val_loss: 1.0908 - val_accuracy: 0.6619\n",
      "Epoch 11/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 1.0155 - accuracy: 0.6763\n",
      "Epoch 00011: val_accuracy improved from 0.66195 to 0.69877, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 1.0151 - accuracy: 0.6756 - val_loss: 0.9412 - val_accuracy: 0.6988\n",
      "Epoch 12/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.8820 - accuracy: 0.7071\n",
      "Epoch 00012: val_accuracy improved from 0.69877 to 0.71010, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.8811 - accuracy: 0.7084 - val_loss: 0.8259 - val_accuracy: 0.7101\n",
      "Epoch 13/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.7859 - accuracy: 0.7278\n",
      "Epoch 00013: val_accuracy improved from 0.71010 to 0.75354, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.7855 - accuracy: 0.7278 - val_loss: 0.7446 - val_accuracy: 0.7535\n",
      "Epoch 14/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.7246 - accuracy: 0.7364\n",
      "Epoch 00014: val_accuracy did not improve from 0.75354\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.7238 - accuracy: 0.7372 - val_loss: 0.7104 - val_accuracy: 0.7167\n",
      "Epoch 15/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6696 - accuracy: 0.7581\n",
      "Epoch 00015: val_accuracy improved from 0.75354 to 0.76771, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6692 - accuracy: 0.7585 - val_loss: 0.6513 - val_accuracy: 0.7677\n",
      "Epoch 16/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6406 - accuracy: 0.7648\n",
      "Epoch 00016: val_accuracy did not improve from 0.76771\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.6395 - accuracy: 0.7656 - val_loss: 0.6392 - val_accuracy: 0.7583\n",
      "Epoch 17/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.6214 - accuracy: 0.7615\n",
      "Epoch 00017: val_accuracy improved from 0.76771 to 0.77432, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.6216 - accuracy: 0.7611 - val_loss: 0.6032 - val_accuracy: 0.7743\n",
      "Epoch 18/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5922 - accuracy: 0.7715\n",
      "Epoch 00018: val_accuracy did not improve from 0.77432\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5924 - accuracy: 0.7715 - val_loss: 0.5928 - val_accuracy: 0.7545\n",
      "Epoch 19/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5850 - accuracy: 0.7801\n",
      "Epoch 00019: val_accuracy improved from 0.77432 to 0.77998, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5855 - accuracy: 0.7791 - val_loss: 0.5834 - val_accuracy: 0.7800\n",
      "Epoch 20/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5718 - accuracy: 0.7798\n",
      "Epoch 00020: val_accuracy did not improve from 0.77998\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5714 - accuracy: 0.7795 - val_loss: 0.5695 - val_accuracy: 0.7781\n",
      "Epoch 21/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5587 - accuracy: 0.7829\n",
      "Epoch 00021: val_accuracy did not improve from 0.77998\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5596 - accuracy: 0.7817 - val_loss: 0.5846 - val_accuracy: 0.7592\n",
      "Epoch 22/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5469 - accuracy: 0.7903\n",
      "Epoch 00022: val_accuracy did not improve from 0.77998\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5469 - accuracy: 0.7904 - val_loss: 0.5749 - val_accuracy: 0.7715\n",
      "Epoch 23/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5465 - accuracy: 0.7863\n",
      "Epoch 00023: val_accuracy improved from 0.77998 to 0.79792, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5455 - accuracy: 0.7873 - val_loss: 0.5355 - val_accuracy: 0.7979\n",
      "Epoch 24/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5344 - accuracy: 0.7948\n",
      "Epoch 00024: val_accuracy improved from 0.79792 to 0.80642, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5353 - accuracy: 0.7937 - val_loss: 0.5299 - val_accuracy: 0.8064\n",
      "Epoch 25/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7922\n",
      "Epoch 00025: val_accuracy did not improve from 0.80642\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5345 - accuracy: 0.7923 - val_loss: 0.5369 - val_accuracy: 0.7970\n",
      "Epoch 26/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5224 - accuracy: 0.7996\n",
      "Epoch 00026: val_accuracy did not improve from 0.80642\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5224 - accuracy: 0.7999 - val_loss: 0.5193 - val_accuracy: 0.8026\n",
      "Epoch 27/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5194 - accuracy: 0.8025\n",
      "Epoch 00027: val_accuracy did not improve from 0.80642\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5206 - accuracy: 0.8017 - val_loss: 0.5184 - val_accuracy: 0.7894\n",
      "Epoch 28/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5192 - accuracy: 0.7941\n",
      "Epoch 00028: val_accuracy did not improve from 0.80642\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5185 - accuracy: 0.7944 - val_loss: 0.5155 - val_accuracy: 0.7989\n",
      "Epoch 29/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5104 - accuracy: 0.8080\n",
      "Epoch 00029: val_accuracy did not improve from 0.80642\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5100 - accuracy: 0.8084 - val_loss: 0.5214 - val_accuracy: 0.8036\n",
      "Epoch 30/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5062 - accuracy: 0.8108\n",
      "Epoch 00030: val_accuracy did not improve from 0.80642\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.5060 - accuracy: 0.8119 - val_loss: 0.6018 - val_accuracy: 0.7460\n",
      "Epoch 31/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.5097 - accuracy: 0.8092\n",
      "Epoch 00031: val_accuracy improved from 0.80642 to 0.81775, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.5087 - accuracy: 0.8098 - val_loss: 0.4974 - val_accuracy: 0.8178\n",
      "Epoch 32/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4951 - accuracy: 0.8206\n",
      "Epoch 00032: val_accuracy did not improve from 0.81775\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4954 - accuracy: 0.8202 - val_loss: 0.5109 - val_accuracy: 0.8064\n",
      "Epoch 33/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4941 - accuracy: 0.8168\n",
      "Epoch 00033: val_accuracy did not improve from 0.81775\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4944 - accuracy: 0.8164 - val_loss: 0.5091 - val_accuracy: 0.8130\n",
      "Epoch 34/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4881 - accuracy: 0.8225\n",
      "Epoch 00034: val_accuracy did not improve from 0.81775\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4885 - accuracy: 0.8225 - val_loss: 0.5250 - val_accuracy: 0.8036\n",
      "Epoch 35/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4844 - accuracy: 0.8316\n",
      "Epoch 00035: val_accuracy did not improve from 0.81775\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4846 - accuracy: 0.8315 - val_loss: 0.5029 - val_accuracy: 0.8121\n",
      "Epoch 36/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4780 - accuracy: 0.8285\n",
      "Epoch 00036: val_accuracy did not improve from 0.81775\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4769 - accuracy: 0.8296 - val_loss: 0.4987 - val_accuracy: 0.8055\n",
      "Epoch 37/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4722 - accuracy: 0.8318\n",
      "Epoch 00037: val_accuracy improved from 0.81775 to 0.81870, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4722 - accuracy: 0.8318 - val_loss: 0.5028 - val_accuracy: 0.8187\n",
      "Epoch 38/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4729 - accuracy: 0.8316\n",
      "Epoch 00038: val_accuracy improved from 0.81870 to 0.83381, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4725 - accuracy: 0.8315 - val_loss: 0.4839 - val_accuracy: 0.8338\n",
      "Epoch 39/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4729 - accuracy: 0.8285\n",
      "Epoch 00039: val_accuracy did not improve from 0.83381\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4724 - accuracy: 0.8284 - val_loss: 0.4817 - val_accuracy: 0.8178\n",
      "Epoch 40/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4701 - accuracy: 0.8323\n",
      "Epoch 00040: val_accuracy did not improve from 0.83381\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4697 - accuracy: 0.8325 - val_loss: 0.5060 - val_accuracy: 0.8102\n",
      "Epoch 41/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.4590 - accuracy: 0.8440\n",
      "Epoch 00041: val_accuracy improved from 0.83381 to 0.83664, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4592 - accuracy: 0.8440 - val_loss: 0.4816 - val_accuracy: 0.8366\n",
      "Epoch 42/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4612 - accuracy: 0.8438\n",
      "Epoch 00042: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4621 - accuracy: 0.8429 - val_loss: 0.4848 - val_accuracy: 0.8083\n",
      "Epoch 43/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4538 - accuracy: 0.8449\n",
      "Epoch 00043: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4535 - accuracy: 0.8450 - val_loss: 0.5308 - val_accuracy: 0.7951\n",
      "Epoch 44/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4573 - accuracy: 0.8402\n",
      "Epoch 00044: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4581 - accuracy: 0.8393 - val_loss: 0.4645 - val_accuracy: 0.8357\n",
      "Epoch 45/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4520 - accuracy: 0.8457\n",
      "Epoch 00045: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4539 - accuracy: 0.8445 - val_loss: 0.5167 - val_accuracy: 0.8083\n",
      "Epoch 46/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4531 - accuracy: 0.8426\n",
      "Epoch 00046: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4527 - accuracy: 0.8424 - val_loss: 0.4727 - val_accuracy: 0.8329\n",
      "Epoch 47/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4445 - accuracy: 0.8466\n",
      "Epoch 00047: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4454 - accuracy: 0.8455 - val_loss: 0.4837 - val_accuracy: 0.8196\n",
      "Epoch 48/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4418 - accuracy: 0.8471\n",
      "Epoch 00048: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4423 - accuracy: 0.8469 - val_loss: 0.5582 - val_accuracy: 0.7970\n",
      "Epoch 49/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4386 - accuracy: 0.8521\n",
      "Epoch 00049: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4390 - accuracy: 0.8518 - val_loss: 0.4802 - val_accuracy: 0.8319\n",
      "Epoch 50/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4372 - accuracy: 0.8562\n",
      "Epoch 00050: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4377 - accuracy: 0.8559 - val_loss: 0.4945 - val_accuracy: 0.8093\n",
      "Epoch 51/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8521\n",
      "Epoch 00051: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4359 - accuracy: 0.8518 - val_loss: 0.4647 - val_accuracy: 0.8291\n",
      "Epoch 52/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4275 - accuracy: 0.8545\n",
      "Epoch 00052: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4273 - accuracy: 0.8549 - val_loss: 0.4621 - val_accuracy: 0.8272\n",
      "Epoch 53/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4293 - accuracy: 0.8566\n",
      "Epoch 00053: val_accuracy did not improve from 0.83664\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4295 - accuracy: 0.8563 - val_loss: 0.5668 - val_accuracy: 0.7866\n",
      "Epoch 54/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4286 - accuracy: 0.8569\n",
      "Epoch 00054: val_accuracy improved from 0.83664 to 0.84042, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4291 - accuracy: 0.8563 - val_loss: 0.4643 - val_accuracy: 0.8404\n",
      "Epoch 55/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4297 - accuracy: 0.8559\n",
      "Epoch 00055: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4308 - accuracy: 0.8556 - val_loss: 0.4636 - val_accuracy: 0.8291\n",
      "Epoch 56/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4250 - accuracy: 0.8619\n",
      "Epoch 00056: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4244 - accuracy: 0.8622 - val_loss: 0.5012 - val_accuracy: 0.8253\n",
      "Epoch 57/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4322 - accuracy: 0.8545\n",
      "Epoch 00057: val_accuracy did not improve from 0.84042\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4317 - accuracy: 0.8547 - val_loss: 0.4749 - val_accuracy: 0.8347\n",
      "Epoch 58/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4234 - accuracy: 0.8578\n",
      "Epoch 00058: val_accuracy improved from 0.84042 to 0.84891, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4229 - accuracy: 0.8585 - val_loss: 0.4434 - val_accuracy: 0.8489\n",
      "Epoch 59/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4127 - accuracy: 0.8709\n",
      "Epoch 00059: val_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4123 - accuracy: 0.8712 - val_loss: 0.4559 - val_accuracy: 0.8281\n",
      "Epoch 60/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4175 - accuracy: 0.8695\n",
      "Epoch 00060: val_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4170 - accuracy: 0.8700 - val_loss: 0.4935 - val_accuracy: 0.8263\n",
      "Epoch 61/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4120 - accuracy: 0.8678\n",
      "Epoch 00061: val_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4126 - accuracy: 0.8674 - val_loss: 0.4628 - val_accuracy: 0.8376\n",
      "Epoch 62/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4083 - accuracy: 0.8695\n",
      "Epoch 00062: val_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4081 - accuracy: 0.8696 - val_loss: 0.4684 - val_accuracy: 0.8329\n",
      "Epoch 63/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4171 - accuracy: 0.8566\n",
      "Epoch 00063: val_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4166 - accuracy: 0.8573 - val_loss: 0.4682 - val_accuracy: 0.8376\n",
      "Epoch 64/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4025 - accuracy: 0.8667\n",
      "Epoch 00064: val_accuracy did not improve from 0.84891\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4049 - accuracy: 0.8653 - val_loss: 0.4453 - val_accuracy: 0.8404\n",
      "Epoch 65/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4108 - accuracy: 0.8633\n",
      "Epoch 00065: val_accuracy improved from 0.84891 to 0.84986, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4112 - accuracy: 0.8634 - val_loss: 0.4538 - val_accuracy: 0.8499\n",
      "Epoch 66/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4070 - accuracy: 0.8709\n",
      "Epoch 00066: val_accuracy improved from 0.84986 to 0.85175, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.4067 - accuracy: 0.8710 - val_loss: 0.4369 - val_accuracy: 0.8517\n",
      "Epoch 67/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4032 - accuracy: 0.8745\n",
      "Epoch 00067: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4045 - accuracy: 0.8738 - val_loss: 0.4377 - val_accuracy: 0.8517\n",
      "Epoch 68/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4088 - accuracy: 0.8652\n",
      "Epoch 00068: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4079 - accuracy: 0.8655 - val_loss: 0.4489 - val_accuracy: 0.8366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4042 - accuracy: 0.8740\n",
      "Epoch 00069: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4032 - accuracy: 0.8748 - val_loss: 0.4582 - val_accuracy: 0.8291\n",
      "Epoch 70/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4020 - accuracy: 0.8731\n",
      "Epoch 00070: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4018 - accuracy: 0.8731 - val_loss: 0.4552 - val_accuracy: 0.8300\n",
      "Epoch 71/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3911 - accuracy: 0.8805\n",
      "Epoch 00071: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3919 - accuracy: 0.8797 - val_loss: 0.4670 - val_accuracy: 0.8423\n",
      "Epoch 72/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3868 - accuracy: 0.8774\n",
      "Epoch 00072: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3865 - accuracy: 0.8776 - val_loss: 0.7105 - val_accuracy: 0.7328\n",
      "Epoch 73/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.4024 - accuracy: 0.8731\n",
      "Epoch 00073: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.4036 - accuracy: 0.8729 - val_loss: 0.4365 - val_accuracy: 0.8508\n",
      "Epoch 74/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3982 - accuracy: 0.8721\n",
      "Epoch 00074: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3992 - accuracy: 0.8712 - val_loss: 0.5753 - val_accuracy: 0.8036\n",
      "Epoch 75/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3854 - accuracy: 0.8857\n",
      "Epoch 00075: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3858 - accuracy: 0.8859 - val_loss: 0.5443 - val_accuracy: 0.8074\n",
      "Epoch 76/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3886 - accuracy: 0.8788\n",
      "Epoch 00076: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3892 - accuracy: 0.8781 - val_loss: 0.6066 - val_accuracy: 0.7951\n",
      "Epoch 77/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3829 - accuracy: 0.8810\n",
      "Epoch 00077: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3832 - accuracy: 0.8807 - val_loss: 0.6028 - val_accuracy: 0.7828\n",
      "Epoch 78/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3963 - accuracy: 0.8767\n",
      "Epoch 00078: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3966 - accuracy: 0.8764 - val_loss: 0.4609 - val_accuracy: 0.8234\n",
      "Epoch 79/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3825 - accuracy: 0.8826\n",
      "Epoch 00079: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3822 - accuracy: 0.8828 - val_loss: 0.4789 - val_accuracy: 0.8376\n",
      "Epoch 80/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3808 - accuracy: 0.8836\n",
      "Epoch 00080: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3801 - accuracy: 0.8840 - val_loss: 0.4409 - val_accuracy: 0.8480\n",
      "Epoch 81/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3795 - accuracy: 0.8826\n",
      "Epoch 00081: val_accuracy did not improve from 0.85175\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3797 - accuracy: 0.8826 - val_loss: 0.4499 - val_accuracy: 0.8329\n",
      "Epoch 82/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3796 - accuracy: 0.8867\n",
      "Epoch 00082: val_accuracy improved from 0.85175 to 0.85364, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3798 - accuracy: 0.8861 - val_loss: 0.4441 - val_accuracy: 0.8536\n",
      "Epoch 83/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3876 - accuracy: 0.8798\n",
      "Epoch 00083: val_accuracy did not improve from 0.85364\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3873 - accuracy: 0.8797 - val_loss: 0.7190 - val_accuracy: 0.7573\n",
      "Epoch 84/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3710 - accuracy: 0.8905\n",
      "Epoch 00084: val_accuracy improved from 0.85364 to 0.85930, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3717 - accuracy: 0.8899 - val_loss: 0.4400 - val_accuracy: 0.8593\n",
      "Epoch 85/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3717 - accuracy: 0.8862\n",
      "Epoch 00085: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3723 - accuracy: 0.8863 - val_loss: 0.7716 - val_accuracy: 0.7290\n",
      "Epoch 86/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3766 - accuracy: 0.8831\n",
      "Epoch 00086: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3772 - accuracy: 0.8828 - val_loss: 0.4661 - val_accuracy: 0.8329\n",
      "Epoch 87/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3813 - accuracy: 0.8807\n",
      "Epoch 00087: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3803 - accuracy: 0.8814 - val_loss: 0.5423 - val_accuracy: 0.8121\n",
      "Epoch 88/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3751 - accuracy: 0.8886\n",
      "Epoch 00088: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3767 - accuracy: 0.8873 - val_loss: 0.4596 - val_accuracy: 0.8461\n",
      "Epoch 89/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3676 - accuracy: 0.8898\n",
      "Epoch 00089: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3679 - accuracy: 0.8901 - val_loss: 0.5075 - val_accuracy: 0.8300\n",
      "Epoch 90/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3732 - accuracy: 0.8874\n",
      "Epoch 00090: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3729 - accuracy: 0.8873 - val_loss: 0.4934 - val_accuracy: 0.8376\n",
      "Epoch 91/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3835 - accuracy: 0.8836\n",
      "Epoch 00091: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3836 - accuracy: 0.8837 - val_loss: 0.4398 - val_accuracy: 0.8451\n",
      "Epoch 92/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3701 - accuracy: 0.8910\n",
      "Epoch 00092: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3702 - accuracy: 0.8908 - val_loss: 0.6302 - val_accuracy: 0.7856\n",
      "Epoch 93/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3713 - accuracy: 0.8938\n",
      "Epoch 00093: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3716 - accuracy: 0.8937 - val_loss: 0.5615 - val_accuracy: 0.7677\n",
      "Epoch 94/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3720 - accuracy: 0.8876\n",
      "Epoch 00094: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3727 - accuracy: 0.8875 - val_loss: 0.4901 - val_accuracy: 0.8187\n",
      "Epoch 95/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3655 - accuracy: 0.8929\n",
      "Epoch 00095: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3653 - accuracy: 0.8934 - val_loss: 0.4729 - val_accuracy: 0.8206\n",
      "Epoch 96/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3631 - accuracy: 0.8967\n",
      "Epoch 00096: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.3627 - accuracy: 0.8970 - val_loss: 0.4741 - val_accuracy: 0.8451\n",
      "Epoch 97/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3783 - accuracy: 0.8860\n",
      "Epoch 00097: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3780 - accuracy: 0.8863 - val_loss: 0.4657 - val_accuracy: 0.8461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3536 - accuracy: 0.9029\n",
      "Epoch 00098: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3537 - accuracy: 0.9029 - val_loss: 0.5188 - val_accuracy: 0.8026\n",
      "Epoch 99/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3737 - accuracy: 0.8872\n",
      "Epoch 00099: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3741 - accuracy: 0.8868 - val_loss: 0.4827 - val_accuracy: 0.8451\n",
      "Epoch 100/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3585 - accuracy: 0.8934\n",
      "Epoch 00100: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3585 - accuracy: 0.8932 - val_loss: 0.4383 - val_accuracy: 0.8489\n",
      "Epoch 101/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3512 - accuracy: 0.9022\n",
      "Epoch 00101: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3516 - accuracy: 0.9017 - val_loss: 0.5525 - val_accuracy: 0.8272\n",
      "Epoch 102/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3575 - accuracy: 0.9008\n",
      "Epoch 00102: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3576 - accuracy: 0.9005 - val_loss: 0.4566 - val_accuracy: 0.8508\n",
      "Epoch 103/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3484 - accuracy: 0.9005\n",
      "Epoch 00103: val_accuracy did not improve from 0.85930\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3490 - accuracy: 0.9003 - val_loss: 0.6141 - val_accuracy: 0.7960\n",
      "Epoch 104/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3580 - accuracy: 0.8938\n",
      "Epoch 00104: val_accuracy improved from 0.85930 to 0.86402, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3572 - accuracy: 0.8941 - val_loss: 0.4201 - val_accuracy: 0.8640\n",
      "Epoch 105/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3604 - accuracy: 0.8950\n",
      "Epoch 00105: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3592 - accuracy: 0.8956 - val_loss: 0.4552 - val_accuracy: 0.8366\n",
      "Epoch 106/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3551 - accuracy: 0.8929\n",
      "Epoch 00106: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3549 - accuracy: 0.8932 - val_loss: 0.5250 - val_accuracy: 0.8347\n",
      "Epoch 107/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3496 - accuracy: 0.9036\n",
      "Epoch 00107: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3497 - accuracy: 0.9038 - val_loss: 0.4300 - val_accuracy: 0.8593\n",
      "Epoch 108/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3549 - accuracy: 0.8946\n",
      "Epoch 00108: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3559 - accuracy: 0.8941 - val_loss: 0.4675 - val_accuracy: 0.8527\n",
      "Epoch 109/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3518 - accuracy: 0.8986\n",
      "Epoch 00109: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3524 - accuracy: 0.8982 - val_loss: 0.5739 - val_accuracy: 0.8083\n",
      "Epoch 110/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3520 - accuracy: 0.8977\n",
      "Epoch 00110: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3523 - accuracy: 0.8972 - val_loss: 0.5390 - val_accuracy: 0.8206\n",
      "Epoch 111/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3476 - accuracy: 0.9010\n",
      "Epoch 00111: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3484 - accuracy: 0.9010 - val_loss: 0.4474 - val_accuracy: 0.8499\n",
      "Epoch 112/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3614 - accuracy: 0.8946\n",
      "Epoch 00112: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3619 - accuracy: 0.8941 - val_loss: 0.6396 - val_accuracy: 0.7941\n",
      "Epoch 113/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.9029\n",
      "Epoch 00113: val_accuracy did not improve from 0.86402\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3433 - accuracy: 0.9034 - val_loss: 0.4434 - val_accuracy: 0.8565\n",
      "Epoch 114/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3413 - accuracy: 0.9058\n",
      "Epoch 00114: val_accuracy improved from 0.86402 to 0.86591, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3411 - accuracy: 0.9057 - val_loss: 0.4292 - val_accuracy: 0.8659\n",
      "Epoch 115/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.9055\n",
      "Epoch 00115: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3406 - accuracy: 0.9052 - val_loss: 0.5026 - val_accuracy: 0.8385\n",
      "Epoch 116/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3388 - accuracy: 0.9058\n",
      "Epoch 00116: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3390 - accuracy: 0.9052 - val_loss: 0.4217 - val_accuracy: 0.8621\n",
      "Epoch 117/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3456 - accuracy: 0.9031\n",
      "Epoch 00117: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3461 - accuracy: 0.9029 - val_loss: 0.4284 - val_accuracy: 0.8480\n",
      "Epoch 118/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3431 - accuracy: 0.9034\n",
      "Epoch 00118: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3427 - accuracy: 0.9038 - val_loss: 0.5013 - val_accuracy: 0.8385\n",
      "Epoch 119/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3434 - accuracy: 0.9029\n",
      "Epoch 00119: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3431 - accuracy: 0.9031 - val_loss: 0.9050 - val_accuracy: 0.6780\n",
      "Epoch 120/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3525 - accuracy: 0.8960\n",
      "Epoch 00120: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3521 - accuracy: 0.8963 - val_loss: 0.5149 - val_accuracy: 0.8414\n",
      "Epoch 121/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3298 - accuracy: 0.9108\n",
      "Epoch 00121: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3313 - accuracy: 0.9104 - val_loss: 0.4653 - val_accuracy: 0.8347\n",
      "Epoch 122/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3421 - accuracy: 0.9060\n",
      "Epoch 00122: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3424 - accuracy: 0.9060 - val_loss: 0.4288 - val_accuracy: 0.8574\n",
      "Epoch 123/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.9067\n",
      "Epoch 00123: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3369 - accuracy: 0.9062 - val_loss: 0.4619 - val_accuracy: 0.8508\n",
      "Epoch 124/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.9074\n",
      "Epoch 00124: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3384 - accuracy: 0.9081 - val_loss: 0.4315 - val_accuracy: 0.8659\n",
      "Epoch 125/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3334 - accuracy: 0.9022\n",
      "Epoch 00125: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3332 - accuracy: 0.9024 - val_loss: 0.4254 - val_accuracy: 0.8631\n",
      "Epoch 126/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/133 [============================>.] - ETA: 0s - loss: 0.3324 - accuracy: 0.9065\n",
      "Epoch 00126: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3322 - accuracy: 0.9064 - val_loss: 0.4366 - val_accuracy: 0.8621\n",
      "Epoch 127/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3414 - accuracy: 0.9024\n",
      "Epoch 00127: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3421 - accuracy: 0.9019 - val_loss: 0.8463 - val_accuracy: 0.7318\n",
      "Epoch 128/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.9113\n",
      "Epoch 00128: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3298 - accuracy: 0.9114 - val_loss: 0.4329 - val_accuracy: 0.8565\n",
      "Epoch 129/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3264 - accuracy: 0.9144\n",
      "Epoch 00129: val_accuracy did not improve from 0.86591\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3258 - accuracy: 0.9147 - val_loss: 0.6544 - val_accuracy: 0.7450\n",
      "Epoch 130/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3365 - accuracy: 0.9079\n",
      "Epoch 00130: val_accuracy improved from 0.86591 to 0.87063, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.3370 - accuracy: 0.9074 - val_loss: 0.4169 - val_accuracy: 0.8706\n",
      "Epoch 131/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3379 - accuracy: 0.9079\n",
      "Epoch 00131: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3371 - accuracy: 0.9083 - val_loss: 0.4260 - val_accuracy: 0.8697\n",
      "Epoch 132/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3253 - accuracy: 0.9115\n",
      "Epoch 00132: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3262 - accuracy: 0.9112 - val_loss: 0.4756 - val_accuracy: 0.8499\n",
      "Epoch 133/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.9043\n",
      "Epoch 00133: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3338 - accuracy: 0.9048 - val_loss: 0.4472 - val_accuracy: 0.8555\n",
      "Epoch 134/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3237 - accuracy: 0.9172\n",
      "Epoch 00134: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3243 - accuracy: 0.9168 - val_loss: 0.4339 - val_accuracy: 0.8451\n",
      "Epoch 135/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3340 - accuracy: 0.9051\n",
      "Epoch 00135: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3358 - accuracy: 0.9041 - val_loss: 0.6543 - val_accuracy: 0.7932\n",
      "Epoch 136/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3400 - accuracy: 0.9008\n",
      "Epoch 00136: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3395 - accuracy: 0.9010 - val_loss: 0.6487 - val_accuracy: 0.8026\n",
      "Epoch 137/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.9039\n",
      "Epoch 00137: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3335 - accuracy: 0.9043 - val_loss: 0.6381 - val_accuracy: 0.8102\n",
      "Epoch 138/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3361 - accuracy: 0.9051\n",
      "Epoch 00138: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3362 - accuracy: 0.9045 - val_loss: 0.5241 - val_accuracy: 0.8338\n",
      "Epoch 139/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.9129\n",
      "Epoch 00139: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3246 - accuracy: 0.9128 - val_loss: 0.4410 - val_accuracy: 0.8442\n",
      "Epoch 140/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3290 - accuracy: 0.9113\n",
      "Epoch 00140: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3284 - accuracy: 0.9116 - val_loss: 0.4742 - val_accuracy: 0.8253\n",
      "Epoch 141/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.9170\n",
      "Epoch 00141: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3237 - accuracy: 0.9164 - val_loss: 0.4360 - val_accuracy: 0.8650\n",
      "Epoch 142/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3182 - accuracy: 0.9182\n",
      "Epoch 00142: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3181 - accuracy: 0.9182 - val_loss: 0.5069 - val_accuracy: 0.8470\n",
      "Epoch 143/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.9070\n",
      "Epoch 00143: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3354 - accuracy: 0.9064 - val_loss: 0.5313 - val_accuracy: 0.8329\n",
      "Epoch 144/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.9120\n",
      "Epoch 00144: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3227 - accuracy: 0.9119 - val_loss: 0.4773 - val_accuracy: 0.8404\n",
      "Epoch 145/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3183 - accuracy: 0.9182\n",
      "Epoch 00145: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3174 - accuracy: 0.9185 - val_loss: 0.5628 - val_accuracy: 0.8395\n",
      "Epoch 146/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.9136\n",
      "Epoch 00146: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3218 - accuracy: 0.9140 - val_loss: 0.8925 - val_accuracy: 0.6789\n",
      "Epoch 147/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3258 - accuracy: 0.9105\n",
      "Epoch 00147: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3256 - accuracy: 0.9107 - val_loss: 0.4709 - val_accuracy: 0.8480\n",
      "Epoch 148/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3218 - accuracy: 0.9082\n",
      "Epoch 00148: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3222 - accuracy: 0.9081 - val_loss: 0.4813 - val_accuracy: 0.8499\n",
      "Epoch 149/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.9127\n",
      "Epoch 00149: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3191 - accuracy: 0.9123 - val_loss: 0.8041 - val_accuracy: 0.7441\n",
      "Epoch 150/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3295 - accuracy: 0.9115\n",
      "Epoch 00150: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3295 - accuracy: 0.9116 - val_loss: 0.7856 - val_accuracy: 0.7573\n",
      "Epoch 151/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3217 - accuracy: 0.9113\n",
      "Epoch 00151: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3221 - accuracy: 0.9109 - val_loss: 0.4285 - val_accuracy: 0.8584\n",
      "Epoch 152/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3098 - accuracy: 0.9206\n",
      "Epoch 00152: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3110 - accuracy: 0.9204 - val_loss: 0.4847 - val_accuracy: 0.8480\n",
      "Epoch 153/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3191 - accuracy: 0.9103\n",
      "Epoch 00153: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3190 - accuracy: 0.9104 - val_loss: 0.4254 - val_accuracy: 0.8640\n",
      "Epoch 154/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3246 - accuracy: 0.9156\n",
      "Epoch 00154: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3256 - accuracy: 0.9154 - val_loss: 0.6851 - val_accuracy: 0.8017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.9122\n",
      "Epoch 00155: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3205 - accuracy: 0.9119 - val_loss: 0.4536 - val_accuracy: 0.8451\n",
      "Epoch 156/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3105 - accuracy: 0.9225\n",
      "Epoch 00156: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3122 - accuracy: 0.9213 - val_loss: 0.4571 - val_accuracy: 0.8584\n",
      "Epoch 157/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3286 - accuracy: 0.9086\n",
      "Epoch 00157: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3279 - accuracy: 0.9090 - val_loss: 0.4850 - val_accuracy: 0.8225\n",
      "Epoch 158/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3308 - accuracy: 0.9055\n",
      "Epoch 00158: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3303 - accuracy: 0.9057 - val_loss: 0.4927 - val_accuracy: 0.8423\n",
      "Epoch 159/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.9151\n",
      "Epoch 00159: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3179 - accuracy: 0.9154 - val_loss: 0.5387 - val_accuracy: 0.8017\n",
      "Epoch 160/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.9206\n",
      "Epoch 00160: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3077 - accuracy: 0.9211 - val_loss: 0.4528 - val_accuracy: 0.8593\n",
      "Epoch 161/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3169 - accuracy: 0.9196\n",
      "Epoch 00161: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3178 - accuracy: 0.9192 - val_loss: 0.4119 - val_accuracy: 0.8687\n",
      "Epoch 162/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3048 - accuracy: 0.9213\n",
      "Epoch 00162: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3050 - accuracy: 0.9211 - val_loss: 0.4486 - val_accuracy: 0.8640\n",
      "Epoch 163/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3132 - accuracy: 0.9172\n",
      "Epoch 00163: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3131 - accuracy: 0.9173 - val_loss: 0.5168 - val_accuracy: 0.8159\n",
      "Epoch 164/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3087 - accuracy: 0.9275\n",
      "Epoch 00164: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3081 - accuracy: 0.9279 - val_loss: 0.5060 - val_accuracy: 0.8451\n",
      "Epoch 165/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3184 - accuracy: 0.9144\n",
      "Epoch 00165: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3173 - accuracy: 0.9152 - val_loss: 0.4937 - val_accuracy: 0.8461\n",
      "Epoch 166/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3125 - accuracy: 0.9203\n",
      "Epoch 00166: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3122 - accuracy: 0.9208 - val_loss: 0.5425 - val_accuracy: 0.8263\n",
      "Epoch 167/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3357 - accuracy: 0.9029\n",
      "Epoch 00167: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3350 - accuracy: 0.9034 - val_loss: 0.4862 - val_accuracy: 0.8281\n",
      "Epoch 168/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3069 - accuracy: 0.9189\n",
      "Epoch 00168: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3067 - accuracy: 0.9190 - val_loss: 0.4651 - val_accuracy: 0.8347\n",
      "Epoch 169/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3113 - accuracy: 0.9165\n",
      "Epoch 00169: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3113 - accuracy: 0.9161 - val_loss: 0.4424 - val_accuracy: 0.8480\n",
      "Epoch 170/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.9227\n",
      "Epoch 00170: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3063 - accuracy: 0.9232 - val_loss: 0.4208 - val_accuracy: 0.8706\n",
      "Epoch 171/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3095 - accuracy: 0.9187\n",
      "Epoch 00171: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3104 - accuracy: 0.9182 - val_loss: 0.4949 - val_accuracy: 0.8432\n",
      "Epoch 172/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3049 - accuracy: 0.9227\n",
      "Epoch 00172: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3044 - accuracy: 0.9230 - val_loss: 0.6209 - val_accuracy: 0.8102\n",
      "Epoch 173/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.9182\n",
      "Epoch 00173: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3069 - accuracy: 0.9180 - val_loss: 0.9719 - val_accuracy: 0.7252\n",
      "Epoch 174/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.9237\n",
      "Epoch 00174: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2994 - accuracy: 0.9239 - val_loss: 0.4369 - val_accuracy: 0.8697\n",
      "Epoch 175/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.9258\n",
      "Epoch 00175: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2984 - accuracy: 0.9258 - val_loss: 0.4684 - val_accuracy: 0.8555\n",
      "Epoch 176/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.9210\n",
      "Epoch 00176: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3126 - accuracy: 0.9211 - val_loss: 0.4324 - val_accuracy: 0.8687\n",
      "Epoch 177/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3108 - accuracy: 0.9198\n",
      "Epoch 00177: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3107 - accuracy: 0.9197 - val_loss: 0.5973 - val_accuracy: 0.8215\n",
      "Epoch 178/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2978 - accuracy: 0.9258\n",
      "Epoch 00178: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2988 - accuracy: 0.9253 - val_loss: 0.6993 - val_accuracy: 0.7932\n",
      "Epoch 179/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3032 - accuracy: 0.9253\n",
      "Epoch 00179: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3038 - accuracy: 0.9244 - val_loss: 0.4496 - val_accuracy: 0.8678\n",
      "Epoch 180/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.9220\n",
      "Epoch 00180: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2997 - accuracy: 0.9220 - val_loss: 0.7335 - val_accuracy: 0.7583\n",
      "Epoch 181/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.9165\n",
      "Epoch 00181: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3098 - accuracy: 0.9161 - val_loss: 1.2547 - val_accuracy: 0.6534\n",
      "Epoch 182/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.9227\n",
      "Epoch 00182: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3045 - accuracy: 0.9218 - val_loss: 0.4530 - val_accuracy: 0.8659\n",
      "Epoch 183/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.9172\n",
      "Epoch 00183: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3039 - accuracy: 0.9175 - val_loss: 0.5348 - val_accuracy: 0.8083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3070 - accuracy: 0.9196\n",
      "Epoch 00184: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3070 - accuracy: 0.9199 - val_loss: 0.5018 - val_accuracy: 0.8281\n",
      "Epoch 185/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.9265\n",
      "Epoch 00185: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2950 - accuracy: 0.9270 - val_loss: 0.8012 - val_accuracy: 0.7309\n",
      "Epoch 186/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2990 - accuracy: 0.9239\n",
      "Epoch 00186: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2989 - accuracy: 0.9237 - val_loss: 0.4451 - val_accuracy: 0.8621\n",
      "Epoch 187/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3015 - accuracy: 0.9272\n",
      "Epoch 00187: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3013 - accuracy: 0.9272 - val_loss: 0.4376 - val_accuracy: 0.8517\n",
      "Epoch 188/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.9251\n",
      "Epoch 00188: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3019 - accuracy: 0.9249 - val_loss: 0.4826 - val_accuracy: 0.8319\n",
      "Epoch 189/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2967 - accuracy: 0.9241\n",
      "Epoch 00189: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2971 - accuracy: 0.9246 - val_loss: 0.7567 - val_accuracy: 0.7885\n",
      "Epoch 190/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2985 - accuracy: 0.9263\n",
      "Epoch 00190: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2976 - accuracy: 0.9265 - val_loss: 0.4384 - val_accuracy: 0.8621\n",
      "Epoch 191/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2954 - accuracy: 0.9306\n",
      "Epoch 00191: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2958 - accuracy: 0.9303 - val_loss: 0.9876 - val_accuracy: 0.7290\n",
      "Epoch 192/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2932 - accuracy: 0.9280\n",
      "Epoch 00192: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2929 - accuracy: 0.9282 - val_loss: 0.8650 - val_accuracy: 0.7290\n",
      "Epoch 193/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2923 - accuracy: 0.9284\n",
      "Epoch 00193: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2921 - accuracy: 0.9286 - val_loss: 0.5500 - val_accuracy: 0.8395\n",
      "Epoch 194/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2981 - accuracy: 0.9246\n",
      "Epoch 00194: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2981 - accuracy: 0.9246 - val_loss: 0.5317 - val_accuracy: 0.8451\n",
      "Epoch 195/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.9234\n",
      "Epoch 00195: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2966 - accuracy: 0.9230 - val_loss: 0.6170 - val_accuracy: 0.8130\n",
      "Epoch 196/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.9227\n",
      "Epoch 00196: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3039 - accuracy: 0.9227 - val_loss: 0.4677 - val_accuracy: 0.8442\n",
      "Epoch 197/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.9256\n",
      "Epoch 00197: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3016 - accuracy: 0.9256 - val_loss: 0.4771 - val_accuracy: 0.8574\n",
      "Epoch 198/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.9234\n",
      "Epoch 00198: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2942 - accuracy: 0.9239 - val_loss: 0.5863 - val_accuracy: 0.8310\n",
      "Epoch 199/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.9275\n",
      "Epoch 00199: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3000 - accuracy: 0.9275 - val_loss: 0.4611 - val_accuracy: 0.8527\n",
      "Epoch 200/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2952 - accuracy: 0.9277\n",
      "Epoch 00200: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2959 - accuracy: 0.9270 - val_loss: 0.4798 - val_accuracy: 0.8565\n",
      "Epoch 201/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9282\n",
      "Epoch 00201: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2919 - accuracy: 0.9284 - val_loss: 0.4426 - val_accuracy: 0.8584\n",
      "Epoch 202/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3024 - accuracy: 0.9232\n",
      "Epoch 00202: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3015 - accuracy: 0.9237 - val_loss: 0.9245 - val_accuracy: 0.7365\n",
      "Epoch 203/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3059 - accuracy: 0.9220\n",
      "Epoch 00203: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3062 - accuracy: 0.9223 - val_loss: 0.5833 - val_accuracy: 0.7941\n",
      "Epoch 204/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.9220\n",
      "Epoch 00204: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.3046 - accuracy: 0.9225 - val_loss: 0.4516 - val_accuracy: 0.8687\n",
      "Epoch 205/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.9330\n",
      "Epoch 00205: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2887 - accuracy: 0.9322 - val_loss: 0.4816 - val_accuracy: 0.8499\n",
      "Epoch 206/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2943 - accuracy: 0.9270\n",
      "Epoch 00206: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2932 - accuracy: 0.9277 - val_loss: 0.9851 - val_accuracy: 0.7450\n",
      "Epoch 207/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.9351\n",
      "Epoch 00207: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2837 - accuracy: 0.9348 - val_loss: 0.8097 - val_accuracy: 0.7630\n",
      "Epoch 208/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2891 - accuracy: 0.9289\n",
      "Epoch 00208: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2902 - accuracy: 0.9277 - val_loss: 0.9759 - val_accuracy: 0.7299\n",
      "Epoch 209/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2840 - accuracy: 0.9368\n",
      "Epoch 00209: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2838 - accuracy: 0.9369 - val_loss: 0.4426 - val_accuracy: 0.8574\n",
      "Epoch 210/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.9263\n",
      "Epoch 00210: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2942 - accuracy: 0.9265 - val_loss: 1.5847 - val_accuracy: 0.6166\n",
      "Epoch 211/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2879 - accuracy: 0.9280\n",
      "Epoch 00211: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2882 - accuracy: 0.9279 - val_loss: 0.5180 - val_accuracy: 0.8451\n",
      "Epoch 212/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.9268\n",
      "Epoch 00212: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2946 - accuracy: 0.9267 - val_loss: 0.4539 - val_accuracy: 0.8650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.9344\n",
      "Epoch 00213: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2835 - accuracy: 0.9343 - val_loss: 0.5695 - val_accuracy: 0.8338\n",
      "Epoch 214/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.9272\n",
      "Epoch 00214: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2913 - accuracy: 0.9260 - val_loss: 0.6371 - val_accuracy: 0.8102\n",
      "Epoch 215/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9253\n",
      "Epoch 00215: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2970 - accuracy: 0.9253 - val_loss: 0.4934 - val_accuracy: 0.8555\n",
      "Epoch 216/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.9327\n",
      "Epoch 00216: val_accuracy did not improve from 0.87063\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2884 - accuracy: 0.9324 - val_loss: 0.9516 - val_accuracy: 0.7488\n",
      "Epoch 217/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.9303\n",
      "Epoch 00217: val_accuracy improved from 0.87063 to 0.87630, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.2941 - accuracy: 0.9298 - val_loss: 0.4388 - val_accuracy: 0.8763\n",
      "Epoch 218/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2915 - accuracy: 0.9265\n",
      "Epoch 00218: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2912 - accuracy: 0.9270 - val_loss: 0.4873 - val_accuracy: 0.8565\n",
      "Epoch 219/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2853 - accuracy: 0.9339\n",
      "Epoch 00219: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2873 - accuracy: 0.9327 - val_loss: 0.4243 - val_accuracy: 0.8687\n",
      "Epoch 220/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.9301\n",
      "Epoch 00220: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2898 - accuracy: 0.9296 - val_loss: 0.4778 - val_accuracy: 0.8593\n",
      "Epoch 221/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2876 - accuracy: 0.9320\n",
      "Epoch 00221: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2870 - accuracy: 0.9324 - val_loss: 0.4782 - val_accuracy: 0.8593\n",
      "Epoch 222/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2803 - accuracy: 0.9365\n",
      "Epoch 00222: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2804 - accuracy: 0.9364 - val_loss: 0.4745 - val_accuracy: 0.8499\n",
      "Epoch 223/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.9220\n",
      "Epoch 00223: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2957 - accuracy: 0.9225 - val_loss: 0.4301 - val_accuracy: 0.8716\n",
      "Epoch 224/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2712 - accuracy: 0.9380\n",
      "Epoch 00224: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2730 - accuracy: 0.9369 - val_loss: 0.8850 - val_accuracy: 0.7271\n",
      "Epoch 225/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2835 - accuracy: 0.9315\n",
      "Epoch 00225: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2836 - accuracy: 0.9317 - val_loss: 0.8900 - val_accuracy: 0.7602\n",
      "Epoch 226/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2809 - accuracy: 0.9318\n",
      "Epoch 00226: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2819 - accuracy: 0.9315 - val_loss: 0.5387 - val_accuracy: 0.8111\n",
      "Epoch 227/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2801 - accuracy: 0.9327\n",
      "Epoch 00227: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2797 - accuracy: 0.9329 - val_loss: 0.9582 - val_accuracy: 0.7318\n",
      "Epoch 228/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2849 - accuracy: 0.9318\n",
      "Epoch 00228: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2849 - accuracy: 0.9315 - val_loss: 1.4098 - val_accuracy: 0.6412\n",
      "Epoch 229/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.9370\n",
      "Epoch 00229: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2810 - accuracy: 0.9371 - val_loss: 0.6628 - val_accuracy: 0.7790\n",
      "Epoch 230/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.9303\n",
      "Epoch 00230: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2922 - accuracy: 0.9305 - val_loss: 0.6050 - val_accuracy: 0.8319\n",
      "Epoch 231/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.9361\n",
      "Epoch 00231: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2869 - accuracy: 0.9362 - val_loss: 0.9223 - val_accuracy: 0.7224\n",
      "Epoch 232/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.9406\n",
      "Epoch 00232: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2696 - accuracy: 0.9400 - val_loss: 0.7125 - val_accuracy: 0.7970\n",
      "Epoch 233/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.9342\n",
      "Epoch 00233: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2834 - accuracy: 0.9338 - val_loss: 0.4719 - val_accuracy: 0.8517\n",
      "Epoch 234/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2791 - accuracy: 0.9358\n",
      "Epoch 00234: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2793 - accuracy: 0.9355 - val_loss: 0.4464 - val_accuracy: 0.8697\n",
      "Epoch 235/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2692 - accuracy: 0.9406\n",
      "Epoch 00235: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2703 - accuracy: 0.9393 - val_loss: 0.6661 - val_accuracy: 0.7771\n",
      "Epoch 236/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.9287\n",
      "Epoch 00236: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2884 - accuracy: 0.9286 - val_loss: 0.9625 - val_accuracy: 0.7413\n",
      "Epoch 237/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2833 - accuracy: 0.9334\n",
      "Epoch 00237: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2838 - accuracy: 0.9329 - val_loss: 0.4820 - val_accuracy: 0.8461\n",
      "Epoch 238/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2789 - accuracy: 0.9346\n",
      "Epoch 00238: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2789 - accuracy: 0.9343 - val_loss: 0.6896 - val_accuracy: 0.8055\n",
      "Epoch 239/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2795 - accuracy: 0.9365\n",
      "Epoch 00239: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2794 - accuracy: 0.9367 - val_loss: 0.6263 - val_accuracy: 0.8206\n",
      "Epoch 240/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2759 - accuracy: 0.9344\n",
      "Epoch 00240: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2758 - accuracy: 0.9343 - val_loss: 0.6193 - val_accuracy: 0.7838\n",
      "Epoch 241/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9289\n",
      "Epoch 00241: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2898 - accuracy: 0.9286 - val_loss: 0.4769 - val_accuracy: 0.8640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2707 - accuracy: 0.9404\n",
      "Epoch 00242: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2704 - accuracy: 0.9405 - val_loss: 0.6134 - val_accuracy: 0.8253\n",
      "Epoch 243/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2745 - accuracy: 0.9358\n",
      "Epoch 00243: val_accuracy did not improve from 0.87630\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2742 - accuracy: 0.9360 - val_loss: 0.5824 - val_accuracy: 0.8414\n",
      "Epoch 244/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2661 - accuracy: 0.9394\n",
      "Epoch 00244: val_accuracy improved from 0.87630 to 0.87724, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 15ms/step - loss: 0.2656 - accuracy: 0.9397 - val_loss: 0.4409 - val_accuracy: 0.8772\n",
      "Epoch 245/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2782 - accuracy: 0.9327\n",
      "Epoch 00245: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2785 - accuracy: 0.9324 - val_loss: 0.6868 - val_accuracy: 0.7696\n",
      "Epoch 246/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2796 - accuracy: 0.9346\n",
      "Epoch 00246: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2791 - accuracy: 0.9350 - val_loss: 0.7456 - val_accuracy: 0.8008\n",
      "Epoch 247/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2799 - accuracy: 0.9361\n",
      "Epoch 00247: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2816 - accuracy: 0.9355 - val_loss: 0.7111 - val_accuracy: 0.8149\n",
      "Epoch 248/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.9368\n",
      "Epoch 00248: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2785 - accuracy: 0.9371 - val_loss: 0.8607 - val_accuracy: 0.7856\n",
      "Epoch 249/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2720 - accuracy: 0.9385\n",
      "Epoch 00249: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2727 - accuracy: 0.9383 - val_loss: 0.9400 - val_accuracy: 0.7328\n",
      "Epoch 250/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2697 - accuracy: 0.9370\n",
      "Epoch 00250: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2698 - accuracy: 0.9374 - val_loss: 0.6139 - val_accuracy: 0.8366\n",
      "Epoch 251/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2811 - accuracy: 0.9318\n",
      "Epoch 00251: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2824 - accuracy: 0.9308 - val_loss: 0.4705 - val_accuracy: 0.8659\n",
      "Epoch 252/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.9399\n",
      "Epoch 00252: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2636 - accuracy: 0.9402 - val_loss: 0.5300 - val_accuracy: 0.8225\n",
      "Epoch 253/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2747 - accuracy: 0.9301\n",
      "Epoch 00253: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2741 - accuracy: 0.9305 - val_loss: 0.4585 - val_accuracy: 0.8650\n",
      "Epoch 254/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2916 - accuracy: 0.9303\n",
      "Epoch 00254: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2917 - accuracy: 0.9305 - val_loss: 0.4732 - val_accuracy: 0.8432\n",
      "Epoch 255/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2692 - accuracy: 0.9392\n",
      "Epoch 00255: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2695 - accuracy: 0.9388 - val_loss: 0.5576 - val_accuracy: 0.8470\n",
      "Epoch 256/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.9420\n",
      "Epoch 00256: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2671 - accuracy: 0.9421 - val_loss: 0.5860 - val_accuracy: 0.8404\n",
      "Epoch 257/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2723 - accuracy: 0.9346\n",
      "Epoch 00257: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2729 - accuracy: 0.9345 - val_loss: 0.4364 - val_accuracy: 0.8669\n",
      "Epoch 258/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2742 - accuracy: 0.9368\n",
      "Epoch 00258: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2750 - accuracy: 0.9362 - val_loss: 0.6291 - val_accuracy: 0.8281\n",
      "Epoch 259/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2701 - accuracy: 0.9332\n",
      "Epoch 00259: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2701 - accuracy: 0.9331 - val_loss: 0.4392 - val_accuracy: 0.8706\n",
      "Epoch 260/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2739 - accuracy: 0.9346\n",
      "Epoch 00260: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2755 - accuracy: 0.9341 - val_loss: 1.1896 - val_accuracy: 0.6950\n",
      "Epoch 261/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2721 - accuracy: 0.9351\n",
      "Epoch 00261: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2716 - accuracy: 0.9353 - val_loss: 0.7675 - val_accuracy: 0.7904\n",
      "Epoch 262/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2717 - accuracy: 0.9370\n",
      "Epoch 00262: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2716 - accuracy: 0.9371 - val_loss: 0.4624 - val_accuracy: 0.8744\n",
      "Epoch 263/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2802 - accuracy: 0.9327\n",
      "Epoch 00263: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2795 - accuracy: 0.9331 - val_loss: 0.4815 - val_accuracy: 0.8659\n",
      "Epoch 264/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2704 - accuracy: 0.9404\n",
      "Epoch 00264: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2716 - accuracy: 0.9400 - val_loss: 0.4787 - val_accuracy: 0.8555\n",
      "Epoch 265/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.9354\n",
      "Epoch 00265: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2769 - accuracy: 0.9353 - val_loss: 1.0051 - val_accuracy: 0.7469\n",
      "Epoch 266/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2686 - accuracy: 0.9427\n",
      "Epoch 00266: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2687 - accuracy: 0.9423 - val_loss: 0.6541 - val_accuracy: 0.8263\n",
      "Epoch 267/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2662 - accuracy: 0.9389\n",
      "Epoch 00267: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2671 - accuracy: 0.9386 - val_loss: 0.5883 - val_accuracy: 0.8026\n",
      "Epoch 268/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2628 - accuracy: 0.9423\n",
      "Epoch 00268: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2638 - accuracy: 0.9414 - val_loss: 0.5646 - val_accuracy: 0.8055\n",
      "Epoch 269/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.9418\n",
      "Epoch 00269: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2654 - accuracy: 0.9412 - val_loss: 0.7485 - val_accuracy: 0.7998\n",
      "Epoch 270/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.9427\n",
      "Epoch 00270: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2660 - accuracy: 0.9428 - val_loss: 0.4772 - val_accuracy: 0.8687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2655 - accuracy: 0.9377\n",
      "Epoch 00271: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2662 - accuracy: 0.9374 - val_loss: 0.4616 - val_accuracy: 0.8546\n",
      "Epoch 272/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2648 - accuracy: 0.9408\n",
      "Epoch 00272: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2649 - accuracy: 0.9409 - val_loss: 0.6439 - val_accuracy: 0.8225\n",
      "Epoch 273/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2566 - accuracy: 0.9416\n",
      "Epoch 00273: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2576 - accuracy: 0.9414 - val_loss: 0.5783 - val_accuracy: 0.8517\n",
      "Epoch 274/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2775 - accuracy: 0.9330\n",
      "Epoch 00274: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2774 - accuracy: 0.9331 - val_loss: 0.6106 - val_accuracy: 0.8036\n",
      "Epoch 275/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2634 - accuracy: 0.9430\n",
      "Epoch 00275: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2648 - accuracy: 0.9426 - val_loss: 0.4388 - val_accuracy: 0.8725\n",
      "Epoch 276/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2548 - accuracy: 0.9456\n",
      "Epoch 00276: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2543 - accuracy: 0.9459 - val_loss: 0.5221 - val_accuracy: 0.8517\n",
      "Epoch 277/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2724 - accuracy: 0.9354\n",
      "Epoch 00277: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2717 - accuracy: 0.9357 - val_loss: 1.3081 - val_accuracy: 0.6686\n",
      "Epoch 278/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.9330\n",
      "Epoch 00278: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2762 - accuracy: 0.9331 - val_loss: 0.5182 - val_accuracy: 0.8612\n",
      "Epoch 279/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2644 - accuracy: 0.9425\n",
      "Epoch 00279: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2653 - accuracy: 0.9421 - val_loss: 0.4650 - val_accuracy: 0.8527\n",
      "Epoch 280/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2672 - accuracy: 0.9392\n",
      "Epoch 00280: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2666 - accuracy: 0.9393 - val_loss: 0.5246 - val_accuracy: 0.8536\n",
      "Epoch 281/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2719 - accuracy: 0.9375\n",
      "Epoch 00281: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2724 - accuracy: 0.9371 - val_loss: 0.4343 - val_accuracy: 0.8716\n",
      "Epoch 282/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2731 - accuracy: 0.9375\n",
      "Epoch 00282: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2725 - accuracy: 0.9374 - val_loss: 0.4636 - val_accuracy: 0.8621\n",
      "Epoch 283/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2532 - accuracy: 0.9449\n",
      "Epoch 00283: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2532 - accuracy: 0.9447 - val_loss: 0.4464 - val_accuracy: 0.8706\n",
      "Epoch 284/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2839 - accuracy: 0.9332\n",
      "Epoch 00284: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2835 - accuracy: 0.9336 - val_loss: 0.8951 - val_accuracy: 0.7941\n",
      "Epoch 285/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2603 - accuracy: 0.9416\n",
      "Epoch 00285: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2600 - accuracy: 0.9419 - val_loss: 0.5593 - val_accuracy: 0.8168\n",
      "Epoch 286/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.9413\n",
      "Epoch 00286: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2569 - accuracy: 0.9416 - val_loss: 0.5161 - val_accuracy: 0.8395\n",
      "Epoch 287/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.9418\n",
      "Epoch 00287: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2589 - accuracy: 0.9419 - val_loss: 0.5085 - val_accuracy: 0.8602\n",
      "Epoch 288/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2609 - accuracy: 0.9404\n",
      "Epoch 00288: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2607 - accuracy: 0.9405 - val_loss: 0.6827 - val_accuracy: 0.8225\n",
      "Epoch 289/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2688 - accuracy: 0.9349\n",
      "Epoch 00289: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2691 - accuracy: 0.9348 - val_loss: 0.5855 - val_accuracy: 0.8461\n",
      "Epoch 290/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2742 - accuracy: 0.9370\n",
      "Epoch 00290: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2741 - accuracy: 0.9369 - val_loss: 1.9785 - val_accuracy: 0.6062\n",
      "Epoch 291/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2594 - accuracy: 0.9394\n",
      "Epoch 00291: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2599 - accuracy: 0.9393 - val_loss: 0.4585 - val_accuracy: 0.8678\n",
      "Epoch 292/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2589 - accuracy: 0.9401\n",
      "Epoch 00292: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2588 - accuracy: 0.9402 - val_loss: 1.2768 - val_accuracy: 0.6959\n",
      "Epoch 293/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2594 - accuracy: 0.9420\n",
      "Epoch 00293: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2598 - accuracy: 0.9423 - val_loss: 0.6046 - val_accuracy: 0.8008\n",
      "Epoch 294/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2684 - accuracy: 0.9392\n",
      "Epoch 00294: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2703 - accuracy: 0.9388 - val_loss: 0.5283 - val_accuracy: 0.8376\n",
      "Epoch 295/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2863 - accuracy: 0.9315\n",
      "Epoch 00295: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2895 - accuracy: 0.9305 - val_loss: 0.4477 - val_accuracy: 0.8735\n",
      "Epoch 296/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2609 - accuracy: 0.9408\n",
      "Epoch 00296: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2619 - accuracy: 0.9402 - val_loss: 0.4611 - val_accuracy: 0.8687\n",
      "Epoch 297/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2535 - accuracy: 0.9427\n",
      "Epoch 00297: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2541 - accuracy: 0.9426 - val_loss: 0.4456 - val_accuracy: 0.8725\n",
      "Epoch 298/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2555 - accuracy: 0.9458\n",
      "Epoch 00298: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2564 - accuracy: 0.9454 - val_loss: 0.8604 - val_accuracy: 0.7847\n",
      "Epoch 299/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.9451\n",
      "Epoch 00299: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2533 - accuracy: 0.9447 - val_loss: 0.8382 - val_accuracy: 0.7800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2682 - accuracy: 0.9385\n",
      "Epoch 00300: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2692 - accuracy: 0.9383 - val_loss: 1.4643 - val_accuracy: 0.6317\n",
      "Epoch 301/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2654 - accuracy: 0.9389\n",
      "Epoch 00301: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2650 - accuracy: 0.9388 - val_loss: 0.5418 - val_accuracy: 0.8347\n",
      "Epoch 302/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2580 - accuracy: 0.9451\n",
      "Epoch 00302: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2611 - accuracy: 0.9442 - val_loss: 0.7896 - val_accuracy: 0.8121\n",
      "Epoch 303/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2755 - accuracy: 0.9315\n",
      "Epoch 00303: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2758 - accuracy: 0.9312 - val_loss: 0.4638 - val_accuracy: 0.8697\n",
      "Epoch 304/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2683 - accuracy: 0.9411\n",
      "Epoch 00304: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2674 - accuracy: 0.9414 - val_loss: 0.8289 - val_accuracy: 0.7885\n",
      "Epoch 305/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2542 - accuracy: 0.9463\n",
      "Epoch 00305: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2538 - accuracy: 0.9468 - val_loss: 0.9259 - val_accuracy: 0.7535\n",
      "Epoch 306/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9416\n",
      "Epoch 00306: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2612 - accuracy: 0.9409 - val_loss: 0.4872 - val_accuracy: 0.8621\n",
      "Epoch 307/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2664 - accuracy: 0.9411\n",
      "Epoch 00307: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2667 - accuracy: 0.9409 - val_loss: 0.8676 - val_accuracy: 0.7290\n",
      "Epoch 308/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.9442\n",
      "Epoch 00308: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2669 - accuracy: 0.9438 - val_loss: 0.4527 - val_accuracy: 0.8697\n",
      "Epoch 309/500\n",
      "130/133 [============================>.] - ETA: 0s - loss: 0.2511 - accuracy: 0.9486\n",
      "Epoch 00309: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 13ms/step - loss: 0.2522 - accuracy: 0.9483 - val_loss: 0.4974 - val_accuracy: 0.8432\n",
      "Epoch 310/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2562 - accuracy: 0.9416\n",
      "Epoch 00310: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2555 - accuracy: 0.9421 - val_loss: 0.4330 - val_accuracy: 0.8697\n",
      "Epoch 311/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2652 - accuracy: 0.9418\n",
      "Epoch 00311: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2654 - accuracy: 0.9414 - val_loss: 0.5968 - val_accuracy: 0.8432\n",
      "Epoch 312/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2677 - accuracy: 0.9380\n",
      "Epoch 00312: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2674 - accuracy: 0.9383 - val_loss: 0.4645 - val_accuracy: 0.8555\n",
      "Epoch 313/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2569 - accuracy: 0.9418\n",
      "Epoch 00313: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2575 - accuracy: 0.9416 - val_loss: 0.9119 - val_accuracy: 0.7875\n",
      "Epoch 314/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2586 - accuracy: 0.9423\n",
      "Epoch 00314: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2582 - accuracy: 0.9423 - val_loss: 0.4775 - val_accuracy: 0.8470\n",
      "Epoch 315/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2584 - accuracy: 0.9456\n",
      "Epoch 00315: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2585 - accuracy: 0.9454 - val_loss: 0.4612 - val_accuracy: 0.8631\n",
      "Epoch 316/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.9475\n",
      "Epoch 00316: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2552 - accuracy: 0.9475 - val_loss: 0.5099 - val_accuracy: 0.8423\n",
      "Epoch 317/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2496 - accuracy: 0.9470\n",
      "Epoch 00317: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2492 - accuracy: 0.9471 - val_loss: 0.4769 - val_accuracy: 0.8442\n",
      "Epoch 318/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2548 - accuracy: 0.9439\n",
      "Epoch 00318: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2544 - accuracy: 0.9442 - val_loss: 0.5722 - val_accuracy: 0.8423\n",
      "Epoch 319/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.9425\n",
      "Epoch 00319: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2568 - accuracy: 0.9431 - val_loss: 1.7811 - val_accuracy: 0.6289\n",
      "Epoch 320/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2467 - accuracy: 0.9485\n",
      "Epoch 00320: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2487 - accuracy: 0.9475 - val_loss: 0.7300 - val_accuracy: 0.8121\n",
      "Epoch 321/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2576 - accuracy: 0.9437\n",
      "Epoch 00321: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2573 - accuracy: 0.9435 - val_loss: 1.5908 - val_accuracy: 0.6421\n",
      "Epoch 322/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2598 - accuracy: 0.9413\n",
      "Epoch 00322: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2596 - accuracy: 0.9414 - val_loss: 1.1243 - val_accuracy: 0.7252\n",
      "Epoch 323/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2640 - accuracy: 0.9365\n",
      "Epoch 00323: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2636 - accuracy: 0.9371 - val_loss: 0.6305 - val_accuracy: 0.7932\n",
      "Epoch 324/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2606 - accuracy: 0.9387\n",
      "Epoch 00324: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2612 - accuracy: 0.9383 - val_loss: 0.6821 - val_accuracy: 0.8347\n",
      "Epoch 325/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2543 - accuracy: 0.9473\n",
      "Epoch 00325: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2541 - accuracy: 0.9475 - val_loss: 0.9539 - val_accuracy: 0.7658\n",
      "Epoch 326/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.9516\n",
      "Epoch 00326: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2433 - accuracy: 0.9516 - val_loss: 0.6552 - val_accuracy: 0.8300\n",
      "Epoch 327/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.9447\n",
      "Epoch 00327: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2513 - accuracy: 0.9445 - val_loss: 0.4715 - val_accuracy: 0.8678\n",
      "Epoch 328/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2555 - accuracy: 0.9401\n",
      "Epoch 00328: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2550 - accuracy: 0.9407 - val_loss: 0.7713 - val_accuracy: 0.8140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 329/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2458 - accuracy: 0.9516\n",
      "Epoch 00329: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2472 - accuracy: 0.9509 - val_loss: 1.1590 - val_accuracy: 0.7271\n",
      "Epoch 330/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2529 - accuracy: 0.9430\n",
      "Epoch 00330: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2533 - accuracy: 0.9428 - val_loss: 0.5977 - val_accuracy: 0.8329\n",
      "Epoch 331/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9490\n",
      "Epoch 00331: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2436 - accuracy: 0.9492 - val_loss: 0.4630 - val_accuracy: 0.8574\n",
      "Epoch 332/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.9463\n",
      "Epoch 00332: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2488 - accuracy: 0.9464 - val_loss: 1.5131 - val_accuracy: 0.6591\n",
      "Epoch 333/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2531 - accuracy: 0.9449\n",
      "Epoch 00333: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2528 - accuracy: 0.9449 - val_loss: 0.7355 - val_accuracy: 0.8196\n",
      "Epoch 334/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2577 - accuracy: 0.9425\n",
      "Epoch 00334: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2588 - accuracy: 0.9419 - val_loss: 0.6663 - val_accuracy: 0.8395\n",
      "Epoch 335/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2610 - accuracy: 0.9385\n",
      "Epoch 00335: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2608 - accuracy: 0.9386 - val_loss: 0.6834 - val_accuracy: 0.8300\n",
      "Epoch 336/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.9375\n",
      "Epoch 00336: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2660 - accuracy: 0.9379 - val_loss: 0.4533 - val_accuracy: 0.8640\n",
      "Epoch 337/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2404 - accuracy: 0.9521\n",
      "Epoch 00337: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2397 - accuracy: 0.9525 - val_loss: 0.6536 - val_accuracy: 0.8263\n",
      "Epoch 338/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2712 - accuracy: 0.9361\n",
      "Epoch 00338: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2715 - accuracy: 0.9357 - val_loss: 1.1091 - val_accuracy: 0.6799\n",
      "Epoch 339/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2484 - accuracy: 0.9458\n",
      "Epoch 00339: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2480 - accuracy: 0.9461 - val_loss: 0.5231 - val_accuracy: 0.8612\n",
      "Epoch 340/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2507 - accuracy: 0.9432\n",
      "Epoch 00340: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2507 - accuracy: 0.9433 - val_loss: 0.4554 - val_accuracy: 0.8716\n",
      "Epoch 341/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2598 - accuracy: 0.9451\n",
      "Epoch 00341: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2603 - accuracy: 0.9452 - val_loss: 0.5719 - val_accuracy: 0.8470\n",
      "Epoch 342/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.9528\n",
      "Epoch 00342: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2376 - accuracy: 0.9530 - val_loss: 1.0645 - val_accuracy: 0.7290\n",
      "Epoch 343/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2473 - accuracy: 0.9468\n",
      "Epoch 00343: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2468 - accuracy: 0.9468 - val_loss: 2.8147 - val_accuracy: 0.5666\n",
      "Epoch 344/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2510 - accuracy: 0.9473\n",
      "Epoch 00344: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2505 - accuracy: 0.9475 - val_loss: 0.5070 - val_accuracy: 0.8687\n",
      "Epoch 345/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2525 - accuracy: 0.9401\n",
      "Epoch 00345: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2521 - accuracy: 0.9402 - val_loss: 0.6154 - val_accuracy: 0.8083\n",
      "Epoch 346/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9475\n",
      "Epoch 00346: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2470 - accuracy: 0.9473 - val_loss: 0.6330 - val_accuracy: 0.7998\n",
      "Epoch 347/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2525 - accuracy: 0.9439\n",
      "Epoch 00347: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2524 - accuracy: 0.9440 - val_loss: 1.1662 - val_accuracy: 0.7262\n",
      "Epoch 348/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.9501\n",
      "Epoch 00348: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2461 - accuracy: 0.9499 - val_loss: 1.0410 - val_accuracy: 0.7526\n",
      "Epoch 349/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2523 - accuracy: 0.9473\n",
      "Epoch 00349: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2524 - accuracy: 0.9468 - val_loss: 0.7825 - val_accuracy: 0.8036\n",
      "Epoch 350/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2615 - accuracy: 0.9451\n",
      "Epoch 00350: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2612 - accuracy: 0.9449 - val_loss: 0.9041 - val_accuracy: 0.7856\n",
      "Epoch 351/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.9413\n",
      "Epoch 00351: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2565 - accuracy: 0.9412 - val_loss: 1.1599 - val_accuracy: 0.6978\n",
      "Epoch 352/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2540 - accuracy: 0.9442\n",
      "Epoch 00352: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2546 - accuracy: 0.9440 - val_loss: 0.5037 - val_accuracy: 0.8404\n",
      "Epoch 353/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2540 - accuracy: 0.9456\n",
      "Epoch 00353: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2535 - accuracy: 0.9461 - val_loss: 0.5399 - val_accuracy: 0.8536\n",
      "Epoch 354/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2520 - accuracy: 0.9451\n",
      "Epoch 00354: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2519 - accuracy: 0.9449 - val_loss: 0.8400 - val_accuracy: 0.7847\n",
      "Epoch 355/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.9509\n",
      "Epoch 00355: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2441 - accuracy: 0.9511 - val_loss: 0.4635 - val_accuracy: 0.8735\n",
      "Epoch 356/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2412 - accuracy: 0.9468\n",
      "Epoch 00356: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2414 - accuracy: 0.9468 - val_loss: 0.5712 - val_accuracy: 0.8527\n",
      "Epoch 357/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2468 - accuracy: 0.9478\n",
      "Epoch 00357: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2462 - accuracy: 0.9483 - val_loss: 0.4752 - val_accuracy: 0.8659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 358/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2535 - accuracy: 0.9451\n",
      "Epoch 00358: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2526 - accuracy: 0.9457 - val_loss: 1.0680 - val_accuracy: 0.7347\n",
      "Epoch 359/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9509\n",
      "Epoch 00359: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2422 - accuracy: 0.9506 - val_loss: 0.4767 - val_accuracy: 0.8725\n",
      "Epoch 360/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9482\n",
      "Epoch 00360: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2400 - accuracy: 0.9485 - val_loss: 0.4705 - val_accuracy: 0.8763\n",
      "Epoch 361/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2434 - accuracy: 0.9482\n",
      "Epoch 00361: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2430 - accuracy: 0.9487 - val_loss: 0.7620 - val_accuracy: 0.8234\n",
      "Epoch 362/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.9535\n",
      "Epoch 00362: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2356 - accuracy: 0.9532 - val_loss: 0.5613 - val_accuracy: 0.8536\n",
      "Epoch 363/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2595 - accuracy: 0.9444\n",
      "Epoch 00363: val_accuracy did not improve from 0.87724\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2600 - accuracy: 0.9442 - val_loss: 0.6259 - val_accuracy: 0.8404\n",
      "Epoch 364/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2445 - accuracy: 0.9485\n",
      "Epoch 00364: val_accuracy improved from 0.87724 to 0.87819, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.2442 - accuracy: 0.9487 - val_loss: 0.4544 - val_accuracy: 0.8782\n",
      "Epoch 365/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9475\n",
      "Epoch 00365: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2448 - accuracy: 0.9473 - val_loss: 1.2206 - val_accuracy: 0.7177\n",
      "Epoch 366/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2399 - accuracy: 0.9504\n",
      "Epoch 00366: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2396 - accuracy: 0.9506 - val_loss: 0.4481 - val_accuracy: 0.8706\n",
      "Epoch 367/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2525 - accuracy: 0.9451\n",
      "Epoch 00367: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2529 - accuracy: 0.9449 - val_loss: 1.2754 - val_accuracy: 0.7148\n",
      "Epoch 368/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2643 - accuracy: 0.9380\n",
      "Epoch 00368: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2664 - accuracy: 0.9376 - val_loss: 0.5643 - val_accuracy: 0.8546\n",
      "Epoch 369/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2575 - accuracy: 0.9427\n",
      "Epoch 00369: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2592 - accuracy: 0.9426 - val_loss: 0.5881 - val_accuracy: 0.8414\n",
      "Epoch 370/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2494 - accuracy: 0.9461\n",
      "Epoch 00370: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2499 - accuracy: 0.9459 - val_loss: 0.7108 - val_accuracy: 0.8244\n",
      "Epoch 371/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9490\n",
      "Epoch 00371: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2413 - accuracy: 0.9492 - val_loss: 0.6929 - val_accuracy: 0.8291\n",
      "Epoch 372/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2334 - accuracy: 0.9566\n",
      "Epoch 00372: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2333 - accuracy: 0.9565 - val_loss: 0.9210 - val_accuracy: 0.7413\n",
      "Epoch 373/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2375 - accuracy: 0.9494\n",
      "Epoch 00373: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2376 - accuracy: 0.9492 - val_loss: 0.5960 - val_accuracy: 0.8168\n",
      "Epoch 374/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.9516\n",
      "Epoch 00374: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2465 - accuracy: 0.9513 - val_loss: 0.4594 - val_accuracy: 0.8669\n",
      "Epoch 375/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.9547\n",
      "Epoch 00375: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2312 - accuracy: 0.9546 - val_loss: 0.4713 - val_accuracy: 0.8687\n",
      "Epoch 376/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.9468\n",
      "Epoch 00376: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2461 - accuracy: 0.9471 - val_loss: 1.3536 - val_accuracy: 0.6969\n",
      "Epoch 377/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9501\n",
      "Epoch 00377: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2389 - accuracy: 0.9499 - val_loss: 0.4693 - val_accuracy: 0.8754\n",
      "Epoch 378/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2516 - accuracy: 0.9439\n",
      "Epoch 00378: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2513 - accuracy: 0.9442 - val_loss: 0.4695 - val_accuracy: 0.8602\n",
      "Epoch 379/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2473 - accuracy: 0.9490\n",
      "Epoch 00379: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2467 - accuracy: 0.9492 - val_loss: 0.6104 - val_accuracy: 0.8178\n",
      "Epoch 380/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2456 - accuracy: 0.9461\n",
      "Epoch 00380: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2451 - accuracy: 0.9461 - val_loss: 0.9416 - val_accuracy: 0.7507\n",
      "Epoch 381/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2419 - accuracy: 0.9475\n",
      "Epoch 00381: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2413 - accuracy: 0.9475 - val_loss: 0.5324 - val_accuracy: 0.8602\n",
      "Epoch 382/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.9454\n",
      "Epoch 00382: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2466 - accuracy: 0.9454 - val_loss: 0.7180 - val_accuracy: 0.8215\n",
      "Epoch 383/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2457 - accuracy: 0.9463\n",
      "Epoch 00383: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2465 - accuracy: 0.9454 - val_loss: 0.5510 - val_accuracy: 0.8593\n",
      "Epoch 384/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2578 - accuracy: 0.9411\n",
      "Epoch 00384: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2576 - accuracy: 0.9414 - val_loss: 0.7371 - val_accuracy: 0.7819\n",
      "Epoch 385/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2376 - accuracy: 0.9535\n",
      "Epoch 00385: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2377 - accuracy: 0.9530 - val_loss: 0.8104 - val_accuracy: 0.8036\n",
      "Epoch 386/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2413 - accuracy: 0.9478\n",
      "Epoch 00386: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2418 - accuracy: 0.9471 - val_loss: 0.4899 - val_accuracy: 0.8678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 387/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2467 - accuracy: 0.9458\n",
      "Epoch 00387: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2466 - accuracy: 0.9459 - val_loss: 0.7642 - val_accuracy: 0.7838\n",
      "Epoch 388/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2394 - accuracy: 0.9504\n",
      "Epoch 00388: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2389 - accuracy: 0.9506 - val_loss: 0.8276 - val_accuracy: 0.8225\n",
      "Epoch 389/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2482 - accuracy: 0.9473\n",
      "Epoch 00389: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2477 - accuracy: 0.9475 - val_loss: 0.4585 - val_accuracy: 0.8697\n",
      "Epoch 390/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2330 - accuracy: 0.9530\n",
      "Epoch 00390: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2330 - accuracy: 0.9530 - val_loss: 1.0869 - val_accuracy: 0.7488\n",
      "Epoch 391/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2368 - accuracy: 0.9521\n",
      "Epoch 00391: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2363 - accuracy: 0.9525 - val_loss: 0.5807 - val_accuracy: 0.8565\n",
      "Epoch 392/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2353 - accuracy: 0.9513\n",
      "Epoch 00392: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2350 - accuracy: 0.9511 - val_loss: 0.4997 - val_accuracy: 0.8754\n",
      "Epoch 393/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2433 - accuracy: 0.9468\n",
      "Epoch 00393: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2444 - accuracy: 0.9459 - val_loss: 1.1056 - val_accuracy: 0.7337\n",
      "Epoch 394/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2444 - accuracy: 0.9497\n",
      "Epoch 00394: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2446 - accuracy: 0.9494 - val_loss: 0.5803 - val_accuracy: 0.8546\n",
      "Epoch 395/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2316 - accuracy: 0.9509\n",
      "Epoch 00395: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2322 - accuracy: 0.9506 - val_loss: 0.4981 - val_accuracy: 0.8716\n",
      "Epoch 396/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2411 - accuracy: 0.9490\n",
      "Epoch 00396: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2411 - accuracy: 0.9487 - val_loss: 0.5205 - val_accuracy: 0.8376\n",
      "Epoch 397/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2396 - accuracy: 0.9535\n",
      "Epoch 00397: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2401 - accuracy: 0.9527 - val_loss: 1.4244 - val_accuracy: 0.6941\n",
      "Epoch 398/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2392 - accuracy: 0.9492\n",
      "Epoch 00398: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2389 - accuracy: 0.9494 - val_loss: 0.5450 - val_accuracy: 0.8461\n",
      "Epoch 399/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2526 - accuracy: 0.9451\n",
      "Epoch 00399: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2529 - accuracy: 0.9447 - val_loss: 0.4991 - val_accuracy: 0.8565\n",
      "Epoch 400/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2301 - accuracy: 0.9552\n",
      "Epoch 00400: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2306 - accuracy: 0.9551 - val_loss: 0.5046 - val_accuracy: 0.8735\n",
      "Epoch 401/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2572 - accuracy: 0.9430\n",
      "Epoch 00401: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2569 - accuracy: 0.9428 - val_loss: 0.9228 - val_accuracy: 0.7554\n",
      "Epoch 402/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2669 - accuracy: 0.9370\n",
      "Epoch 00402: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2675 - accuracy: 0.9369 - val_loss: 0.9151 - val_accuracy: 0.7866\n",
      "Epoch 403/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.9497\n",
      "Epoch 00403: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2455 - accuracy: 0.9494 - val_loss: 1.7073 - val_accuracy: 0.6280\n",
      "Epoch 404/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2419 - accuracy: 0.9528\n",
      "Epoch 00404: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2417 - accuracy: 0.9527 - val_loss: 0.9271 - val_accuracy: 0.7894\n",
      "Epoch 405/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2508 - accuracy: 0.9480\n",
      "Epoch 00405: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2517 - accuracy: 0.9480 - val_loss: 0.5072 - val_accuracy: 0.8735\n",
      "Epoch 406/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2423 - accuracy: 0.9499\n",
      "Epoch 00406: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2418 - accuracy: 0.9501 - val_loss: 0.4984 - val_accuracy: 0.8678\n",
      "Epoch 407/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2399 - accuracy: 0.9504\n",
      "Epoch 00407: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2392 - accuracy: 0.9509 - val_loss: 0.9423 - val_accuracy: 0.7904\n",
      "Epoch 408/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.9501\n",
      "Epoch 00408: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2322 - accuracy: 0.9504 - val_loss: 1.1059 - val_accuracy: 0.7205\n",
      "Epoch 409/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2459 - accuracy: 0.9482\n",
      "Epoch 00409: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2463 - accuracy: 0.9483 - val_loss: 0.7271 - val_accuracy: 0.7809\n",
      "Epoch 410/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.9406\n",
      "Epoch 00410: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2479 - accuracy: 0.9409 - val_loss: 0.5233 - val_accuracy: 0.8432\n",
      "Epoch 411/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2493 - accuracy: 0.9423\n",
      "Epoch 00411: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2487 - accuracy: 0.9428 - val_loss: 0.5241 - val_accuracy: 0.8650\n",
      "Epoch 412/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2308 - accuracy: 0.9523\n",
      "Epoch 00412: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2311 - accuracy: 0.9518 - val_loss: 1.5022 - val_accuracy: 0.6704\n",
      "Epoch 413/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9413\n",
      "Epoch 00413: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2501 - accuracy: 0.9412 - val_loss: 0.4863 - val_accuracy: 0.8763\n",
      "Epoch 414/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.9504\n",
      "Epoch 00414: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2354 - accuracy: 0.9504 - val_loss: 0.4662 - val_accuracy: 0.8697\n",
      "Epoch 415/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.9537\n",
      "Epoch 00415: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2343 - accuracy: 0.9537 - val_loss: 0.9233 - val_accuracy: 0.7847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2345 - accuracy: 0.9523\n",
      "Epoch 00416: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2351 - accuracy: 0.9518 - val_loss: 0.5782 - val_accuracy: 0.8508\n",
      "Epoch 417/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2443 - accuracy: 0.9513\n",
      "Epoch 00417: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2448 - accuracy: 0.9511 - val_loss: 0.8175 - val_accuracy: 0.7611\n",
      "Epoch 418/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2496 - accuracy: 0.9444\n",
      "Epoch 00418: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2489 - accuracy: 0.9447 - val_loss: 0.4790 - val_accuracy: 0.8508\n",
      "Epoch 419/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2306 - accuracy: 0.9530\n",
      "Epoch 00419: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2300 - accuracy: 0.9534 - val_loss: 0.4510 - val_accuracy: 0.8697\n",
      "Epoch 420/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2317 - accuracy: 0.9532\n",
      "Epoch 00420: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2312 - accuracy: 0.9534 - val_loss: 0.5726 - val_accuracy: 0.8178\n",
      "Epoch 421/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2258 - accuracy: 0.9530\n",
      "Epoch 00421: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2257 - accuracy: 0.9530 - val_loss: 0.5561 - val_accuracy: 0.8272\n",
      "Epoch 422/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9511\n",
      "Epoch 00422: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2319 - accuracy: 0.9509 - val_loss: 0.4976 - val_accuracy: 0.8517\n",
      "Epoch 423/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2295 - accuracy: 0.9544\n",
      "Epoch 00423: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2288 - accuracy: 0.9549 - val_loss: 0.5432 - val_accuracy: 0.8499\n",
      "Epoch 424/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2419 - accuracy: 0.9487\n",
      "Epoch 00424: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2435 - accuracy: 0.9483 - val_loss: 0.5896 - val_accuracy: 0.8074\n",
      "Epoch 425/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2435 - accuracy: 0.9468\n",
      "Epoch 00425: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2433 - accuracy: 0.9468 - val_loss: 0.4680 - val_accuracy: 0.8735\n",
      "Epoch 426/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.9521\n",
      "Epoch 00426: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2347 - accuracy: 0.9525 - val_loss: 0.5964 - val_accuracy: 0.8074\n",
      "Epoch 427/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2378 - accuracy: 0.9521\n",
      "Epoch 00427: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2371 - accuracy: 0.9525 - val_loss: 0.7306 - val_accuracy: 0.8300\n",
      "Epoch 428/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2361 - accuracy: 0.9563\n",
      "Epoch 00428: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2364 - accuracy: 0.9563 - val_loss: 0.9574 - val_accuracy: 0.7904\n",
      "Epoch 429/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2323 - accuracy: 0.9556\n",
      "Epoch 00429: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2321 - accuracy: 0.9558 - val_loss: 0.5760 - val_accuracy: 0.8206\n",
      "Epoch 430/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2328 - accuracy: 0.9513\n",
      "Epoch 00430: val_accuracy did not improve from 0.87819\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2329 - accuracy: 0.9513 - val_loss: 0.9537 - val_accuracy: 0.7743\n",
      "Epoch 431/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2357 - accuracy: 0.9497\n",
      "Epoch 00431: val_accuracy improved from 0.87819 to 0.88196, saving model to ./weight_cp\\weight_lstm2_noms.hdf5\n",
      "133/133 [==============================] - 2s 14ms/step - loss: 0.2358 - accuracy: 0.9494 - val_loss: 0.4682 - val_accuracy: 0.8820\n",
      "Epoch 432/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.9475\n",
      "Epoch 00432: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2404 - accuracy: 0.9475 - val_loss: 0.7847 - val_accuracy: 0.8187\n",
      "Epoch 433/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2553 - accuracy: 0.9416\n",
      "Epoch 00433: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2556 - accuracy: 0.9414 - val_loss: 0.4954 - val_accuracy: 0.8678\n",
      "Epoch 434/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.9449\n",
      "Epoch 00434: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2444 - accuracy: 0.9447 - val_loss: 1.1185 - val_accuracy: 0.7715\n",
      "Epoch 435/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9490\n",
      "Epoch 00435: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2394 - accuracy: 0.9483 - val_loss: 0.5845 - val_accuracy: 0.8508\n",
      "Epoch 436/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2339 - accuracy: 0.9547\n",
      "Epoch 00436: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2332 - accuracy: 0.9551 - val_loss: 0.4985 - val_accuracy: 0.8621\n",
      "Epoch 437/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2205 - accuracy: 0.9583\n",
      "Epoch 00437: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2197 - accuracy: 0.9586 - val_loss: 0.6760 - val_accuracy: 0.8347\n",
      "Epoch 438/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9511\n",
      "Epoch 00438: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2336 - accuracy: 0.9513 - val_loss: 0.5681 - val_accuracy: 0.8593\n",
      "Epoch 439/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2355 - accuracy: 0.9540\n",
      "Epoch 00439: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2359 - accuracy: 0.9539 - val_loss: 0.9388 - val_accuracy: 0.7649\n",
      "Epoch 440/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2333 - accuracy: 0.9509\n",
      "Epoch 00440: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2332 - accuracy: 0.9509 - val_loss: 0.4806 - val_accuracy: 0.8687\n",
      "Epoch 441/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2320 - accuracy: 0.9521\n",
      "Epoch 00441: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2336 - accuracy: 0.9520 - val_loss: 0.6072 - val_accuracy: 0.8234\n",
      "Epoch 442/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9585\n",
      "Epoch 00442: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2271 - accuracy: 0.9582 - val_loss: 1.8663 - val_accuracy: 0.6327\n",
      "Epoch 443/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2322 - accuracy: 0.9509\n",
      "Epoch 00443: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2325 - accuracy: 0.9506 - val_loss: 1.2415 - val_accuracy: 0.7101\n",
      "Epoch 444/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2246 - accuracy: 0.9559\n",
      "Epoch 00444: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2242 - accuracy: 0.9560 - val_loss: 0.6819 - val_accuracy: 0.8244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 445/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2279 - accuracy: 0.9563\n",
      "Epoch 00445: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2277 - accuracy: 0.9563 - val_loss: 1.3688 - val_accuracy: 0.7007\n",
      "Epoch 446/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2323 - accuracy: 0.9494\n",
      "Epoch 00446: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2328 - accuracy: 0.9490 - val_loss: 0.4733 - val_accuracy: 0.8744\n",
      "Epoch 447/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2316 - accuracy: 0.9528\n",
      "Epoch 00447: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2322 - accuracy: 0.9527 - val_loss: 0.4813 - val_accuracy: 0.8706\n",
      "Epoch 448/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9523\n",
      "Epoch 00448: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2332 - accuracy: 0.9511 - val_loss: 0.4773 - val_accuracy: 0.8612\n",
      "Epoch 449/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2231 - accuracy: 0.9590\n",
      "Epoch 00449: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2224 - accuracy: 0.9594 - val_loss: 0.7323 - val_accuracy: 0.8206\n",
      "Epoch 450/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2289 - accuracy: 0.9540\n",
      "Epoch 00450: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2288 - accuracy: 0.9537 - val_loss: 1.0494 - val_accuracy: 0.7488\n",
      "Epoch 451/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2314 - accuracy: 0.9521\n",
      "Epoch 00451: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2315 - accuracy: 0.9520 - val_loss: 0.7550 - val_accuracy: 0.7762\n",
      "Epoch 452/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2381 - accuracy: 0.9511\n",
      "Epoch 00452: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2379 - accuracy: 0.9511 - val_loss: 0.6110 - val_accuracy: 0.8102\n",
      "Epoch 453/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2269 - accuracy: 0.9556\n",
      "Epoch 00453: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2275 - accuracy: 0.9556 - val_loss: 0.8841 - val_accuracy: 0.7809\n",
      "Epoch 454/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2329 - accuracy: 0.9540\n",
      "Epoch 00454: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2334 - accuracy: 0.9537 - val_loss: 0.4652 - val_accuracy: 0.8810\n",
      "Epoch 455/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2486 - accuracy: 0.9442\n",
      "Epoch 00455: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2476 - accuracy: 0.9447 - val_loss: 0.5648 - val_accuracy: 0.8593\n",
      "Epoch 456/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2310 - accuracy: 0.9518\n",
      "Epoch 00456: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2317 - accuracy: 0.9516 - val_loss: 1.0673 - val_accuracy: 0.7649\n",
      "Epoch 457/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2379 - accuracy: 0.9506\n",
      "Epoch 00457: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2387 - accuracy: 0.9506 - val_loss: 0.6335 - val_accuracy: 0.8130\n",
      "Epoch 458/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2372 - accuracy: 0.9480\n",
      "Epoch 00458: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2372 - accuracy: 0.9480 - val_loss: 0.5104 - val_accuracy: 0.8555\n",
      "Epoch 459/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2300 - accuracy: 0.9511\n",
      "Epoch 00459: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2298 - accuracy: 0.9511 - val_loss: 0.4995 - val_accuracy: 0.8669\n",
      "Epoch 460/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2326 - accuracy: 0.9540\n",
      "Epoch 00460: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2331 - accuracy: 0.9537 - val_loss: 0.5033 - val_accuracy: 0.8706\n",
      "Epoch 461/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2298 - accuracy: 0.9530\n",
      "Epoch 00461: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2294 - accuracy: 0.9532 - val_loss: 0.5155 - val_accuracy: 0.8602\n",
      "Epoch 462/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2342 - accuracy: 0.9499\n",
      "Epoch 00462: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2340 - accuracy: 0.9499 - val_loss: 0.8719 - val_accuracy: 0.8159\n",
      "Epoch 463/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9458\n",
      "Epoch 00463: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2451 - accuracy: 0.9459 - val_loss: 0.5891 - val_accuracy: 0.8536\n",
      "Epoch 464/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2277 - accuracy: 0.9537\n",
      "Epoch 00464: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2277 - accuracy: 0.9537 - val_loss: 0.4752 - val_accuracy: 0.8697\n",
      "Epoch 465/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2434 - accuracy: 0.9518\n",
      "Epoch 00465: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2442 - accuracy: 0.9511 - val_loss: 0.8667 - val_accuracy: 0.7668\n",
      "Epoch 466/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.9621\n",
      "Epoch 00466: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2146 - accuracy: 0.9615 - val_loss: 0.6734 - val_accuracy: 0.8319\n",
      "Epoch 467/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2265 - accuracy: 0.9542\n",
      "Epoch 00467: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2277 - accuracy: 0.9537 - val_loss: 0.4922 - val_accuracy: 0.8565\n",
      "Epoch 468/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.9473\n",
      "Epoch 00468: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2411 - accuracy: 0.9475 - val_loss: 0.7052 - val_accuracy: 0.8300\n",
      "Epoch 469/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2206 - accuracy: 0.9566\n",
      "Epoch 00469: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2211 - accuracy: 0.9563 - val_loss: 1.1506 - val_accuracy: 0.7394\n",
      "Epoch 470/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2246 - accuracy: 0.9561\n",
      "Epoch 00470: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2246 - accuracy: 0.9563 - val_loss: 0.4752 - val_accuracy: 0.8565\n",
      "Epoch 471/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2284 - accuracy: 0.9556\n",
      "Epoch 00471: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2298 - accuracy: 0.9551 - val_loss: 0.7121 - val_accuracy: 0.7904\n",
      "Epoch 472/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.9518\n",
      "Epoch 00472: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2331 - accuracy: 0.9520 - val_loss: 0.4984 - val_accuracy: 0.8744\n",
      "Epoch 473/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2315 - accuracy: 0.9561\n",
      "Epoch 00473: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2313 - accuracy: 0.9563 - val_loss: 0.4734 - val_accuracy: 0.8706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2300 - accuracy: 0.9528\n",
      "Epoch 00474: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2298 - accuracy: 0.9527 - val_loss: 0.4814 - val_accuracy: 0.8782\n",
      "Epoch 475/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2325 - accuracy: 0.9501\n",
      "Epoch 00475: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2317 - accuracy: 0.9506 - val_loss: 0.8422 - val_accuracy: 0.7668\n",
      "Epoch 476/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2280 - accuracy: 0.9537\n",
      "Epoch 00476: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2277 - accuracy: 0.9539 - val_loss: 0.5152 - val_accuracy: 0.8574\n",
      "Epoch 477/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2261 - accuracy: 0.9580\n",
      "Epoch 00477: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2256 - accuracy: 0.9584 - val_loss: 0.9846 - val_accuracy: 0.7262\n",
      "Epoch 478/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2320 - accuracy: 0.9516\n",
      "Epoch 00478: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2318 - accuracy: 0.9518 - val_loss: 0.5333 - val_accuracy: 0.8687\n",
      "Epoch 479/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2363 - accuracy: 0.9518\n",
      "Epoch 00479: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2377 - accuracy: 0.9511 - val_loss: 1.1535 - val_accuracy: 0.7658\n",
      "Epoch 480/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9509\n",
      "Epoch 00480: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2310 - accuracy: 0.9506 - val_loss: 0.5646 - val_accuracy: 0.8584\n",
      "Epoch 481/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2237 - accuracy: 0.9592\n",
      "Epoch 00481: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2237 - accuracy: 0.9591 - val_loss: 0.5571 - val_accuracy: 0.8669\n",
      "Epoch 482/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9542\n",
      "Epoch 00482: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2262 - accuracy: 0.9537 - val_loss: 0.5446 - val_accuracy: 0.8376\n",
      "Epoch 483/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.9501\n",
      "Epoch 00483: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2355 - accuracy: 0.9501 - val_loss: 0.4953 - val_accuracy: 0.8716\n",
      "Epoch 484/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2225 - accuracy: 0.9606\n",
      "Epoch 00484: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2228 - accuracy: 0.9603 - val_loss: 0.7638 - val_accuracy: 0.8149\n",
      "Epoch 485/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2723 - accuracy: 0.9363\n",
      "Epoch 00485: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2718 - accuracy: 0.9364 - val_loss: 0.5308 - val_accuracy: 0.8659\n",
      "Epoch 486/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9580\n",
      "Epoch 00486: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2305 - accuracy: 0.9575 - val_loss: 0.5330 - val_accuracy: 0.8451\n",
      "Epoch 487/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2444 - accuracy: 0.9475\n",
      "Epoch 00487: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2436 - accuracy: 0.9478 - val_loss: 1.8211 - val_accuracy: 0.6317\n",
      "Epoch 488/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2263 - accuracy: 0.9571\n",
      "Epoch 00488: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2259 - accuracy: 0.9572 - val_loss: 0.7534 - val_accuracy: 0.8263\n",
      "Epoch 489/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2360 - accuracy: 0.9501\n",
      "Epoch 00489: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2353 - accuracy: 0.9506 - val_loss: 0.4926 - val_accuracy: 0.8763\n",
      "Epoch 490/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2264 - accuracy: 0.9561\n",
      "Epoch 00490: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2262 - accuracy: 0.9560 - val_loss: 0.5603 - val_accuracy: 0.8650\n",
      "Epoch 491/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9604\n",
      "Epoch 00491: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2191 - accuracy: 0.9603 - val_loss: 0.9416 - val_accuracy: 0.7894\n",
      "Epoch 492/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2173 - accuracy: 0.9580\n",
      "Epoch 00492: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2171 - accuracy: 0.9582 - val_loss: 0.4920 - val_accuracy: 0.8744\n",
      "Epoch 493/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2347 - accuracy: 0.9494\n",
      "Epoch 00493: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2343 - accuracy: 0.9497 - val_loss: 0.6327 - val_accuracy: 0.8480\n",
      "Epoch 494/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.9554\n",
      "Epoch 00494: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2312 - accuracy: 0.9558 - val_loss: 0.7521 - val_accuracy: 0.8291\n",
      "Epoch 495/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2252 - accuracy: 0.9563\n",
      "Epoch 00495: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2255 - accuracy: 0.9563 - val_loss: 0.6169 - val_accuracy: 0.8281\n",
      "Epoch 496/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.9540\n",
      "Epoch 00496: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2289 - accuracy: 0.9537 - val_loss: 0.6414 - val_accuracy: 0.8196\n",
      "Epoch 497/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2164 - accuracy: 0.9611\n",
      "Epoch 00497: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2171 - accuracy: 0.9605 - val_loss: 0.6937 - val_accuracy: 0.8385\n",
      "Epoch 498/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9559\n",
      "Epoch 00498: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2275 - accuracy: 0.9558 - val_loss: 0.5834 - val_accuracy: 0.8593\n",
      "Epoch 499/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2430 - accuracy: 0.9461\n",
      "Epoch 00499: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2424 - accuracy: 0.9461 - val_loss: 0.5496 - val_accuracy: 0.8480\n",
      "Epoch 500/500\n",
      "131/133 [============================>.] - ETA: 0s - loss: 0.2404 - accuracy: 0.9468\n",
      "Epoch 00500: val_accuracy did not improve from 0.88196\n",
      "133/133 [==============================] - 2s 12ms/step - loss: 0.2399 - accuracy: 0.9471 - val_loss: 0.5547 - val_accuracy: 0.8602\n"
     ]
    }
   ],
   "source": [
    "# Output is only based on the output generated after conv\n",
    "def conv_only_no_ms(w2v):\n",
    "    inputs = Input(shape=(X_train[0].shape[-1],))\n",
    "\n",
    "    embedding_layer = gensim_to_keras_embedding(w2v)\n",
    "    \n",
    "    embedding = embedding_layer(inputs)\n",
    "\n",
    "    lstm1 = LSTM(lstm_units,return_sequences=True, return_state=True, kernel_regularizer=l2(w_decay),recurrent_regularizer=l2(w_decay), dropout=dropout_rate)(embedding)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = Dense(units=1, activation='sigmoid', name='op_main')(lstm1[1])\n",
    "    \n",
    "\n",
    "    output_td_gap = GlobalAveragePooling1D(data_format='channels_first')(lstm1[0])\n",
    "    \n",
    "    output_td = TimeDistributed(Dense(units=1, activation='sigmoid'))(lstm1[0])\n",
    "    output_td = Flatten()(output_td)\n",
    "    \n",
    "    output_td = Multiply()([output_td_gap, output_td])\n",
    "    \n",
    "    output_td = Activation('relu', name='before_split')(output_td)\n",
    "    \n",
    "    output_td_splits = tf.split(output_td, 10, axis=-1)\n",
    "    \n",
    "    features = concatenate([output_td_splits[0], output_td_splits[1], output_td_splits[-2], output_td_splits[-1]])\n",
    "    \n",
    "    print(features.shape)\n",
    "    \n",
    "    output_td = Reshape((8, 10, 1))(features)\n",
    "    \n",
    "    output_td = Conv2D(2, 8, padding='same', strides=1, activation='relu', kernel_regularizer=l2(w_decay))(output_td)\n",
    "    output_td = BatchNormalization()(output_td)\n",
    "    output_td = Flatten()(output_td)\n",
    "   \n",
    "\n",
    "    output_td = Dense(units=1, activation='sigmoid', name='op_conv')(output_td)\n",
    "    \n",
    "    \n",
    "    \n",
    "    avg = tf.keras.layers.Average(name='avg')([output, output_td])\n",
    "    \n",
    "\n",
    "    model = Model(inputs, output_td)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = conv_only_no_ms(w2v_model)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('./weight_cp/weight_lstm2_noms.hdf5', save_freq=\"epoch\",  verbose=1, monitor='val_accuracy', save_best_only=True,\n",
    "    save_weights_only=False)\n",
    "\n",
    "metrics = ['accuracy']\n",
    "optimizer = Adam(0.0001)\n",
    "model.compile(optimizer = optimizer, loss='binary_crossentropy', metrics=metrics)\n",
    "model.summary()\n",
    "history2 = model.fit(X_train, y_train, epochs=epochs_to_run, validation_data=(X_val, y_val), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b6bcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF Not-Supervised LSTM (Convolution output)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88       661\n",
      "           1       0.89      0.87      0.88       662\n",
      "\n",
      "    accuracy                           0.88      1323\n",
      "   macro avg       0.88      0.88      0.88      1323\n",
      "weighted avg       0.88      0.88      0.88      1323\n",
      "\n",
      "0.8767951625094482\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./weight_cp/weight_lstm2_noms.hdf5')\n",
    "predictionss = model.predict(X_test)\n",
    "predictions = np.where(predictionss > 0.5, 1, 0)\n",
    "y_pred = []\n",
    "for p in predictions:\n",
    "    y_pred.append(p[0])\n",
    "y_pred = np.array(y_pred)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"CLASSIFICATION REPORT OF Not-MultiSupervised LSTM (Convolution output)\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7013a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history1.history['val_avg_accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('Validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['With MS', 'Without MS (Conv only)'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c0a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b2d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
